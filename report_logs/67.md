Query ID: 67
============================================================

QUESTION:
Summarize recent research progress in reinforcement learning focused on enabling agents to explore efficiently and proactively under conditions of sparse rewards and constraints, respectively. Additionally, analyze and discuss the potential implications and insights this research provides for trajectory planning problems.

============================================================

RESEARCH TASKS (5 total):
  #1: Research 2023–Feb 2026 literature taxonomies and key papers for efficient exploration in RL under sparse rewards. Cover intrinsic motivation/curiosity, count/coverage bonuses, goal-conditioned RL + hindsight relabeling, unsupervised skill discovery, world-model-based exploration, and any notable scalable deep RL results; summarize empirical evidence, benchmarks (Atari, Procgen, DMControl, hard-exploration), and pros/cons.
  #2: Research 2023–Feb 2026 literature on proactive exploration / information-seeking RL under uncertainty and partial observability. Focus on methods that explicitly plan to reduce uncertainty or maximize information gain (Bayesian RL, ensembles, posterior sampling, belief-space planning, active inference, exploration via model-based planning), especially for long-horizon sparse-reward tasks; capture how 'proactive' is defined and key empirical/theoretical results.
  #3: Research 2023–Feb 2026 state of the art in constrained RL and safe exploration. Include CMDP methods (Lagrangian/primal-dual), risk-sensitive RL (CVaR, distributional constraints), shielding/barrier functions/reachability, safe model-based RL, offline-to-online safe RL, and reported theoretical guarantees and failure modes. Also look for work unifying safe constraints with exploration bonuses.
  #4: Research 2023–Feb 2026 work that unifies sparse-reward exploration with safety/constraints (safe curiosity, constrained intrinsic motivation, safe model-based rollouts, offline-to-online safe exploration). Also collect commonly used benchmarks and metrics for evaluating 'efficient + safe/proactive' exploration (e.g., Safety Gym, DSRL, D4RL/NeoRL with costs, Atari hard exploration, DMControl URLB, navigation/robotics). Summarize evaluation practices and open problems.
  #5: Research implications of these RL advances (efficient exploration, proactive information seeking, constrained/safe RL) for trajectory planning and control problems in robotics/navigation/autonomous driving/manipulation. Focus on how intrinsic motivation/uncertainty bonuses translate to planners, uncertainty-aware MPC and belief-space planning, safe MPC/control barrier/shielding layers, model-based rollouts, sampling-based planners with learned models/costs, and long-horizon credit assignment for trajectory generation (2023–Feb 2026 sources).

============================================================

ANSWER:
## 1) Taxonomy (2023–Feb 2026): what “efficient exploration” and “proactive exploration under constraints” mean in practice

### A. Efficient exploration under *sparse rewards* (how research decomposes the problem)
Recent work largely treats sparse-reward exploration as a *state/trajectory discovery* problem: the agent must reliably reach informative states before any reward signal can shape behavior. The main families:

1) **Intrinsic motivation / intrinsic rewards**
- *Prediction-error / curiosity* (learn a forward model, reward surprise), but vulnerable to “novelty loops” (e.g., stochastic/noisy stimuli) and misalignment with task progress (“detachment”) [[1]](https://arxiv.org/abs/2302.10825), [[2]](https://arxiv.org/pdf/2403.17542v1).
- *Information gain / epistemic uncertainty bonuses* (reward reduction in model uncertainty), increasingly implemented with ensembles/world models and tied to more “directed” exploration [[24]](https://proceedings.iclr.cc/paper_files/paper/2025/file/bd996108ed57d388866ca6deb7acf6cb-Paper-Conference.pdf), [[52]](https://openreview.net/pdf/f3c9a883879962fd46e5ea5300ccc2844cd61c3b.pdf).

2) **Count-/coverage-driven exploration**
- Exact counts don’t scale; modern variants use hashing or density models for pseudo-counts [[2]](https://arxiv.org/pdf/2403.17542v1).
- A notable trend is **using novelty/counts as a *policy switch/trigger*** (when to explore) rather than always adding a bonus to the reward, to reduce reward distortion and novelty traps [[2]](https://arxiv.org/pdf/2403.17542v1).

3) **Goal-directed and return-to-state exploration**
- “Hard exploration” methods such as **Go-Explore** explicitly (i) remember promising states, (ii) reliably return to them, then (iii) explore outward, which attacks long-horizon credit assignment by construction [[3]](https://arxiv.org/abs/1901.10995). This remains a reference point for sparse-reward settings like Montezuma’s Revenge-style exploration [[2]](https://arxiv.org/pdf/2403.17542v1), [[3]](https://arxiv.org/abs/1901.10995).

4) **Goal-conditioned RL + relabeling**
- **Hindsight Experience Replay (HER)** remains foundational: relabel failed trajectories as successes for alternative goals, making sparse rewards learnable in goal spaces [[10]](https://arxiv.org/abs/1707.01495). (Many 2023–2026 works build on this paradigm; a 2025 “hindsight regularization” direction is visible but not fully available in the provided sources [[11]](https://arxiv.org/html/2508.06108v1).)

5) **Unsupervised / reward-free RL and skill discovery**
- Learn diverse skills that cover the state space, then adapt quickly once rewards appear (two-phase evaluation popularized by URLB) [[13]](https://arxiv.org/abs/2110.15191).
- Newer work focuses on *what diversity means* and *how to get coverage*, not only mutual information.

6) **World-model-based exploration**
- Learn a latent dynamics model; use imagined rollouts to pick actions that maximize novelty/uncertainty/semantic objectives while reducing environment interactions [[8]](https://neurips.cc/virtual/2024/poster/95344), [[9]](https://icml.cc/virtual/2025/poster/44870), [[52]](https://openreview.net/pdf/f3c9a883879962fd46e5ea5300ccc2844cd61c3b.pdf).

---

### B. “Proactive exploration” under *constraints* (how research frames it)
In constrained/safe RL, “proactive” increasingly means the agent must **seek information and improve performance without violating constraints during learning** (or while controlling tail risk), rather than treating safety as an afterthought.

Dominant formalizations and method families:

1) **CMDPs + primal–dual/Lagrangian optimization**
- Optimize reward subject to expected cost constraints; scalable but prone to estimation bias and training-time violations; new work emphasizes convergence guarantees and correcting known failure modes [[37]](https://arxiv.org/html/2505.15138v2), [[46]](https://arxiv.org/abs/2401.14758).

2) **Risk-sensitive / distributional constraints**
- Replace expectation constraints with tail constraints (e.g., CVaR), addressing the fact that “safe in expectation” can still be unsafe with meaningful probability [[44]](https://ieeexplore.ieee.org/document/10444044/), [[45]](https://link.springer.com/article/10.1007/s10994-022-06187-8).

3) **Shielding / safety filters / reachability & barrier methods**
- Place a “safety layer” between policy and environment: minimally modify actions to remain in a safe set (CBFs, reachability, predictive safety filters), sometimes with formal verification [[39]](https://herowanzhu.github.io/CAV_2024.pdf), [[41]](https://calinbelta.com/wp-content/uploads/2023/12/Safe-exploration-in-model-based.pdf), [[42]](https://arxiv.org/html/2510.14959v2), [[83]](https://hybrid-robotics.berkeley.edu/publications/CSM2023_Safety_Filters.pdf).
- A key trend is **training with the filter** so the policy internalizes constraints and avoids “chattering”/performance loss from a separated filter-controller design [[43]](https://www.dynsyslab.org/wp-content/papercite-data/pdf/pizarro-ral25.pdf).

4) **Safe model-based RL (world models + constrained planning)**
- Use world models to plan safely in imagination and reduce real unsafe interactions; particularly prominent for vision-based safe control [[55]](https://proceedings.iclr.cc/paper_files/paper/2024/file/ece182f93af26c64187ba3f7dfd4309a-Paper-Conference.pdf), [[56]](https://arxiv.org/html/2307.07176v3).

5) **Unified safe exploration formulations**
- Meta-algorithms that explicitly combine uncertainty quantification with exploration to guarantee (under assumptions) no violations during training [[35]](https://neurips.cc/virtual/2023/poster/71024), [[63]](https://arxiv.org/abs/2310.03225).

---

## 2) Recent progress: enabling **efficient exploration under sparse rewards**

### 2.1 Intrinsic motivation: gains, but clearer understanding of failure modes

**Key observed limitations (now explicitly named and targeted):**
- **Detachment problem**: intrinsic curiosity can become misaligned with task progress, especially in multi-agent sparse reward settings; ICM-like rewards may drive exploration that doesn’t help reach rewarding outcomes [[1]](https://arxiv.org/abs/2302.10825).
- **Novelty loops / “Noisy-TV”**: prediction-error bonuses can lock onto inherently stochastic/unpredictable observations, preventing task completion [[2]](https://arxiv.org/pdf/2403.17542v1).

**Representative responses/trends:**
- **Hybridizing curiosity with structured exploration**: I-Go-Explore combines ICM-style curiosity with Go-Explore’s return-and-expand structure to mitigate detachment in sparse-reward multi-agent settings [[1]](https://arxiv.org/abs/2302.10825). The broader insight is that *intrinsic reward alone is often not enough*; you also need a mechanism to revisit and systematically expand from progress states.

---

### 2.2 Count-/coverage-based exploration: from “bonus everywhere” → “when to explore”

A notable recent perspective shift is that exploration is not only about *how much* novelty to reward, but also **when to trigger exploration**.

- **VDSC (2024)** proposes exploration “triggers” based on:
  - **Value Promise Discrepancy (VPD)**: discrepancy between predicted returns and realized returns over a window; high discrepancy triggers exploration [[2]](https://arxiv.org/pdf/2403.17542v1).
  - **State counts via hashing (SimHash)**: novelty triggers exploration without directly adding a bonus to the reward function [[2]](https://arxiv.org/pdf/2403.17542v1).

**Why this matters in sparse-reward problems:**
- Sparse reward often means the extrinsic reward is a fragile learning signal; always injecting intrinsic rewards can distort optimization.
- Trigger-based exploration is explicitly motivated as a way to preserve the original reward structure and reduce pathological intrinsic-reward behaviors (reward distortion, novelty loops) [[2]](https://arxiv.org/pdf/2403.17542v1).

**Caveats (also clearly articulated):**
- VPD can be high due to early learning error or environment stochasticity rather than meaningful uncertainty; VPD alone ignores novelty, motivating combining triggers [[2]](https://arxiv.org/pdf/2403.17542v1).

---

### 2.3 Goal-directed hard exploration: remembering and returning as an algorithmic primitive

**Go-Explore** remains the canonical exemplar: systematically return to previously found states (“cells”) and explore outward, rather than relying on undirected random action noise [[3]](https://arxiv.org/abs/1901.10995). In the taxonomy of hard exploration (e.g., Montezuma-like domains), it is repeatedly cited as a breakthrough because it attacks the *long-horizon* aspect directly—by construction, it breaks exploration into (i) return, (ii) local exploration from promising states [[2]](https://arxiv.org/pdf/2403.17542v1), [[3]](https://arxiv.org/abs/1901.10995).

**Practical implication for sparse rewards:**
- When reward requires long action sequences, *reset-to-subgoal* or *return-to-state* mechanisms can dominate pure intrinsic reward shaping.

---

### 2.4 Unsupervised skill discovery: from “mutual information” → “coverage objectives”

A strong 2024 trend is sharpening objectives so “diverse skills” actually mean **state-space coverage**, not just an information-theoretic quantity that can be gamed.

- **LEADS (NeurIPS 2024)** argues mutual information can be *ambiguous* for exploration: two sets of skills can yield the same MI while one covers the space much better [[5]](https://proceedings.neurips.cc/paper_files/paper/2024/file/8997f9fff71e14a2c01bae9c6e226f44-Paper-Conference.pdf), [[6]](https://arxiv.org/html/2406.10127v1). LEADS uses successor-state representations/measures to directly maximize differences between skill-conditioned state distributions, targeting uniform coverage without reward bonuses [[5]](https://proceedings.neurips.cc/paper_files/paper/2024/file/8997f9fff71e14a2c01bae9c6e226f44-Paper-Conference.pdf), [[6]](https://arxiv.org/html/2406.10127v1).
- **DUSDi (NeurIPS 2024)** targets *disentanglement*: skills that control independent factors are easier to compose and chain in hierarchical RL, improving downstream problem solving after reward appears [[7]](https://neurips.cc/virtual/2024/poster/94271).

**Why this matters for sparse rewards:**
- Sparse-reward tasks often require a repertoire of reusable behaviors (reach, push, navigate corridors, etc.). Skill discovery reframes sparse reward as a *downstream* fine-tuning problem after broad competence acquisition.

**Benchmarking trend:**
- **URLB** formalizes reward-free pretraining followed by data-efficient adaptation and shows existing baselines make progress but do not solve the benchmark, highlighting headroom and giving a standardized way to measure exploration utility [[13]](https://arxiv.org/abs/2110.15191).

---

### 2.5 World-model-based exploration: better scaling, better targeting, and new failure modes (distractors)

World models shift exploration from “act randomly and learn from reality” to “learn a model, then plan exploration in imagination,” reducing environment interactions.

Two newer issues and responses:

1) **Observation distractors corrupt intrinsic rewards**
- **SeeX (NeurIPS 2024 poster)**: in pixel-based settings, distractors bias uncertainty estimates and intrinsic rewards. SeeX proposes separating endogenous (task-relevant) and exogenous factors in the world model; exploration maximizes uncertainty in endogenous space using imaginary rollouts [[8]](https://neurips.cc/virtual/2024/poster/95344).

2) **Semantic exploration via foundation models**
- **SENSEI (ICML 2025 poster)** argues standard intrinsic bonuses yield low-level behaviors; it distills a semantic “interestingness” reward from VLM annotations and trains a world model to predict it, then explores by maximizing semantic reward and uncertainty [[9]](https://icml.cc/virtual/2025/poster/44870).
- The key conceptual shift: “exploration” can be guided toward *meaningful interaction* priors, not only novelty.

---

### 2.6 More principled uncertainty-driven exploration (bridging to “proactive”)
Even in sparse-reward (not explicitly constrained) settings, exploration is becoming more “proactive” via Bayesian/ensemble uncertainty and planning.

- **PSDRL (ICML/PMLR)** scales **posterior sampling** (Thompson-style) to deep RL: maintain a Bayesian distribution over latent dynamics model parameters, sample a model, plan, act for an episode, update posterior [[21]](https://proceedings.mlr.press/v202/sasso23a/sasso23a.pdf), [[22]](https://arxiv.org/pdf/2305.00477). It reports strong Atari results vs prior posterior-sampling approximations and competitiveness with DreamerV2 [[21]](https://proceedings.mlr.press/v202/sasso23a/sasso23a.pdf), [[22]](https://arxiv.org/pdf/2305.00477).
- This approach is important because it explicitly distinguishes:
  - uncertainty about the environment vs uncertainty about value estimates,
  - and uses model-based planning to convert uncertainty into structured exploration [[21]](https://proceedings.mlr.press/v202/sasso23a/sasso23a.pdf), [[22]](https://arxiv.org/pdf/2305.00477).

---

## 3) Recent progress: enabling agents to explore **proactively under constraints**

### 3.1 CMDP optimization (primal–dual / Lagrangian): better theory, clearer practical pitfalls

**Theoretical progress:**
- Work on average-reward CMDPs provides global convergence rate characterizations for primal-dual actor-critic methods under mixing assumptions, including rates for objective convergence and constraint violation that approach \~O(T^{-1/2}) under known mixing time and slightly weaker rates otherwise [[37]](https://arxiv.org/html/2505.15138v2).

**Practical failure modes now explicitly emphasized:**
- **Off-policy cost underestimation** can break safety: primal–dual methods rely on cumulative cost estimates; estimation errors lead to unsafe policies [[46]](https://arxiv.org/abs/2401.14758).
- A response is **conservative policy optimization** (optimize in constraint-satisfying regions acknowledging cost uncertainty), but it can reduce reward; “local policy convexification” is proposed to reduce suboptimality as uncertainty shrinks [[46]](https://arxiv.org/abs/2401.14758).

**Takeaway for “proactive constrained exploration”:**
- Pure Lagrangian training is often reactive—violations teach the agent. Proactive safety requires either strong uncertainty-aware conservatism or external safety enforcement.

---

### 3.2 Risk-sensitive safe RL: moving beyond “safe in expectation”

A prominent message in 2023–2025 safe RL is: **expectation constraints are insufficient**.

- **CVaR-CPO** explicitly frames expectation-only constraints as inadequate and optimizes policies under **CVaR constraints** to control tail risk of cumulative costs [[44]](https://ieeexplore.ieee.org/document/10444044/).
- **WCSAC (distributional safety critic)** models the distribution of safety costs and enforces a CVaR-style constraint; it highlights an estimation pitfall: Gaussian approximations may *underestimate* safety cost tails, while quantile regression is more conservative (often safer but potentially more conservative) [[45]](https://link.springer.com/article/10.1007/s10994-022-06187-8).

**Implication for proactive constrained exploration:**
- Tail-risk formulations are inherently more proactive: they discourage “rare catastrophe” exploration strategies that might look safe on average but are unacceptable in safety-critical settings.

---

### 3.3 Safety filters, shielding, and verification: proactive safety as *action editing* (and why training must include it)

A major practical trend is that **hard constraints are enforced outside the learning objective** (or jointly with it), via shields/filters:

- **SPICE** uses weakest preconditions in a neurosymbolic shield to guide safe exploration and claims fewer violations plus convergence under assumptions [[38]](https://arxiv.org/abs/2209.14148).
- **VELM (CAV 2024)** pushes toward *formal reachability* by learning symbolic dynamics formulas and performing reachability analysis to build a shield that restricts exploration to verified-safe regions; it argues prior neural barriers and short-horizon weakest-precondition approaches remain hard to verify or too myopic for long horizons [[39]](https://herowanzhu.github.io/CAV_2024.pdf).
- **CBF-based approaches** provide forward-invariant safe sets; model-based RL frameworks integrate barrier functions to decouple safety guarantees from RL convergence assumptions [[41]](https://calinbelta.com/wp-content/uploads/2023/12/Safe-exploration-in-model-based.pdf).
- **CBF-RL (2025)** aims to fuse safety filtering and training-time shaping so the final policy internalizes constraints and can run without a runtime filter (per its claims) [[42]](https://arxiv.org/html/2510.14959v2).

**Separation failure mode (“chattering”):**
- If a safety filter is applied only at runtime, the learned policy may repeatedly propose unsafe actions; the filter corrects them, degrading performance and causing oscillations [[43]](https://www.dynsyslab.org/wp-content/papercite-data/pdf/pizarro-ral25.pdf).
- Training with the filter improves convergence and reduces chattering; it also can eliminate violations during training and enable direct physical training in some settings [[43]](https://www.dynsyslab.org/wp-content/papercite-data/pdf/pizarro-ral25.pdf).

**Key conceptual takeaway:**
- For constrained exploration, “proactive” often means *predicting future constraint violations* and editing actions *before* reaching irreversible unsafe states—an invariance/reachability view emphasized in safety filter tutorials [[83]](https://hybrid-robotics.berkeley.edu/publications/CSM2023_Safety_Filters.pdf).

---

### 3.4 Unified safe exploration formulations: safety guarantees via uncertainty quantification

- **MASE / Generalized Safe Exploration (NeurIPS 2023)** frames safe exploration as a unified problem and proposes a meta-algorithm that combines an unconstrained RL learner with an uncertainty quantifier to guarantee (under assumptions) no safety violations with high probability; it also penalizes unsafe explorations *before* violations [[35]](https://neurips.cc/virtual/2023/poster/71024), [[63]](https://arxiv.org/abs/2310.03225).
- It reports better performance than baselines on gridworld and Safety Gym while avoiding violations even during training [[35]](https://neurips.cc/virtual/2023/poster/71024), [[63]](https://arxiv.org/abs/2310.03225).

This is explicitly “proactive” in the sense that the algorithm uses uncertainty to identify and avoid potentially unsafe explorations.

---

### 3.5 Safe model-based RL: planning safely “in imagination” (especially important in vision-based tasks)

Vision-based safe RL is where model-based methods have shown particularly strong momentum.

- **SafeDreamer (ICLR 2024)** integrates a world model with constrained planning (including constrained CEM planning and Lagrangian constraint handling) and reports nearly zero-cost behavior across Safety-Gymnasium tasks, including vision-only tasks [[55]](https://proceedings.iclr.cc/paper_files/paper/2024/file/ece182f93af26c64187ba3f7dfd4309a-Paper-Conference.pdf), [[56]](https://arxiv.org/html/2307.07176v3).
- This line argues that world models improve both **sample efficiency** and the ability to maintain constraints when costs must remain very low—something model-free safe RL struggles with in complex/high-dimensional settings [[55]](https://proceedings.iclr.cc/paper_files/paper/2024/file/ece182f93af26c64187ba3f7dfd4309a-Paper-Conference.pdf), [[56]](https://arxiv.org/html/2307.07176v3).

**Unifying sparse reward + constraints (explicitly):**
- **ActSafe (2024)** uses epistemic uncertainty as intrinsic reward to learn dynamics efficiently *while maintaining safety constraints*, reporting reduced constraint violations on Safety-Gym vision tasks and strong performance on newly introduced sparse-reward Safety Gym variants (no shaping) [[53]](https://arxiv.org/pdf/2410.09486).
  - It also surfaces a key practicality: fully safe learning from scratch with a randomly initialized neural world model is unrealistic; ActSafe uses a warm-up data collection period to calibrate the model before enforcing strict safety [[53]](https://arxiv.org/pdf/2410.09486).

---

### 3.6 Offline-to-online safe exploration / safe generalization (deployment-time constraints)
A newer practical concern is: policies trained offline may be safe in the dataset distribution, but deployment requires *safe adaptation*.

- **FOSP (ICLR 2025)** fine-tunes an offline safe policy online using world models with reachability guidance, aiming to improve safe deployment and generalization in unseen constrained scenarios; it reports simulation and real-robot improvements in success rate and reduced constraint violation after limited fine-tuning [[57]](https://sunlighted.github.io/fosp_web/), [[58]](https://arxiv.org/html/2407.04942v2).
- It provides a particularly valuable empirical insight: offline dataset composition matters—
  - too much “safe” data can lead to aggressive policies that violate constraints,
  - too much unsafe data can cause over-conservatism that blocks exploration,
  - due to cost-model prediction errors propagating through training [[58]](https://arxiv.org/html/2407.04942v2).

---

## 4) What counts as “proactive exploration” in the recent literature (beyond reactive stochasticity)

Across the sources, proactive exploration is characterized by **explicitly selecting actions for information**, not merely adding noise:

1) **Information gain / epistemic objectives added to action selection**
- **MaxInfoRL (ICLR 2025)**: incorporate information-gain bonuses (estimated via model ensembles) into Boltzmann-style exploration; claims strong performance across deep RL benchmarks and sublinear regret in simplified bandits [[24]](https://proceedings.iclr.cc/paper_files/paper/2025/file/bd996108ed57d388866ca6deb7acf6cb-Paper-Conference.pdf).

2) **Posterior sampling with planning**
- **PSDRL**: sample a plausible dynamics model from a Bayesian posterior, plan, execute, update—exploration naturally decreases as uncertainty shrinks [[21]](https://proceedings.mlr.press/v202/sasso23a/sasso23a.pdf), [[22]](https://arxiv.org/pdf/2305.00477).

3) **Taking locally suboptimal actions to make rewards/information observable**
- **Beyond Optimism (NeurIPS 2024)**: in “Monitored MDPs” where rewards may be unobservable unless the agent takes an enabling action (paying a cost), optimistic algorithms can fail because they never choose the suboptimal information-acquisition action. The paper proposes a successor-representation-based exploration strategy and claims convergence guarantees in that setting [[25]](https://proceedings.neurips.cc/paper_files/paper/2024/file/784fd5a46dfe303e5b51c8621b84cf3f-Paper-Conference.pdf), [[26]](https://arxiv.org/html/2406.13909v1).

4) **Belief-space planning (POMDP solvers and learned heuristics)**
- **GPOMCP (NeurIPS 2023)**: online belief-tree search with belief reuse and mechanisms to handle information-less/reward-less cases in object search POMDPs [[27]](https://neurips.cc/virtual/2023/poster/72506).
- **GammaZero (2025)**: graph representations to guide belief-space MCTS with learned heuristics, enabling zero-shot generalization to larger POMDPs and explicitly targeting long-horizon information-gathering depth [[30]](https://arxiv.org/html/2510.14035v1).

5) **Active inference formulations**
- Expected free energy connects pragmatic reward-seeking and epistemic information-seeking components; work analyzes EFE as approximating Bayes-optimal RL under particular belief-MDP constructions [[28]](https://arxiv.org/abs/2408.06542).
- **Active Inference Tree Search (AcT)** attempts to scale epistemic/pragmatic objectives to large POMDPs via tree search and reports competitive POMDP performance in benchmarks like RockSample [[29]](https://www.sciencedirect.com/science/article/pii/S0925231224020903).

**Connection to constrained settings:**
- These proactive methods become constrained exploration mechanisms when combined with explicit safety budgets/filters—e.g., epistemic bonuses can drive exploration *within* safe sets (via shields) or safe belief regions.

---

## 5) Unifying sparse-reward exploration and constraints: what’s emerging and how it is evaluated

### 5.1 Algorithmic unification patterns
Several concrete patterns recur:

1) **Optimism + pessimism combination**
- **DOPE** explicitly combines optimistic reward bonuses for exploration with pessimistic handling of constraints in model-based CMDPs and claims no constraint violation during learning with regret guarantees (in its setting) [[36]](https://arxiv.org/abs/2112.00885).

2) **Uncertainty-driven intrinsic reward + explicit safety constraints**
- **ActSafe** and related uncertainty-optimistic MBRL work use epistemic uncertainty as an intrinsic signal to solve sparse rewards while maintaining constraints [[53]](https://arxiv.org/pdf/2410.09486), [[52]](https://openreview.net/pdf/f3c9a883879962fd46e5ea5300ccc2844cd61c3b.pdf).

3) **World-model rollouts + constrained planning**
- **SafeDreamer** uses a world model and constrained planning to achieve low cost and good returns, especially in vision-only tasks where exploration is expensive and risky [[55]](https://proceedings.iclr.cc/paper_files/paper/2024/file/ece182f93af26c64187ba3f7dfd4309a-Paper-Conference.pdf), [[56]](https://arxiv.org/html/2307.07176v3).

4) **Shielding / filters that enable exploration without catastrophe**
- Shielding under partial observability can both enforce safety and *improve convergence* in sparse-reward POMDP environments; there is also evidence of “bootstrapping then relax the shield” as a practical strategy [[62]](https://ojs.aaai.org/index.php/AAAI/article/view/26723).
- Approximate shielding using latent world models has been explored even in Atari-like domains with safety labels, reducing violations and sometimes improving learning speed [[61]](https://arxiv.org/abs/2304.11104).

5) **Constraining intrinsic motivation itself (avoid distraction / unsafe curiosity)**
- **CIM (IJCAI 2024)** frames intrinsic motivation through constrained optimization: maximize exploration-related objectives (e.g., conditional state entropy lower bound) subject to representation alignment constraints, and adaptively tune intrinsic coefficients via constrained policy optimization to reduce distraction when extrinsic rewards exist [[59]](https://arxiv.org/abs/2407.09247), [[60]](https://arxiv.org/pdf/2407.09247).
  - While not “safety constraints” per se, the methodological point generalizes: intrinsic drives should be *regulated* by constraints to avoid harmful or wasteful exploration.

6) **Representation learning to avoid over-conservatism that kills exploration**
- Contrastive representation learning has been proposed to reduce false positives from safety critics that otherwise block safe actions and hinder exploration in sparse reward settings [[64]](https://arxiv.org/html/2503.10318v1).

---

### 5.2 Benchmarks and metrics (what “efficient + safe/proactive” is actually measured with)

**Online safe exploration (continuous control):**
- **Safety Gym** introduced the CMDP benchmark style with separate reward and cost signals and recommended reporting not just final performance but *training-time safety regret* (unsafe exploration cost incurred during learning) [[50]](https://cdn.openai.com/safexp-short.pdf).
- **Safety-Gymnasium (NeurIPS 2023)** modernizes and expands this ecosystem (MuJoCo-native, vision tasks, multi-agent tasks) and ships SafePO (16 algorithms) enabling standardized comparisons across many environments [[51]](https://proceedings.neurips.cc/paper_files/paper/2023/file/3c557a3d6a48cc99444f85e924c66753-Paper-Datasets_and_Benchmarks.pdf). It is used heavily by SafeDreamer, ActSafe, FOSP, and other recent safe model-based RL work [[55]](https://proceedings.iclr.cc/paper_files/paper/2024/file/ece182f93af26c64187ba3f7dfd4309a-Paper-Conference.pdf), [[56]](https://arxiv.org/html/2307.07176v3), [[53]](https://arxiv.org/pdf/2410.09486), [[57]](https://sunlighted.github.io/fosp_web/), [[58]](https://arxiv.org/html/2407.04942v2).

**Offline safe RL and constraint generalization:**
- The **Offline SafeRL benchmark suite / DSRL** provides cost-labeled datasets (38 tasks) across SafetyGymnasium, BulletSafetyGym, and MetaDrive, with standardized APIs and explicit evaluation emphasis on reward–cost tradeoffs, constraint variation, and robustness to dataset filtering [[65]](https://github.com/liuzuxin/DSRL), [[66]](https://www.offline-saferl.org/), [[67]](https://data.mlr.press/assets/pdf/v01-12.pdf).
- Key metric design:
  - normalized reward,
  - normalized cost as ratio to a target threshold,
  - evaluate under multiple constraint thresholds to test adaptability [[66]](https://www.offline-saferl.org/), [[65]](https://github.com/liuzuxin/DSRL).

**Broader “realism” trend:**
- **NeoRL-2** argues offline RL benchmarks should reflect real-world constraints, delays, exogenous factors, and safe-operation dataset bias; it reports that SOTA methods often fail to beat behavior policies in these harder tasks [[69]](https://arxiv.org/html/2503.19267v1).
- **SafeOR-Gym (2025)** brings safe RL benchmarks to operations-research-like structured constraints and hybrid action spaces, highlighting limitations of current safe RL algorithms beyond robotics [[70]](https://arxiv.org/html/2506.02255v1).

---

## 6) Implications and insights for **trajectory planning** (robotics/navigation/autonomy/manipulation)

Trajectory planning problems (MPC, sampling-based planning, belief-space planning) and the RL research above are converging on a shared set of principles: **optimize trajectories not only for reward but also for information, uncertainty, and constraints—over long horizons with imperfect models.**

Below are the most actionable implications.

---

### 6.1 Replace “random exploration” with *trajectory-level objectives* (novelty, information gain, semantics)

**Planning interpretation of intrinsic motivation:**
- Intrinsic reward methods are equivalent to adding terms to a trajectory cost functional:
  - prediction error → “surprise cost/bonus,”
  - state entropy/coverage → “coverage regularizer,”
  - information gain → “epistemic value.”
- MaxInfoRL-style information gain bonuses can be interpreted as *actively selecting trajectories that reduce model uncertainty* [[24]](https://proceedings.iclr.cc/paper_files/paper/2025/file/bd996108ed57d388866ca6deb7acf6cb-Paper-Conference.pdf).

**Planner-side consequence:**
- In trajectory optimization, this suggests designing objectives like:
  - maximize expected task reward
  - plus maximize expected information gain about dynamics/latent state
  - subject to constraints (hard or chance constraints)

This is exactly the structure seen in uncertainty-aware MPC that actively chooses actions to both control and improve learning efficiency [[73]](https://dl.acm.org/doi/abs/10.1109/TRO.2023.3339543), and in belief-space planners that act to reduce uncertainty [[27]](https://neurips.cc/virtual/2023/poster/72506), [[81]](https://www.mdpi.com/2218-6581/14/9/127).

---

### 6.2 Uncertainty must be propagated along the trajectory, not treated as a point estimate

Sparse rewards and constraints both amplify the “compounding error” problem: a small model error early in the horizon can invalidate the rest of a planned trajectory.

Two converging strands stress *multi-step uncertainty propagation*:

- Offline/world-model RL for robotics explicitly propagates epistemic uncertainty over long imagined rollouts and penalizes risky imagined transitions during policy optimization [[72]](https://arxiv.org/html/2504.16680v3).
- Uncertainty-aware MPC methods explicitly condition control decisions on uncertainty and actively learn dynamics online to remain accurate under changing regimes [[73]](https://dl.acm.org/doi/abs/10.1109/TRO.2023.3339543).

**Trajectory-planning insight:**
- Planning should treat uncertainty as a first-class dynamical quantity (belief covariance, ensemble disagreement, mutual information), and the planner should:
  - shorten horizon or reduce speed when uncertainty grows (dynamic horizon / speed planning is explicitly used in hybrid RL–MPC navigation with ensemble MI) [[74]](https://arxiv.org/abs/2310.00760),
  - or use robust/tube or risk-sensitive planning variants (seen broadly in MPPI extensions) [[75]](https://www.sciencedirect.com/science/article/abs/pii/S1367578823000950).

---

### 6.3 “When to explore” becomes a scheduling problem in planners (mode switching)

VDSC’s “exploration triggers” are highly relevant to planners: many real systems need **mode switching** between:
- exploitation (trajectory tracking / goal-reaching),
- exploration (information gathering / system identification),
- safety recovery (backup controller).

In planning terms, VDSC-like triggers (value discrepancy + novelty counts) correspond to **online residual monitors** that decide when to invoke exploratory maneuvers rather than continuously perturbing the controls [[2]](https://arxiv.org/pdf/2403.17542v1).

This matches practical MPC/controls practice: excite the system only when needed, because excitation costs energy, risks constraint boundary interactions, and can degrade mission performance.

---

### 6.4 Constraints: from penalties to *hard shields* and *train-with-the-shield* designs

In trajectory planning, constraints are typically handled by:
- constrained optimization (hard constraints),
- soft penalties,
- or safety filters (edit controls online).

Safe RL work provides several sharpened lessons:

1) **Penalty methods can be brittle**
- Safe RL repeatedly documents that pure Lagrangian/penalty approaches can violate constraints during training and are sensitive to estimation error (especially off-policy cost underestimation) [[46]](https://arxiv.org/abs/2401.14758).

2) **Hard safety filters are modular and align with planning**
- Safety filters are naturally “planner-agnostic modules” that minimally modify proposed actions to ensure invariance (CBFs, reachability, predictive safety filters) [[83]](https://hybrid-robotics.berkeley.edu/publications/CSM2023_Safety_Filters.pdf), [[41]](https://calinbelta.com/wp-content/uploads/2023/12/Safe-exploration-in-model-based.pdf).
- This maps cleanly onto trajectory planning stacks: the planner proposes a trajectory; a safety filter projects it into a safe set (or replaces it with a safe fallback trajectory).

3) **But separation causes performance loss (“chattering”)**
- If the planner/policy is unaware of the filter, it may repeatedly propose infeasible trajectories, causing oscillations and poor performance [[43]](https://www.dynsyslab.org/wp-content/papercite-data/pdf/pizarro-ral25.pdf).
- Therefore, **train/optimize with the filter in the loop**—a principle shown to reduce chattering and improve final performance on the certified system [[43]](https://www.dynsyslab.org/wp-content/papercite-data/pdf/pizarro-ral25.pdf).

4) **Formal verification and reachability are becoming practical in some regimes**
- VELM’s symbolic-model + reachability shield suggests a direction where parts of the planning model are learned but kept verifiable, enabling long-horizon safety reasoning [[39]](https://herowanzhu.github.io/CAV_2024.pdf).
- This is a trajectory planning design pattern: learn *verification-friendly* surrogates for dynamics/constraints, then use reachability-based safe sets to prune trajectory search.

---

### 6.5 Risk-sensitive constraints (CVaR) map directly to risk-aware planning

Trajectory planning often needs to avoid rare but catastrophic outcomes (collision, tip-over, actuator saturation). Expectation constraints are a mismatch.

- CVaR-constrained RL methods explicitly address tail events [[44]](https://ieeexplore.ieee.org/document/10444044/), [[45]](https://link.springer.com/article/10.1007/s10994-022-06187-8).
- MPPI literature already includes **CVaR-based risk-aware MPPI** variants for motion planning under probabilistic uncertainty (surveyed in the path integral control review) [[75]](https://www.sciencedirect.com/science/article/abs/pii/S1367578823000950).

**Planning insight:**
- Safe RL’s migration to distributional/CVaR constraints is a strong signal that real trajectory planners should:
  - optimize not only expected cost but the tail of the cost distribution,
  - especially when uncertainty is epistemic (unknown unknowns) rather than purely aleatoric noise.

---

### 6.6 Belief-space planning is the “native” form of proactive exploration (and RL is feeding it)

For trajectory planning under partial observability, proactive exploration becomes *belief steering*: choose trajectories that reduce state uncertainty or reveal hidden variables.

- POMDP planning for object search (GPOMCP) explicitly tackles unknown environments and information-poor cases using belief-tree reuse and heuristic constructs [[27]](https://neurips.cc/virtual/2023/poster/72506).
- Active inference work formalizes epistemic value and scales it via tree search (AcT) [[29]](https://www.sciencedirect.com/science/article/pii/S0925231224020903), with analysis connecting epistemic objectives to Bayes-optimality approximations [[28]](https://arxiv.org/abs/2408.06542).
- GammaZero shows learning can provide heuristics that let belief-space search look deep enough to find information-gathering sequences and generalize to larger instances [[30]](https://arxiv.org/html/2510.14035v1).
- Safe fault estimation planning (s-FEAST) demonstrates belief-space tree search that selects informative trajectories *while enforcing probabilistic state constraints*, and explicitly notes that pure information gain without safety can crash [[79]](https://www.science.org/doi/10.1126/scirobotics.adn4722).

**Trajectory planning takeaway:**
- “Exploration” in real systems is often *active sensing with constraints*. The above results collectively support designing planners that jointly optimize:
  - task progress,
  - information gain,
  - and safety chance constraints.

---

### 6.7 Sampling-based trajectory optimization (MPPI) is converging with RL (and even diffusion) into a unified view

Several threads connect modern RL exploration to trajectory optimization:

- MPPI is a sampling-based receding-horizon trajectory optimizer; surveys highlight many safety/uncertainty-aware MPPI variants (uncertainty-averse, tube-based, CBF-safe, CVaR-risk-aware) [[75]](https://www.sciencedirect.com/science/article/abs/pii/S1367578823000950).
- A 2025 unification argues MPPI updates, policy gradients, and diffusion reverse sampling can be seen under a Gibbs-measure optimization lens [[76]](https://arxiv.org/html/2502.20476v1).
- Practical hybrids integrate RL with MPPI for obstacle avoidance and adaptation, attributing RL with exploration/learning and MPPI with short-term trajectory rollout optimization [[77]](https://onlinelibrary.wiley.com/doi/10.1002/rob.70006).

**Design implication for planners:**
- You can treat RL not as “the controller,” but as:
  - a learned proposal distribution (good samples),
  - a learned terminal cost/value function,
  - a learned feasibility classifier (“feasibility expert”),
  - or a learned world model for rollouts.
- The PoE “optimality expert × feasibility expert” approach is exactly this decomposition: keep MPPI as an optimality sampler, learn feasibility to avoid rejection sampling under constraints [[78]](https://arxiv.org/html/2412.17462v2).

---

### 6.8 Long-horizon structure: skills, subgoals, and trajectory “sketches” reduce sparse-reward difficulty

Sparse reward is fundamentally a long-horizon trajectory planning issue (find a narrow set of successful trajectories). RL work suggests two planning-relevant structural priors:

1) **Skill libraries / motion primitives**
- Skill discovery aiming at uniform coverage (LEADS) and disentanglement (DUSDi) provides composable primitives [[5]](https://proceedings.neurips.cc/paper_files/paper/2024/file/8997f9fff71e14a2c01bae9c6e226f44-Paper-Conference.pdf), [[6]](https://arxiv.org/html/2406.10127v1), [[7]](https://neurips.cc/virtual/2024/poster/94271).
- Planning can then operate in *skill space* (macro-actions), reducing horizon length and improving search.

2) **Trajectory-level guidance / “sketch then refine”**
- Diffusion trajectory-guided manipulation proposes generating a trajectory guide to reduce compounding errors in long-horizon tasks [[88]](https://arxiv.org/abs/2502.10040).
- This aligns with classical planning pipelines: generate a coarse plan, then refine with MPC; RL/generative models can provide the coarse plan distribution.

---

## 7) Practical synthesis: what this research suggests you should do for real trajectory planning problems

If you translate the 2023–Feb 2026 RL progress into trajectory planning design choices, the dominant actionable blueprint is:

1) **Use model-based rollouts (world models) whenever real interaction is costly or risky**, but:
   - explicitly track epistemic uncertainty (ensembles/Bayesian models),
   - propagate it across the horizon,
   - and adapt planning horizon/speed/aggressiveness to uncertainty [[72]](https://arxiv.org/html/2504.16680v3), [[73]](https://dl.acm.org/doi/abs/10.1109/TRO.2023.3339543), [[74]](https://arxiv.org/abs/2310.00760).

2) **Make exploration an explicit trajectory objective** (information gain / epistemic value), not a noise process:
   - add an information term (ensemble disagreement, expected entropy reduction),
   - schedule exploration using triggers/monitors rather than constant perturbation [[2]](https://arxiv.org/pdf/2403.17542v1), [[24]](https://proceedings.iclr.cc/paper_files/paper/2025/file/bd996108ed57d388866ca6deb7acf6cb-Paper-Conference.pdf).

3) **Enforce constraints with shields/filters or verification-friendly models when violations are unacceptable**, and:
   - train with the shield in the loop to avoid chattering and performance cliffs [[43]](https://www.dynsyslab.org/wp-content/papercite-data/pdf/pizarro-ral25.pdf), [[83]](https://hybrid-robotics.berkeley.edu/publications/CSM2023_Safety_Filters.pdf), [[39]](https://herowanzhu.github.io/CAV_2024.pdf).

4) **Prefer tail-risk-aware constraint formulations (CVaR/distributional) when rare events matter**, and integrate with risk-aware planning (e.g., CVaR-MPPI style) [[44]](https://ieeexplore.ieee.org/document/10444044/), [[45]](https://link.springer.com/article/10.1007/s10994-022-06187-8), [[75]](https://www.sciencedirect.com/science/article/abs/pii/S1367578823000950).

5) **Adopt hierarchical planning via skills/subgoals to beat sparse reward**, using skill discovery as a pretraining or library-building stage [[5]](https://proceedings.neurips.cc/paper_files/paper/2024/file/8997f9fff71e14a2c01bae9c6e226f44-Paper-Conference.pdf), [[6]](https://arxiv.org/html/2406.10127v1), [[7]](https://neurips.cc/virtual/2024/poster/94271), [[13]](https://arxiv.org/abs/2110.15191).

---

## 8) Open gaps and cautionary lessons (especially relevant for planning)

- **Intrinsic reward pathologies are real** (detachment, novelty loops). In planning, this corresponds to designing info/novelty objectives that do not over-prioritize stochastic distractors or irrelevant novelty [[1]](https://arxiv.org/abs/2302.10825), [[2]](https://arxiv.org/pdf/2403.17542v1), [[8]](https://neurips.cc/virtual/2024/poster/95344).
- **Safe RL without strong priors often needs warm-start data**; ActSafe’s warm-up for vision-world-model calibration reflects a common reality for safe deployment [[53]](https://arxiv.org/pdf/2410.09486).
- **Cost estimation error is a first-order failure mode** in constrained optimization; planners relying on learned cost models should adopt uncertainty-aware conservative updates or filtering [[46]](https://arxiv.org/abs/2401.14758), [[58]](https://arxiv.org/html/2407.04942v2).
- **Benchmarks show progress but not closure**: URLB remains unsolved by baselines [[13]](https://arxiv.org/abs/2110.15191), NeoRL-2 reports many offline RL methods fail to outperform behavior in realistic constrained tasks [[69]](https://arxiv.org/html/2503.19267v1), and new benchmark suites (Safety-Gymnasium, DSRL, SafeOR-Gym) exist largely because existing methods struggle across diverse constrained settings [[51]](https://proceedings.neurips.cc/paper_files/paper/2023/file/3c557a3d6a48cc99444f85e924c66753-Paper-Datasets_and_Benchmarks.pdf), [[66]](https://www.offline-saferl.org/), [[70]](https://arxiv.org/html/2506.02255v1).

---

### Bottom line for trajectory planning
Recent RL research is converging on a planning-native picture:

- Efficient exploration under sparse rewards is increasingly treated as **structured trajectory search** (return-and-expand, skills, coverage, world-model planning), not just stochastic action noise [[3]](https://arxiv.org/abs/1901.10995), [[5]](https://proceedings.neurips.cc/paper_files/paper/2024/file/8997f9fff71e14a2c01bae9c6e226f44-Paper-Conference.pdf), [[6]](https://arxiv.org/html/2406.10127v1), [[21]](https://proceedings.mlr.press/v202/sasso23a/sasso23a.pdf), [[22]](https://arxiv.org/pdf/2305.00477).
- Proactive constrained exploration is increasingly treated as **belief-/uncertainty-aware planning under hard or tail-risk constraints**, often with explicit shields/filters and with learning happening inside that constrained closed loop [[63]](https://arxiv.org/abs/2310.03225), [[43]](https://www.dynsyslab.org/wp-content/papercite-data/pdf/pizarro-ral25.pdf), [[39]](https://herowanzhu.github.io/CAV_2024.pdf), [[44]](https://ieeexplore.ieee.org/document/10444044/), [[79]](https://www.science.org/doi/10.1126/scirobotics.adn4722).
- For trajectory planning, the key insight is to **optimize trajectories for reward + information + risk**, and to **separate feasibility/safety enforcement from performance optimization**, while still training/optimizing with that enforcement in the loop to avoid brittle separation effects [[83]](https://hybrid-robotics.berkeley.edu/publications/CSM2023_Safety_Filters.pdf), [[43]](https://www.dynsyslab.org/wp-content/papercite-data/pdf/pizarro-ral25.pdf), [[78]](https://arxiv.org/html/2412.17462v2).
