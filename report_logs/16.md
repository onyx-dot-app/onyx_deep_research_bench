Query ID: 16
============================================================

QUESTION:
收集整理目前非接触式感知领域做的最好的算法策略，并为我评估他们的输入信号与准确率

============================================================

RESEARCH TASKS (6 total):
  #1: 请调研2023-2026年非接触式生命体征感知（呼吸/心率/睡眠）中表现最强的算法策略，覆盖至少mmWave/FMCW雷达、WiFi CSI/UWB两类；总结常用公开数据集与评价指标（MAE/RMSE/bpm误差、相关系数、Bland-Altman等），并给出代表性论文/开源项目的输入信号形式（IQ/Range-Doppler/点云/CSI维度等）与报道的准确率、测试条件与局限。优先检索顶会期刊/综述/基准数据集。
  #2: 请调研2023-2026年非接触式人体姿态/骨架估计与手势识别中表现最强的算法策略，重点在mmWave雷达（Range-Angle-Doppler/点云）与WiFi CSI两条线，也可涵盖RGB/深度/热成像用于对比或融合；汇总常用数据集（如mRI、MMBody、MiliPoint、RF-Pose相关、WiFi手势数据集等）与指标（MPJPE/PA-MPJPE、mAP、Acc/F1），并整理每类方法的输入信号规格与代表性SOTA结果/条件及代码资源。
  #3: 请调研2023-2026年非接触式行为识别/跌倒检测/人员计数与占用感知中表现最强的算法策略，覆盖mmWave雷达、WiFi、UWB/超宽带、热成像等模态；总结公开基准数据集与常用指标（Acc/F1/mAP、MAE计数、检测AP等），并提取代表性论文的方法范式（Transformer/自监督/多模态融合/领域自适应等）、输入信号格式与报告准确率、测试条件和部署代价。
  #4: 请补充调研2023-2026年mmWave/FMCW雷达在人类活动识别(HAR)、跌倒检测、人员计数/占用感知等任务上的SOTA算法与可比结果：重点关注公开数据集/基准（如MiliPoint、MM-Fi、IURHA/其他雷达跌倒数据集、单芯片雷达occupancy/segmentation基准等），提取典型输入表征（点云、Range-Doppler、micro-Doppler、raw ADC等）、常用指标（Acc/F1、ROC-AUC、MAE计数、IoU等）以及论文报告的最佳数值与测试条件/泛化设置，尽量包含代码或排行榜信息。
  #5: 请针对WiFi CSI在跌倒检测、呼吸/生命体征、人员计数/占用、行为识别等多任务场景补充2023-2026年最强基准与算法：重点检索CSI-Bench(NeurIPS 2025)的基线结果细节、WiAR/WiFlow等计数占用任务的评测协议与SOTA数值、以及常见WiFi跌倒检测/行为识别公开数据集与指标；同时提取典型CSI输入维度/采样配置（Intel 5300/AX200等、子载波数、MIMO天线、包率、窗口长度）与相应精度。
  #6: 为横向比较与部署评估，请调研2023-2026年非接触式感知在视觉/深度/热成像/激光雷达等模态上同类任务的强基线或SOTA（姿态估计、跌倒检测、占用/计数、生命体征如rPPG等），提取这些方法的输入规格（分辨率/帧率/热像阵列等）与关键准确率指标，以便与雷达/WiFi/UWB结果对齐，并说明隐私、遮挡、光照等限制。

============================================================

ANSWER:
下面按**任务**把 2023–2026 年“非接触式感知”里**最强/最值得复用的算法策略**系统整理，并对每类方法给出（尽可能从原文可核对的）**输入信号形式**与**报告准确率**；同时我会明确标注：哪些结果来自**公开基准**、哪些来自**作者自建数据/特定协议**、以及可比性风险点（例如划分协议、覆盖率、是否 in-the-wild、是否多目标等）。

---

## 0) 先说明：什么叫“最好”，以及为什么必须同时看“输入信号 + 指标 + 协议”

同一任务里，“最好”通常取决于你更看重哪一个维度：

- **纯精度**：MAE/RMSE、Acc/F1、AP/mAP、MPJPE 等；
- **可用性/覆盖率**：例如生命体征在真实睡眠里经常会出现“高精度但大量时间无法输出”的现象（coverage 很低）[[4]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12788250/)；
- **泛化**：跨人、跨房间/域、跨设备（不同 WiFi 芯片/子载波/天线）、跨姿态/角度；
- **工程代价**：输入数据带宽（raw ADC vs 点云）、模型大小/延迟、端侧可运行性；
- **隐私与部署边界**：摄像头/热成像 vs 雷达/WiFi/UWB。

所以我下面会尽量用“**方法范式（策略）→ 输入信号 → 指标与数值 → 测试条件/局限**”的格式来写。

---

## 1) 生命体征（呼吸/心率/睡眠呼吸事件）：目前最强的算法策略与可核对结果

### 1.1 mmWave/FMCW 雷达（呼吸/心率/睡眠）：强策略 = “空间单元融合 + 质量控制 +（必要时）波束/定位解耦”
#### 策略 A：**多 range-bin 选择/融合（multi-bin selection）**提升精度，但可能牺牲覆盖率
- **核心思想**：不要只盯一个 range bin；对多个候选 bin 做选择/融合，用一致性/信噪比/生理先验做质量控制，从而抑制多径、姿态变化、床上位移等导致的错误峰。
- **输入信号（可确认）**：FMCW 雷达按距离划分的 **range bins**（算法围绕 bin 选择对比）[[4]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12788250/)。
- **代表性结果（临床真实睡眠）**：卒中患者 49 人、与 PSG 对照的研究（Sensors, 2026）：
  - 呼吸 **MAE 0.39 brpm**（多 bin） vs 0.87（单 bin）
  - 心率 **MAE 0.84 bpm**（多 bin） vs 3.99（单 bin）
  - 相关系数：呼吸 0.95 vs 0.85；心率 0.96 vs 0.56  
  - **但 coverage 明显下降**：呼吸覆盖率 73.38%（多 bin） vs 93.49%；心率覆盖率 **19.93%（多 bin）** vs 81.85%（单 bin）[[4]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12788250/)  
- **你该怎么用这条结论**：如果你做的是“临床/长期连续监测”，只报 MAE 不够，必须同时报 coverage（或有效输出占比）[[4]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12788250/)。多-bin 在真实病患数据上能把 MAE 拉得很低，但如果 coverage 过低，临床价值会被严重打折。

#### 策略 B：**角度无关/波束聚焦 + 位置速度跟踪（Angular-Free / beam focusing after tracking）**提高房间内可用性
- **核心思想**：先做人体位置/速度跟踪，再把处理“聚焦到静止位置”提取微动，从而减少角度、走动、室内布局对生命体征估计的影响。
- **输入信号**：FMCW 雷达回波（论文描述含跟踪与波束聚焦，但当前摘录未给出更细张量规格）[[5]](https://www.mdpi.com/1424-8220/24/8/2448)。
- **代表性结果**（Sensors 2024 摘录可见）：
  - Bland–Altman 分析；平均误差：呼吸 **−0.06 BPM**，心率 **−1.5 BPM**[[5]](https://www.mdpi.com/1424-8220/24/8/2448)  
- **局限**：当前上下文摘录缺少受试规模/距离/遮挡/姿态细分结果，因此更适合作为“方法范式”参考，而不是精度对标的终局证据[[5]](https://www.mdpi.com/1424-8220/24/8/2448)。

#### 策略 C：**给足“可学习输入层级”——从 raw ADC 到 range map 到胸部位移**
- **你会看到的新趋势**：数据集开始同时提供多层级表示，方便比较“端到端 raw 学习”与“传统信号处理+学习”的权衡。
- **代表性公开数据资源**：4TU 发布的 mmWave FMCW 生命体征数据集（10 人，包含极端生理场景：升高心率、哮喘、冥想等）：
  - **ADC samples（原始采样）**
  - **range maps**
  - **chest displacement signals（胸部位移）**
  - 并由 Polar H10 对照验证[[7]](https://research.utwente.nl/en/datasets/mm-wave-fmcw-radar-vital-sign-monitoring-dataset-diverse-physiolo/)[[8]](https://arxiv.org/abs/2405.12659)  
- **如何评估输入信号**：
  - *ADC/raw*：信息最全、也最重；适合训练基础模型或端到端网络，但数据量/标注/算力成本高；
  - *range map*：工程最常用（便于做目标选择、去静态杂波、相位跟踪）；
  - *chest displacement*：最轻、最接近“生命体征波形”，但把上游鲁棒性问题前置到信号处理阶段（上游一旦错，学习也救不回来）。

> 综述层面也强调：雷达生命体征机器学习需要更标准的评估协议、交叉验证与指标报告（并指出“人口学多样性→泛化”强相关、运动伪影与多目标解耦仍是难点）[[1]](https://www.sciencedirect.com/science/article/pii/S0263224125010668)。

---

### 1.2 WiFi CSI（心率/呼吸）：强策略 = “相位可用化 + 子载波选择 + 多子载波融合”
WiFi 做生命体征的核心难点是：**心跳引起的形变非常微弱**，CSI 里有效成分容易被环境与设备噪声淹没[[9]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11013971/)。因此很多“最强策略”并不是堆大模型，而是把**信号增强/选择**做扎实。

#### 策略：**幅度+相位融合的旋转投影（rotational projection）+ HSR 子载波选择 + 多子载波峰值融合**
- **输入信号**：WiFi **CSI 幅度与相位**（明确使用 amplitude and phase）[[9]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11013971/)
- **算法要点**：
  - 用 rotational projection 融合幅度/相位，提高心跳可见度
  - 用 Heartbeat-to-subcomponent ratio (HSR) 在频域选子载波
  - 融合多个子载波的峰值频率估计心率[[9]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11013971/)
- **结果（Sensors 2024 摘录可见）**：
  - 平均准确率 **96.8%**
  - 中位数误差 **0.8 bpm**
  - 作者声称较现有技术约提升 20%（但当前上下文缺少可核对的统一基线协议细节，因此建议只把它当作作者报告）[[9]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11013971/)
- **你该怎么用**：如果你要做“低数据量、可部署”的 WiFi 心率，这类“信号选择+融合”的方法往往比端到端黑盒更稳（尤其是跨房间/跨设备时）。

---

### 1.3 UWB（睡眠呼吸事件/呼吸暂停严重度）：强策略 = “定位/状态剖面 + 注意力事件识别 + 对比学习泛化 + 投票纠错”
#### Respnea（ACM TOSN）：把“呼吸率估计”升级为“秒级事件识别 + SAHS 严重度诊断”
- **输入信号**：UWB 信号的**幅度与相位**（明确提到用于床位定位、状态识别与呼吸率估计）[[2]](https://dl.acm.org/doi/10.1145/3680550)
- **策略组合**：
  1) **profiling**：非受控环境下定位睡眠位置、识别受试者状态  
  2) **multi-head self-attention**：从呼吸信号学习隐含模式，做**秒级**呼吸事件区分  
  3) **contrastive learning**：学更鲁棒表征提升泛化  
  4) **multi-window voting**：纠错并聚合为 apnea/hypopnea 事件[[2]](https://dl.acm.org/doi/10.1145/3680550)
- **结果（医院+家庭；健康人与患者）**：
  - 呼吸率 **median error 0.27 bpm**
  - SAHS 严重度诊断准确率 **94.44%**[[2]](https://dl.acm.org/doi/10.1145/3680550)
- **意义**：在“睡眠障碍”里，真正有临床价值的往往是事件检测与严重度分级，而不仅是 RR 平均误差。Respnea 是目前你这类需求里最“系统化 + 有医学任务闭环”的范式之一[[2]](https://dl.acm.org/doi/10.1145/3680550)。

---

### 1.4 热成像 / RGB rPPG（对比与融合参考）
如果你要做跨模态对比（隐私 vs 精度）：

- **热成像 RR（深度学习，端到端）**：22 人、戴/不戴口罩、坐/站等四条件，RR 误差 **1.6 brpm**（SOTA 声称）[[75]](https://www.mdpi.com/1424-8220/24/19/6386)  
- **热成像 RR（端侧实时系统）**：TOPDON TC001 + Jetson Orin Nano；YOLO 隔帧检测（stride=2）+ Kalman 跟踪 ROI；但当前摘录中 MAE/RMSE 数值被截断，无法填表[[76]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12788367/)  
- **rPPG 工具箱**：提供大量传统与深度方法基线与数据集入口，适合你做“视觉生命体征”对照实验，但 README 摘录未给统一 SOTA 表[[77]](https://github.com/ubicomplab/rPPG-Toolbox)  
- **ME-rPPG（低延迟轻量）**：报告跨数据集 MAE（MMPD 5.38、VitalVideo 0.70、PURE 0.25；摘要未写明单位，常见是 bpm 但这里必须标注口径风险），并给出**内存 3.6MB、延迟 9.46ms**[[78]](https://arxiv.org/pdf/2504.01774)  
- **RhythmMamba**：强调 Mamba/状态空间模型用于 rPPG 的吞吐/显存优势，但当前摘录未给 HR MAE 等数值[[79]](https://ojs.aaai.org/index.php/AAAI/article/view/33204/35359)

---

### 1.5 生命体征方法对照表（输入信号 vs 精度/指标）

| 模态/任务 | 代表策略（强点） | 输入信号（你能拿到/做得到的形式） | 指标与结果（摘录可核对） | 关键局限/注意 |
|---|---|---|---|---|
| FMCW 雷达睡眠 HR/RR | 多 range-bin 选择/融合 + 质量控制 | range bins 序列[[4]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12788250/) | RR MAE 0.39；HR MAE 0.84；但 HR coverage 19.93%[[4]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12788250/) | 精度-覆盖率权衡非常突出[[4]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12788250/) |
| FMCW 雷达 HR/RR | 角度无关 + 跟踪后聚焦提取 | 雷达回波（细节缺失）[[5]](https://www.mdpi.com/1424-8220/24/8/2448) | BR 平均误差 −0.06；HR 平均误差 −1.5（BA分析）[[5]](https://www.mdpi.com/1424-8220/24/8/2448) | 缺少完整协议细节[[5]](https://www.mdpi.com/1424-8220/24/8/2448) |
| WiFi 心率 | 幅相融合旋转投影 + HSR 子载波选择 + 多子载波融合 | CSI 幅度+相位[[9]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11013971/) | 平均准确率 96.8%；中位误差 0.8 bpm[[9]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11013971/) | CSI 维度/设备配置未在摘录给出[[9]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11013971/) |
| UWB 睡眠呼吸事件 | profiling + attention + 对比学习 + 投票 | UWB 幅度+相位[[2]](https://dl.acm.org/doi/10.1145/3680550) | RR 中位误差 0.27 bpm；SAHS 严重度 94.44%[[2]](https://dl.acm.org/doi/10.1145/3680550) | 未给 CIR/range-bin 等更细输入格式[[2]](https://dl.acm.org/doi/10.1145/3680550) |
| 热成像 RR | ROI检测 + 3D CNN + BiLSTM + 相位差感知loss | 热视频（ROI序列）[[75]](https://www.mdpi.com/1424-8220/24/19/6386) | RR 误差 1.6 brpm[[75]](https://www.mdpi.com/1424-8220/24/19/6386) | 相机规格/帧率/分辨率摘录缺失[[75]](https://www.mdpi.com/1424-8220/24/19/6386) |

---

## 2) 姿态/骨架估计与手势：目前最强的算法策略与输入信号评估

### 2.1 mmWave 雷达姿态估计：强策略 = “热图/多维 FFT 表示 + 显式位置编码/概率图 + 注意力融合”
#### 策略 A：**多格式特征融合（FFT 热图 + 概率图/位置编码）**
- **代表方法**：ProbRadarM3F（2024）
- **输入信号链路（描述很具体，便于你复现同类管线）**：
  - 原始 **ADC** → radar data cube  
  - **4D-FFT 分支**生成：
    - range–Doppler–azimuth heatmaps
    - range–Doppler–elevation heatmaps  
  - 并行 **概率图 + positional encoding（ProbPE）**分支提取位置信息  
  - Cross-/Self-Attention 做多帧多格式融合，输出 14 关键点[[14]](https://arxiv.org/html/2405.05164v2)
- **结果**：在 HuPR 数据集上 **AP = 69.9%**（并声称优于其他方法）[[14]](https://arxiv.org/html/2405.05164v2)
- **输入信号评价**：
  - *优点*：热图保留了 Doppler/角度维，有利于关节微运动与肢体分离；
  - *代价*：FFT 热图通常数据量大、冗余高；端侧算力压力更大；
  - *工程关键*：作者强调“点云生成依赖手工参数且丢 Doppler/部分位置信息；热图有冗余”，因此其思路是“把位置显式补回去”[[14]](https://arxiv.org/html/2405.05164v2)。

#### 策略 B：**跨模态伪标签训练（相机姿态网络监督雷达）+ 注意力模块 + 图卷积细化**
- **代表方法/数据集**：HuPR（mmWave + RGB，同步标定）
- **输入信号**：mmWave 雷达特征（含速度信息的预处理、多尺度雷达特征）+ 训练阶段来自单目 RGB 姿态网络生成的 2D 关键点伪标签[[13]](https://arxiv.org/abs/2210.12564)
- **网络策略**：CSAM（cross/self-attention 融合多尺度）+ PRGCN（图卷积细化关键点置信热图）[[13]](https://arxiv.org/abs/2210.12564)
- **评价**：当前摘录未给出 HuPR 的量化表（但 ProbRadarM3F 在 HuPR 上给了 AP=69.9%）[[14]](https://arxiv.org/html/2405.05164v2)[[13]](https://arxiv.org/abs/2210.12564)。

#### 策略 C：**可穿戴/自运动解耦（egocentric mmWave）+ 人体运动学先验两阶段估计**
- **代表方法**：mmEgo（SenSys）
- **输入信号**：头戴式 mmWave 雷达（细节张量未在摘录中给出）
- **策略**：root-relative 雷达运动跟踪用于自运动解耦；两阶段估计器引入运动学先验；摘要声称关节定位误差降低 44.2%[[16]](https://dl.acm.org/doi/10.1145/3625687.3625799)  
- **可用性**：更适合移动/第一视角交互场景，但需要你额外处理“雷达自运动”这一类问题。

---

### 2.2 WiFi CSI 姿态估计：强策略 = “多设备多天线 CSI 张量化 + Transformer/集合预测 + 相位去噪 +（可选）自监督预训练”
#### Person-in-WiFi 3D（CVPR 2024）：多人 3D 姿态端到端集合预测
这篇是目前上下文里**输入规格写得最清楚**、也最适合作为你“CSI 输入设计”模板的工作之一。

- **任务**：WiFi 多人 3D 姿态估计（首次公开多人 3D WiFi 姿态数据集的声明）[[19]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Person-in-WiFi_3D_End-to-End_Multi-Person_3D_Pose_Estimation_with_Wi-Fi_CVPR_2024_paper.pdf)
- **硬件与采样**：
  - Intel 5300；1 发射端 + 3 接收端（放感知区域角落）；每接收端 3 天线  
  - 5.64GHz（channel 128），30 子载波；发射端 1 天线；**300 packets/s**[[19]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Person-in-WiFi_3D_End-to-End_Multi-Person_3D_Pose_Estimation_with_Wi-Fi_CVPR_2024_paper.pdf)
- **CSI 输入张量（非常关键）**：
  - CSI samples 尺寸：**1 × 3 × 3 × 30 × 20 × 2**（含幅度与去噪相位；维度含义为 #TX,#RX,#ANT,#subcarrier,#time）  
  - 幅度+相位拼接后 token 维度变 60；将 (#TX×#RX×#ANT×#time) 展平成 180 tokens，每 token 30 子载波，拼接后 60，再 FC 到 256，并加空间-时间 embedding，送入 6 层 encoder[[19]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Person-in-WiFi_3D_End-to-End_Multi-Person_3D_Pose_Estimation_with_Wi-Fi_CVPR_2024_paper.pdf)
- **指标与结果（3D 关节定位误差）**：
  - 单人：**91.7 mm**
  - 两人：**108.1 mm**
  - 三人：**125.3 mm**[[19]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Person-in-WiFi_3D_End-to-End_Multi-Person_3D_Pose_Estimation_with_Wi-Fi_CVPR_2024_paper.pdf)
- **输入信号评价**：
  - *优势*：CSI 张量+Transformer 能直接学“多人集合输出”，避免“先检测人数再回归姿态”的繁琐 pipeline；
  - *代价*：多接收端部署成本更高；对同步/时钟/相位稳定性要求高（论文用 PhaseFi 线性变换做相位去噪）[[19]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Person-in-WiFi_3D_End-to-End_Multi-Person_3D_Pose_Estimation_with_Wi-Fi_CVPR_2024_paper.pdf)。

#### 其他 WiFi 姿态范式（当前上下文能核对的点）
- **CSI-Former**：Performer + CNN encoder-decoder；教师网络用 AlphaPose 从图像生成骨架监督，训练后仅 CSI 推理；并建立并公开 Wi-Pose 数据集（5GHz CSI + 图像 + 骨架）[[20]](https://www.mdpi.com/1099-4300/25/1/20)。但当前摘录未给出 MPJPE/mAP 数值[[20]](https://www.mdpi.com/1099-4300/25/1/20)。
- **MetaFi**：跨模态监督；摘要报告 **PCK@50 = 95.23%**[[21]](https://arxiv.org/abs/2208.10414)（需注意 PCK@50 的定义与数据集协议不同于 MPJPE，跨论文可比性有限）。
- **CRPose**：用“压缩表示”兼顾精度与效率；摘要报告 14 关节平均误差 **5.1 cm**[[22]](https://ieeexplore.ieee.org/document/10551004/)（同样缺少协议细节，建议只作方向参考）。
- **DP-Pose（2025）**：自监督 MAE 式 masked pre-training + 时间对比学习 + uniformity 正则；解码用 GCN（拓扑约束）+ Transformer（全局依赖）提升骨架结构合理性；但当前摘录未给具体数值[[23]](https://arxiv.org/html/2501.09411v1)。

---

### 2.3 手势识别：输入信号“可部署性”比网络更关键（Widar/微多普勒/点云序列）
你若做手势，常见输入有三类：

1) **WiFi CSI / Doppler 特征**  
   - Widar3.0：258K 手势实例、75 domains；提供 RSSI/CSI 与衍生特征 DFS（Doppler frequency shift）与 BVP（Body-coordinate velocity profile）[[25]](https://ieee-dataport.org/open-access/widar-30-wifi-based-activity-recognition-dataset)。  
   - 这种数据最适合做跨域泛化研究（domain 很多），但你必须面对不同域设备/环境造成的分布差。

2) **mmWave micro-Doppler 时频图**（把“时序运动”转换成类图像识别）  
   - LPVT（2024）给出了标准 pipeline：多通道非相干合成 → 静态杂波滤除 → range FFT → Doppler FFT → 投影到速度维形成 micro-Doppler time spectrogram 作为输入；并自建 77GHz AWR1642 数据集 MWRGR（12 人×6手势×30次=2160样本，距离 20–30cm）[[34]](https://www.mdpi.com/2072-4292/16/23/4602)。  
   - 但当前摘录没给最终 Acc/F1，因此它更多是“输入构造+网络结构”的范式参考[[34]](https://www.mdpi.com/2072-4292/16/23/4602)。

3) **mmWave 点云/点云特征序列**  
   - IEEE DataPort 的 MIMO 雷达点云手势数据集提供可直接 reshape 的输入：例如 `[-1, 20, 8, 1]`（TIME_LEN=20，FEA_NUM=8）[[26]](https://ieee-dataport.org/documents/dataset-article-gesture-recognition-using-mimo-radar-point-clouds-targeted-signal)。这类输入非常工程友好：固定时间长度、固定特征维，适合轻量 CNN/RNN/Transformer。

---

### 2.4 姿态/骨架/手势的数据集与“输入信号规范”要点（你做表格时可直接抄的关键信息）
- **mmBody（3D body reconstruction）**：点云每点包含 **3D 位置 + range velocity + amplitude + energy power**；坐标轴定义与标定矩阵给得很细；但数据需邮件申请且有非商业限制[[11]](https://github.com/Chen3110/mmBody)。
- **mmFree-Pose（隐私优先 mmWave 姿态）**：点云每点 **(x,y,z,velocity,intensity)** 五维；提供 **23 关节**标注；3 名受试、4534 对齐样本；强调完全不使用视觉传感器，靠动捕服标注[[15]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12383697/)。
- **mRI（多模态康复姿态）**：>500 万帧；mmWave + RGB-D + IMU；但当前仅先发布不含相机模态版本（隐私原因），已发布 keypoints 与动作标签[[10]](https://sizhean.github.io/mri)。
- **MM-Fi（NeurIPS 2023）**：5 模态同步（含 mmWave、WiFi CSI 等），40 人、>320k 同步帧、4 domains；工具箱支持 modality 组合、protocol、sequence/frame 等配置；raw RGB/infra 不公开但提供 17 keypoints[[24]](https://arxiv.org/abs/2305.10345)[[40]](https://github.com/ybhbingo/MMFi_dataset)[[38]](https://ntu-aiot-lab.github.io/mm-fi)。
- **Wi-Pose（CSI-Former）**：5GHz CSI + 图像 + 骨架，已公开[[20]](https://www.mdpi.com/1099-4300/25/1/20)。
- **Person-in-WiFi-3D**：给出了最完整的 CSI 张量维度与包率设置（300 pkt/s、30 子载波等）[[19]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Person-in-WiFi_3D_End-to-End_Multi-Person_3D_Pose_Estimation_with_Wi-Fi_CVPR_2024_paper.pdf)。

---

## 3) 行为识别 / 跌倒检测 / 占用与人数（计数）：最强策略与可核对结果

这一块“最好”的定义尤其依赖：你追求的是**实验室精度**还是**跨环境可部署**。我会把两类分别列出。

---

## 3.1 WiFi CSI：从“实验室小数据集”走向“in-the-wild 基准 + 多任务学习 + 跨域自适应”

### (A) 最重要的新基准：CSI-Bench（NeurIPS 2025）——真实环境、多芯片异构、多任务、标准划分
这是目前上下文里对“WiFi 非接触感知”最关键的一条：它把“论文里可控环境的高分”与“真实部署泛化”之间的鸿沟，变成可被系统评测的问题[[3]](https://neurips.cc/virtual/2025/poster/121605)[[54]](https://arxiv.org/abs/2505.21866)[[55]](https://arxiv.org/pdf/2505.21866)[[56]](https://arxiv.org/html/2505.21866v2)。

- **规模与覆盖**：>461 小时、35 用户、26 室内环境、16 种设备配置[[3]](https://neurips.cc/virtual/2025/poster/121605)[[55]](https://arxiv.org/pdf/2505.21866)[[56]](https://arxiv.org/html/2505.21866v2)  
- **硬件异构性（非常关键）**：来自 Qualcomm/MediaTek/Broadcom/Espressif/NXP 等商用设备；802.11n/ac/ax；2.4/5GHz；20/40/80MHz；天线从 1×1 到 2×2、1×4 等；子载波数也随设备变化（例如 Qualcomm 40MHz 128 子载波、80MHz 256；NXP 40MHz 58 子载波；ESP32 64 子载波；BCM4345 20/40MHz 14/28 子载波）[[55]](https://arxiv.org/pdf/2505.21866)[[56]](https://arxiv.org/html/2505.21866v2)  
- **采样率（sounding rate）**：一般任务 100Hz；呼吸 30Hz[[55]](https://arxiv.org/pdf/2505.21866)[[56]](https://arxiv.org/html/2505.21866v2)
- **官方 baseline（项目页直接给表）**：指标为 Acc/F1（3 次运行 mean±std）[[57]](https://ai-iot-sensing.github.io/projects/project.html)  
  - **跌倒检测**：最佳（在表内）为 LSTM **94.93±0.51 Acc / 94.92±0.50 F1**；ResNet-18 94.88±0.26 / 94.89±0.26[[57]](https://ai-iot-sensing.github.io/projects/project.html)  
  - **呼吸检测**：PatchTST **98.84±0.13 / 98.84±0.13**（最高）[[57]](https://ai-iot-sensing.github.io/projects/project.html)  
  - **房间级定位**：ResNet-18 与 TimeSformer-1D 均 **100.00±0.00 / 100.00±0.00**[[57]](https://ai-iot-sensing.github.io/projects/project.html)  
  - **运动来源识别**：ResNet-18 **99.56±0.07 / 99.56±0.07**[[57]](https://ai-iot-sensing.github.io/projects/project.html)
- **多任务学习增益（同一套共享数据）**：Transformer 在 HAR 与 Proximity 上提升显著：  
  - HAR：75.40±0.93 → **87.79±0.00**（+12.39）  
  - Proximity：77.52±3.13 → **87.85±0.00**（+10.33）[[57]](https://ai-iot-sensing.github.io/projects/project.html)
- **输入张量规范（来自官方代码配置）**：baseline 默认 `win_len=500`、`feature_size=232`[[59]](https://github.com/Jenny-Zhu/CSI-Bench-Real-WiFi-Sensing-Benchmark/)（232 的物理含义在摘录中未解释，所以你做复现时要回到代码的数据预处理逻辑去确认）。

> 结论：如果你问“WiFi 做跌倒/呼吸/定位现在最强的可比证据是什么”，CSI-Bench 给了一个非常强的答案：它不是某个模型单点最强，而是**可泛化、可复现、可对比**的评测底座[[3]](https://neurips.cc/virtual/2025/poster/121605)[[57]](https://ai-iot-sensing.github.io/projects/project.html)。

---

### (B) 跌倒检测：强策略 = “环境无关特征 +（可选）Transformer 时空注意力 + 跨模态监督”
#### XFall（IEEE JSAC 2024）：一年真实部署 + 跨 70 设置评测的“环境无关”范式
- **策略**：
  - 环境无关特征 **SDP（speed distribution profile）**
  - 空间-时间注意力 Transformer 编码器 **STATE**
  - 跨模态学习框架 **CURL** 借助预训练视觉模型监督，缓解 WiFi 标注不足[[64]](https://tns.thss.tsinghua.edu.cn/~guoxuan/assets/pdf/Paper-XFall.pdf)
- **平台**：华为 AX3 Pro（802.11ax / Wi-Fi 6）[[64]](https://tns.thss.tsinghua.edu.cn/~guoxuan/assets/pdf/Paper-XFall.pdf)
- **结果（强调真实世界泛化）**：
  - Accuracy **96.8%**
  - Miss Alarm Rate (MAR) **3.1%**
  - False Alarm Rate (FAR) **3.3%**
  - 场景：一年部署、跨 70 个真实设置[[64]](https://tns.thss.tsinghua.edu.cn/~guoxuan/assets/pdf/Paper-XFall.pdf)

#### DeFall：速度/加速度“生理模式” + DTW 模板匹配（强调无需环境重训练）
- **策略**：用速度/加速度序列与模板相似度（增强 DTW）判别跌倒，强调速度/加速度是“人体固有属性”因此环境无关，并声称 LOS/NLOS 可用[[65]](https://www.originwirelessai.com/wp-content/uploads/2024/04/DeFall.pdf)
- **报告结果**：真实跌倒检测率 >95%，FAR=1.47%；并在表中汇总对比多个方法的“claimed performance”[[65]](https://www.originwirelessai.com/wp-content/uploads/2024/04/DeFall.pdf)  
- **注意**：该对比表跨论文口径不同；应当把这些数字视为作者总结的“主张”，严格对标需统一协议[[65]](https://www.originwirelessai.com/wp-content/uploads/2024/04/DeFall.pdf)。

---

### (C) 占用/人数（计数）：强策略 = “自监督预训练 + 无监督域适配 + 参数高效微调（Adapter/LoRA）+ 状态机计数”
- **代表工作（2026 arXiv）**：Parameter-Efficient Domain Adaption for CSI Crowd-Counting…[[31]](https://arxiv.org/html/2601.02203v1)
- **输入信号**：WiFi CSI（论文围绕跨域自适应与 PEFT 设计，摘录未给子载波/天线细节）[[31]](https://arxiv.org/html/2601.02203v1)
- **可核对结果**：
  - 公共 WiAR：作者报告达到 **98.8% Accuracy 与 98.8% F1**（并声称新 SOTA）[[31]](https://arxiv.org/html/2601.02203v1)
  - WiFlow（作者私有数据集）：10-shot 最终占用计数 **MAE=0.44**（摘要）[[31]](https://arxiv.org/html/2601.02203v1)
  - **部署代价（很有工程价值）**：
    - Full fine-tune 更新 **1,092,806** 参数 → Acc 99.67%
    - Adapter fine-tune 仅更新 **30,438** 参数 → Acc 98.84%  
    - 以 <1% 性能损失换来 97.2% 可训练参数减少[[31]](https://arxiv.org/html/2601.02203v1)
- **注意**：WiFlow 的公开性与评测协议在当前上下文主要来自摘要与第三方页面转述，严格复现需回到论文正文/数据集说明核验[[31]](https://arxiv.org/html/2601.02203v1)[[62]](https://www.emergentmind.com/topics/wiflow-dataset)。

---

## 3.2 mmWave 雷达：HAR/动作识别/点云基准已经很强，但“公开可比的 SOTA 表”仍不如 WiFi 完整

### (A) MiliPoint（NeurIPS 2023 Datasets & Benchmarks）：mmWave 点云 HAR 的可复现基准底座
如果你要做“雷达点云动作识别”的严谨对标，MiliPoint 是当前上下文里**最完整（输入→预处理→划分→指标→数值→代码）**的一套[[42]](https://proceedings.neurips.cc/paper_files/paper/2023/file/c60468eca9cd0b0083f0ff9d0aeb171a-Paper-Datasets_and_Benchmarks.pdf)[[45]](https://github.com/yizzfz/MiliPoint)。

- **硬件与点云生成**：TI IWR1843（77–81GHz，3Tx4Rx，4GHz 带宽，距离分辨率 4cm）；片上 DSP 输出点云；CFAR range/doppler 阈值均 10dB（经验设置）[[42]](https://proceedings.neurips.cc/paper_files/paper/2023/file/c60468eca9cd0b0083f0ff9d0aeb171a-Paper-Datasets_and_Benchmarks.pdf)
- **输入信号形式（极其明确）**：
  - 单帧点集：\(d \in \mathbb{R}^{N \times 3}\)，每点 (x,y,z)
  - 固定点数：上限 k，超出随机采样、不足 zero padding → \(\hat d \in \mathbb{R}^{k \times 3}\)
  - 堆叠 s 帧：\(d \in \mathbb{R}^{s \times k \times 3}\)  
  - 动作分类用 **s=50**（作者解释动作持续 1–2 秒，帧率 24FPS，所以需要更长序列）[[42]](https://proceedings.neurips.cc/paper_files/paper/2023/file/c60468eca9cd0b0083f0ff9d0aeb171a-Paper-Datasets_and_Benchmarks.pdf)
- **划分与指标**：80/10/10 train/val/test；动作分类 Top1/Top3 Acc；多次随机种子取均值±方差[[42]](https://proceedings.neurips.cc/paper_files/paper/2023/file/c60468eca9cd0b0083f0ff9d0aeb171a-Paper-Datasets_and_Benchmarks.pdf)
- **基线结果（动作分类 Top1/Top3）**：
  - PointNet++：**Top1 34.45±0.80%**；Top3 54.96±1.21%（表内最佳）[[42]](https://proceedings.neurips.cc/paper_files/paper/2023/file/c60468eca9cd0b0083f0ff9d0aeb171a-Paper-Datasets_and_Benchmarks.pdf)
  - Pointformer：Top1 29.27±0.55；Top3 50.44±1.18[[42]](https://proceedings.neurips.cc/paper_files/paper/2023/file/c60468eca9cd0b0083f0ff9d0aeb171a-Paper-Datasets_and_Benchmarks.pdf)
  - DGCNN：Top1 13.61±2.09；Top3 34.59±2.74[[42]](https://proceedings.neurips.cc/paper_files/paper/2023/file/c60468eca9cd0b0083f0ff9d0aeb171a-Paper-Datasets_and_Benchmarks.pdf)
- **解读**：在该设定下，点云动作分类远低于识别/关键点任务，说明“点云 HAR”对序列建模、数据规模与多样性非常敏感[[42]](https://proceedings.neurips.cc/paper_files/paper/2023/file/c60468eca9cd0b0083f0ff9d0aeb171a-Paper-Datasets_and_Benchmarks.pdf)。

### (B) MM-Fi + 对比学习式动作识别：在另一种协议下能到更高准确率，但可比性取决于划分
- **SMC-HAR（2026，摘要可见）**：针对稀疏 mmWave 点云（摘要提到每帧 20–70 点），用 supervised contrastive + momentum 稳定特征空间，并做 domain-specific augmentation[[41]](https://www.sciencedirect.com/science/article/abs/pii/S1574119225001208)
- **结果**：在 MM-Fi 上分类准确率 **88.40%**，并称较纯交叉熵 baseline 提升 8.40%[[41]](https://www.sciencedirect.com/science/article/abs/pii/S1574119225001208)
- **关键可比性风险**：摘要说明 MM-Fi 采用随机 8:2 划分训练/测试[[41]](https://www.sciencedirect.com/science/article/abs/pii/S1574119225001208)，而 MM-Fi 工具箱支持多种 protocol/split/cross-domain 设置[[40]](https://github.com/ybhbingo/MMFi_dataset)。因此该 88.40% 不能直接与“跨域/跨人协议”的结果对标，使用时必须写明划分。

---

## 3.3 室内占用/场景理解（单静态雷达）：强策略 = “多径几何建模 + 生成式补全（扩散）”或“raw radar 基础模型”
### (A) RISE（2025）：单静态雷达室内场景理解基准 + 多径增强 + 扩散式 sim2real
- **任务**：layout reconstruction + object detection
- **数据规模**：50,000 帧，100 条真实室内轨迹[[51]](https://arxiv.org/abs/2511.14019)
- **报告结果**：
  - layout reconstruction Chamfer Distance 降到 **16 cm**（并称较 SOTA 降 60%）
  - object detection 达到 **58% IoU**（摘要表述为“first mmWave-based object detection”）[[51]](https://arxiv.org/abs/2511.14019)
- **注意**：摘要未给输入表征是 raw/RD/点云等，也未给 IoU 定义细节；但从“单静态雷达 + 多径”这条路线看，它是“输入信号物理结构建模”非常强的范式参考[[51]](https://arxiv.org/abs/2511.14019)。

### (B) 单芯片雷达基础模型（ICCV 2025）：强策略 = “Transformer 基础模型 + 坚持 raw radar 输入”
- **GRT（Generalizable Radar Transformer）**：收集 100 万样本（约 29 小时）的 raw radar 数据，训练基础模型预测 3D occupancy 与 semantic segmentation；摘要强调：**raw radar 显著优于常用有损表示**，效果相当于训练数据增加 10 倍；并提出数据扩展规律（每 10× 数据约 20% 提升，指标未在摘要中定义）[[33]](https://arxiv.org/abs/2509.12482)
- **意义（对你做输入设计非常关键）**：当你纠结“要不要把 raw→RD→点云”时，这篇给出强烈信号：**有损表示可能是性能上限的硬瓶颈**；但代价是数据量与算力成本暴涨[[33]](https://arxiv.org/abs/2509.12482)。

---

## 3.4 热成像 / 红外视频（作为“隐私友好但非 RF”的跌倒检测强对照）
如果你需要“非 RF 的隐私方案”：

- **热成像跌倒（2026 arXiv）**：BiConvLSTM + 注意力 + motion flow；在 TSF 数据集上 **ROC-AUC 99.7%**；基线 3D-CNN 输入张量维度为 **(bb, tt, 256, 256, 1)**（单通道热帧序列）[[74]](https://arxiv.org/html/2509.16479v2)  
- **TF-66 热成像跌倒数据集**：强调多环境与参与者多样性，数据按请求提供[[36]](https://www.sciencedirect.com/science/article/pii/S0952197625018214)  
- **红外骨架 + ST-GCN 跌倒**：综述型来源给出若干引用结果，如 2D 骨架 ST-GCN 跌倒识别准确率 94.1%、并可达 23 FPS；TensorRT 加速 AlphaPose+ST-GCN 在 UR/Le2i 数据集分别 97.28%/96.86%（但这些是该文转述引用，严格对比需回原论文统一协议）[[71]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11280873/)  
- **YOLO+HRNet（2025）**：遮挡场景跌倒检测准确率 >95%，帧率 18.1 fps[[72]](https://www.mdpi.com/1424-8220/25/13/4128)（其“accuracy”口径需注意，摘要未给 mAP/召回等定义）。

---

## 4) 最关键的“输入信号”评估：你选什么输入，往往决定上限与可部署性

下面是你做系统对照表时最实用的一张“输入信号—信息量—工程代价”速查（结合上述已出现的工作）：

### 4.1 RF/雷达类输入的主流层级（从“信息最全”到“最轻”）

1) **raw ADC / raw radar（I/Q 或 ADC samples）**  
   - 信息量最大；可学到更通用的表征  
   - 代价：数据吞吐/存储/训练成本最高  
   - 趋势证据：基础模型工作强调 raw radar 显著优于有损表示[[33]](https://arxiv.org/abs/2509.12482)；生命体征数据集也开始直接提供 ADC samples[[7]](https://research.utwente.nl/en/datasets/mm-wave-fmcw-radar-vital-sign-monitoring-dataset-diverse-physiolo/)

2) **Radar data cube / Range–Doppler / Range–Angle / 多维 heatmap（FFT 后张量）**  
   - 强项：保留 Doppler/角度等对人体结构与运动很关键的维度  
   - 代表：ProbRadarM3F 的 4D-FFT heatmaps + attention 融合[[14]](https://arxiv.org/html/2405.05164v2)

3) **Range map / range-bin 相位序列 / 多 bin 融合**  
   - 生命体征工程常用；便于做目标选择、相位微动跟踪  
   - 代表：卒中患者睡眠里 multi range-bin selection 把 MAE 做到 0.84 bpm 级别，但 coverage 可能非常低[[4]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12788250/)

4) **点云（x,y,z + velocity/intensity/amplitude 等）**  
   - 强项：轻量、直观、易与 3D 任务对齐（姿态/动作/检测）  
   - 代价：点云生成依赖 CFAR 等手工参数；可能丢 Doppler 细节（一些工作明确指出这一点）[[14]](https://arxiv.org/html/2405.05164v2)  
   - 代表：MiliPoint 用 s×k×3 点云序列做基准；mmBody/mmFree-Pose 定义点属性很清楚[[42]](https://proceedings.neurips.cc/paper_files/paper/2023/file/c60468eca9cd0b0083f0ff9d0aeb171a-Paper-Datasets_and_Benchmarks.pdf)[[11]](https://github.com/Chen3110/mmBody)[[15]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12383697/)

### 4.2 WiFi CSI 输入的两条主线
1) **复 CSI 张量（子载波×天线×收发端×时间窗）**  
   - 代表：Person-in-WiFi-3D 的 1×3×3×30×20×2 输入，并做 token 化 Transformer[[19]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Person-in-WiFi_3D_End-to-End_Multi-Person_3D_Pose_Estimation_with_Wi-Fi_CVPR_2024_paper.pdf)  
2) **统一成“固定特征维×窗口长度”的时序输入**  
   - 代表：CSI-Bench baseline 配置 `feature_size=232, win_len=500`[[59]](https://github.com/Jenny-Zhu/CSI-Bench-Real-WiFi-Sensing-Benchmark/)，并在异构设备上做标准评测[[55]](https://arxiv.org/pdf/2505.21866)[[57]](https://ai-iot-sensing.github.io/projects/project.html)

---

## 5) 你可以直接复用的“对照表骨架”（算法→输入→数据集→指标→准确率→条件→局限）

下面这张表我把当前上下文里**能填出“输入与数值”**的条目尽量填满；填不出的我会标注“原摘录缺失”。

### 5.1 跨任务/跨模态“强策略条目清单”（可核对版本）

| 任务 | 模态 | 方法/策略（年份） | 输入信号形式（明确度） | 数据集/场景 | 指标与结果 | 局限/注意 |
|---|---|---|---|---|---|---|
| 睡眠 HR/RR | FMCW 雷达 | 多 range-bin selection（2026）[[4]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12788250/) | range bins（明确）[[4]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12788250/) | 49 卒中患者 + PSG[[4]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12788250/) | RR MAE 0.39；HR MAE 0.84；但 HR coverage 19.93%[[4]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12788250/) | 精度-覆盖率权衡必须报告[[4]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12788250/) |
| HR/RR | FMCW 雷达 | Angular-Free + 跟踪后聚焦（2024）[[5]](https://www.mdpi.com/1424-8220/24/8/2448) | 雷达回波（细节缺失）[[5]](https://www.mdpi.com/1424-8220/24/8/2448) | 智能家居场景（细节缺失） | BR 平均误差 −0.06；HR −1.5（BA）[[5]](https://www.mdpi.com/1424-8220/24/8/2448) | 缺少协议细节[[5]](https://www.mdpi.com/1424-8220/24/8/2448) |
| HR | WiFi CSI | 幅相融合旋转投影 + HSR 子载波选择（2024）[[9]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11013971/) | CSI 幅度+相位（明确）[[9]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11013971/) | 家庭环境（摘要） | 96.8% 准确率；0.8 bpm 中位误差[[9]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11013971/) | 缺 CSI 维度/设备信息[[9]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11013971/) |
| 睡眠呼吸事件/SAHS | UWB | profiling + attention + 对比学习 + 投票（Respnea）[[2]](https://dl.acm.org/doi/10.1145/3680550) | UWB 幅度+相位（明确）[[2]](https://dl.acm.org/doi/10.1145/3680550) | 医院+家庭；健康+患者[[2]](https://dl.acm.org/doi/10.1145/3680550) | RR median error 0.27 bpm；严重度 94.44%[[2]](https://dl.acm.org/doi/10.1145/3680550) | 未给更细输入格式（CIR等）[[2]](https://dl.acm.org/doi/10.1145/3680550) |
| 多人 3D 姿态 | WiFi CSI | WiFi Pose Transformer（CVPR 2024）[[19]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Person-in-WiFi_3D_End-to-End_Multi-Person_3D_Pose_Estimation_with_Wi-Fi_CVPR_2024_paper.pdf) | 1×3×3×30×20×2 CSI；token=180（非常明确）[[19]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Person-in-WiFi_3D_End-to-End_Multi-Person_3D_Pose_Estimation_with_Wi-Fi_CVPR_2024_paper.pdf) | 自采集 97K 帧，4m×3.5m，7人[[19]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Person-in-WiFi_3D_End-to-End_Multi-Person_3D_Pose_Estimation_with_Wi-Fi_CVPR_2024_paper.pdf) | 3D 关节误差：1人 91.7mm；2人 108.1；3人 125.3[[19]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Person-in-WiFi_3D_End-to-End_Multi-Person_3D_Pose_Estimation_with_Wi-Fi_CVPR_2024_paper.pdf) | 多接收端部署成本更高[[19]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Person-in-WiFi_3D_End-to-End_Multi-Person_3D_Pose_Estimation_with_Wi-Fi_CVPR_2024_paper.pdf) |
| 2D/骨架（雷达） | mmWave | ProbRadarM3F（2024）[[14]](https://arxiv.org/html/2405.05164v2) | ADC→4D-FFT heatmaps + prob map/PE（明确）[[14]](https://arxiv.org/html/2405.05164v2) | HuPR[[14]](https://arxiv.org/html/2405.05164v2) | AP=69.9%[[14]](https://arxiv.org/html/2405.05164v2) | AP 定义细节需回正文核对[[14]](https://arxiv.org/html/2405.05164v2) |
| HAR（点云） | mmWave | MiliPoint 基线（2023）[[42]](https://proceedings.neurips.cc/paper_files/paper/2023/file/c60468eca9cd0b0083f0ff9d0aeb171a-Paper-Datasets_and_Benchmarks.pdf) | 点云序列 s×k×3，动作分类 s=50（明确）[[42]](https://proceedings.neurips.cc/paper_files/paper/2023/file/c60468eca9cd0b0083f0ff9d0aeb171a-Paper-Datasets_and_Benchmarks.pdf) | MiliPoint（公开+代码）[[45]](https://github.com/yizzfz/MiliPoint) | Action Top1：PointNet++ 34.45±0.80；Top3 54.96±1.21[[42]](https://proceedings.neurips.cc/paper_files/paper/2023/file/c60468eca9cd0b0083f0ff9d0aeb171a-Paper-Datasets_and_Benchmarks.pdf) | 随机划分；动作分类很难[[42]](https://proceedings.neurips.cc/paper_files/paper/2023/file/c60468eca9cd0b0083f0ff9d0aeb171a-Paper-Datasets_and_Benchmarks.pdf) |
| HAR（点云） | mmWave | SMC-HAR（2026）[[41]](https://www.sciencedirect.com/science/article/abs/pii/S1574119225001208) | 稀疏点云（摘要级）[[41]](https://www.sciencedirect.com/science/article/abs/pii/S1574119225001208) | MM-Fi；随机 8:2[[41]](https://www.sciencedirect.com/science/article/abs/pii/S1574119225001208) | 分类 Acc=88.40%（摘要）[[41]](https://www.sciencedirect.com/science/article/abs/pii/S1574119225001208) | 与 cross-domain split 不可直接对比[[40]](https://github.com/ybhbingo/MMFi_dataset)[[41]](https://www.sciencedirect.com/science/article/abs/pii/S1574119225001208) |
| 跌倒检测 | WiFi CSI | CSI-Bench baseline（2025）[[57]](https://ai-iot-sensing.github.io/projects/project.html) | feature_size=232, win_len=500（配置）[[59]](https://github.com/Jenny-Zhu/CSI-Bench-Real-WiFi-Sensing-Benchmark/) | in-the-wild，多芯片异构[[55]](https://arxiv.org/pdf/2505.21866) | LSTM 94.93±0.51 Acc / 94.92±0.50 F1[[57]](https://ai-iot-sensing.github.io/projects/project.html) | 指标是分类Acc/F1，不是MAR/FAR[[57]](https://ai-iot-sensing.github.io/projects/project.html) |
| 跌倒检测 | WiFi CSI | XFall（JSAC 2024）[[64]](https://tns.thss.tsinghua.edu.cn/~guoxuan/assets/pdf/Paper-XFall.pdf) | 未在摘录给 CSI 维度 | 1年部署+70设置[[64]](https://tns.thss.tsinghua.edu.cn/~guoxuan/assets/pdf/Paper-XFall.pdf) | Acc 96.8%；MAR 3.1%；FAR 3.3%[[64]](https://tns.thss.tsinghua.edu.cn/~guoxuan/assets/pdf/Paper-XFall.pdf) | 强在真实部署；需看实现细节[[64]](https://tns.thss.tsinghua.edu.cn/~guoxuan/assets/pdf/Paper-XFall.pdf) |
| 占用/计数 | WiFi CSI | 域适配+Adapter PEFT（2026）[[31]](https://arxiv.org/html/2601.02203v1) | CSI（细节缺失） | WiAR（公共）+ WiFlow（私有）[[31]](https://arxiv.org/html/2601.02203v1) | WiAR：98.8% Acc/F1；WiFlow：10-shot MAE=0.44[[31]](https://arxiv.org/html/2601.02203v1) | WiFlow 公开性/协议需核验[[31]](https://arxiv.org/html/2601.02203v1)[[62]](https://www.emergentmind.com/topics/wiflow-dataset) |
| 热成像跌倒 | Thermal | BiConvLSTM+注意力+motion flow（2026）[[74]](https://arxiv.org/html/2509.16479v2) | 热帧序列 256×256×tt（明确）[[74]](https://arxiv.org/html/2509.16479v2) | TSF + TF-66（后者数值缺失）[[74]](https://arxiv.org/html/2509.16479v2) | TSF ROC-AUC=99.7%[[74]](https://arxiv.org/html/2509.16479v2) | TF-66 上的具体数值摘录缺失[[74]](https://arxiv.org/html/2509.16479v2) |

---

## 6) 面向你的“选型建议”：按任务给出最优策略组合与边界

### 6.1 如果你做生命体征（尤其睡眠/临床）
- **追求临床可信度**：优先考虑能与 PSG/医疗参考对齐并报告 coverage 的雷达路线（multi-bin selection 的精度很强，但必须解决 coverage）[[4]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12788250/)。
- **追求可检测“事件/严重度”**：UWB 的 Respnea 范式更接近临床需求闭环（attention+对比学习+投票，输出 SAHS 严重度）[[2]](https://dl.acm.org/doi/10.1145/3680550)。
- **追求低成本与易部署**：WiFi 心率/呼吸要把“相位可用化 + 子载波选择 + 多子载波融合”做扎实，往往比直接堆大模型更稳[[9]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11013971/)；如果你要做跨设备/跨房间泛化，建议以 CSI-Bench 的异构设备设置做压力测试（其呼吸检测 baseline 已到 98.8% Acc/F1 级别）[[57]](https://ai-iot-sensing.github.io/projects/project.html)[[55]](https://arxiv.org/pdf/2505.21866)。

### 6.2 如果你做姿态/骨架
- **多人 3D 姿态（WiFi）**：Person-in-WiFi-3D 是当前上下文里“输入规格最清晰 + 多人任务闭环 + 可对标指标（mm级误差）”的代表[[19]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Person-in-WiFi_3D_End-to-End_Multi-Person_3D_Pose_Estimation_with_Wi-Fi_CVPR_2024_paper.pdf)。
- **雷达骨架（mmWave）**：如果你能接受 FFT 热图与注意力模型的算力，ProbRadarM3F 这种“热图 + 位置信息补全 + attention 融合”的路线很强，且输入链路明确[[14]](https://arxiv.org/html/2405.05164v2)。
- **隐私优先、不要视觉伪标签**：关注 mmFree-Pose 这类“无视觉、动捕标注”的数据与管线[[15]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12383697/)，以及 mmBody 这类标定严格的数据（虽然获取门槛高）[[11]](https://github.com/Chen3110/mmBody)。

### 6.3 如果你做跌倒检测（真正落地）
- **要真实世界泛化**：优先参考 XFall（跨 70 设置、一年部署、MAR/FAR 同时报）这类“工程闭环”论文，而不仅是实验室数据集高分[[64]](https://tns.thss.tsinghua.edu.cn/~guoxuan/assets/pdf/Paper-XFall.pdf)。
- **要隐私且全天候**：热成像跌倒的 AUC 可以非常高（TSF 上到 99.7%）[[74]](https://arxiv.org/html/2509.16479v2)，但要注意相机部署合规与数据可得性（TF-66 按请求提供）[[36]](https://www.sciencedirect.com/science/article/pii/S0952197625018214)。
- **要在 WiFi 上快速对标**：用 CSI-Bench 的 fall 子集 baseline（LSTM 94.93% Acc/F1）作为你第一版可比目标，然后再做跨设备/跨房间泛化[[57]](https://ai-iot-sensing.github.io/projects/project.html)[[55]](https://arxiv.org/pdf/2505.21866)。

### 6.4 如果你做占用/人数（计数）
- **强可迁移策略**：自监督预训练 + 无监督域适配 + PEFT（Adapter/LoRA）是近两年很“工程化”的方向，尤其适合你要跨房间/跨设备部署时降低标注与微调成本[[31]](https://arxiv.org/html/2601.02203v1)。
- **指标要拆开报**：建议把计数拆为（1）事件/变化分类 Acc/F1，（2）最终 occupancy 的 MAE/RMSE（并给状态机去抖逻辑），与现有工作对齐[[31]](https://arxiv.org/html/2601.02203v1)。

---

## 7) 你如果要我进一步“定量评估输入信号与准确率”，我需要你补充的 5 个问题（否则跨论文很难公平比）
1) 你关注的生命体征是 **RR/HR 数值**，还是 **事件（apnea/hypopnea）**，还是 **睡眠分期/睡眠质量指标**？  
2) 你希望“输入信号”最终落在什么层级：**raw ADC/CSI**、还是 **range map / 点云 / micro-Doppler**、还是已经提取的波形？（不同层级决定数据采集带宽、端侧算力、算法上限）  
3) 你要做 **单人**还是 **多人**？多人会显著改变最优策略（需要分离/跟踪/集合预测）。  
4) 你对部署场景的约束：距离范围、遮挡、穿墙需求、是否床上/是否走动、是否允许多设备（如 Person-in-WiFi-3D 的 3 接收端）[[19]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Person-in-WiFi_3D_End-to-End_Multi-Person_3D_Pose_Estimation_with_Wi-Fi_CVPR_2024_paper.pdf)。  
5) 你希望输出一个“**排行榜式表格**”（强依赖统一基准），还是“**工程选型表**”（可接受跨数据集但必须标注协议差异）？

如果你回答上述问题，我可以把你关心的子任务抽出来，按同一套指标（例如 HR/RR：MAE/RMSE/coverage/相关系数/BA；跌倒：Acc/F1+MAR/FAR；姿态：MPJPE/PA-MPJPE 或 3D joint error；计数：MAE）进一步把表格补齐，并给出“在你的输入层级与部署约束下”的首选策略组合。
