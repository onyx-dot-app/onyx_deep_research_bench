Query ID: 74
============================================================

QUESTION:
Please conduct a study and prepare a report on the 'Construction and Application of a Sports Intelligent Tutoring and Learning Guidance System Driven by Multimodal Data Fusion.'

============================================================

RESEARCH TASKS (6 total):
  #1: Research the state of the art (2023–Feb 2026) in sports intelligent tutoring systems (ITS), AI coaching, and learning guidance platforms, focusing on multimodal data fusion for skill/tactics/fitness/injury prevention. Identify key systems/products, academic prototypes, major papers, and open gaps/limitations.
  #2: Research multimodal data sources used for sports skill assessment and coaching (video, pose, IMUs, pressure insoles, EMG, audio, HR/HRV, GPS, context) and best practices for data collection, synchronization, labeling, and quality control for robust multimodal fusion, including any standard pipelines/datasets up to Feb 2026.
  #3: Research multimodal data fusion strategies and model families relevant to sports coaching tasks (technique diagnosis, error attribution, feedback generation, training plan recommendation): early/late/hybrid fusion, cross-attention Transformers, GNNs for kinematics, multimodal self-supervised learning, video/pose foundation models, personalization/continual learning, and explainability approaches; include notable works 2023–Feb 2026.
  #4: Research pedagogical models and functional requirements for a sports intelligent tutoring & learning guidance system: target sports/user groups/contexts, mastery learning, deliberate practice, constraints-led approach, feedback timing (KR/KP), adaptive curricula, goal setting, motivation, and measurable outcomes. Include any frameworks/standards or studies (2020–Feb 2026) linking AI coaching/ITS to learning science in sports.
  #5: Research end-to-end system architectures for multimodal sports coaching/ITS: edge vs cloud inference, real-time multimodal pipelines, interfaces (AR/VR, mobile), knowledge representation/tutoring logic, safety constraints, explainability, and LLM integration patterns (e.g., sensor-to-text prompting). Include reference architectures and notable systems 2023–Feb 2026.
  #6: Research evaluation and deployment for multimodal sports intelligent tutoring systems: benchmarks/datasets, experimental design (ablation, user studies, RCT-like interventions), metrics (skill improvement, latency, accuracy, usability, fairness), and practical deployment considerations (privacy/consent/minors, bias, security, regulatory/medical boundary). Include relevant reporting guidelines (e.g., TRIPOD+AI, CONSORT-AI) and sports/health AI safety considerations up to Feb 2026.

============================================================

ANSWER:
## 1) State of the art (2023–Feb 2026): what exists, what works, what’s missing

### 1.1 System types that currently define the landscape

**A. Vision-/pose-driven technique tutoring (often mobile-first)**
- Many coaching/tutoring prototypes (and some products) rely on **camera → pose estimation → rule/ML-based assessment → feedback** because it is inexpensive and scalable. A monitored exercise guidance system using smartphone/computer camera and BlazePose exemplifies this pattern, adding kinematic/range-of-motion analyses to detect deviations and provide guidance in real time [[79]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11013996/).
- However, purely vision-based systems are repeatedly described as fragile in real-world sports conditions: dependence on camera viewpoint/field-of-view, occlusion, and limited ability to estimate high-speed dynamics (velocity/acceleration) needed for explosive movements [[1]](https://www.mdpi.com/2079-9292/14/21/4155); plus broader broadcast/field CV challenges like occlusions, lighting, resolution, motion blur, identity switches, and calibration variability [[5]](https://link.springer.com/article/10.1007/s44163-025-00586-1).

**B. Multimodal tutoring/feedback systems (video + wearables)**
- A clear trend is combining **video/pose with IMUs/pressure/EMG/audio** to compensate for vision’s weaknesses and to capture dynamics and internal load.
- Example: **BoxingPro** (Electronics, 2025) fuses wearable inertial sensing and video-derived pose cues, then translates quantitative findings into structured text prompts to drive **LLM-generated coaching feedback**. It is explicitly motivated by the “semantic gap” between low-level sensor features and high-level, actionable coaching language [[1]](https://www.mdpi.com/2079-9292/14/21/4155).

**C. LLM-augmented coaching: from “chat” to grounded multimodal guidance**
- The field is moving from text-only “AI coaches” to **grounded** systems where LLMs are conditioned on motion/biomechanics features.
- A Sensors (2025) method generates **skill-level-aware expert comments** by extracting spatial-temporal motion features with an STA‑GCN, classifying skill level, then feeding video + motion features + skill level into a multimodal model to generate actionable feedback [[44]](https://www.mdpi.com/1424-8220/25/2/447).
- A key insight across these works: LLMs are strong communicators/reasoners but need **grounding** and **structured representations** of kinematics to avoid generic or unphysical advice—hence prompt-translation methods (BoxingPro) [[1]](https://www.mdpi.com/2079-9292/14/21/4155) and motion-feature tokenization (Sensors 2025) [[44]](https://www.mdpi.com/1424-8220/25/2/447).

**D. AR/VR/XR coaching interfaces**
- AR feedback is increasingly tested as a delivery mechanism for immediate, situated correction. A youth soccer study comparing **live AR feedback vs conventional verbal feedback** reports greater gains in dribbling/passing/shooting and improved intrinsic motivation (interest/competence/effort), with motivation gains correlating with performance improvements [[62]](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1661936/full), [[63]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12714985/).
- In combat sports, an AR taekwondo coaching system reports real-time recognition/feedback with an AR overlay and high user satisfaction, while also documenting deployment constraints (field-of-view, lighting sensitivity, headset fatigue, hardware requirements) that matter for any “real-time XR tutor” [[78]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12630583/), [[88]](https://www.nature.com/articles/s41598-025-24608-1).
- On the VR tutoring side, an architecture for VR ITS integrates **LLM-based NPC tutors** with multimodal interaction (speech, hand tracking, gaze, haptics) but notes latency considerations and the complexity of real-time conversational loops [[86]](https://www.mdpi.com/2078-2489/16/7/556).

**E. Analytics platforms used in real settings (often not “tutors,” but relevant building blocks)**
- Products like SwingVision emphasize AI video analysis and statistics for tennis/pickleball and include features such as audio feedback and shot stats/heatmaps [[10]](https://swing.vision/).
- Platforms like Kitman Labs focus on integrating many data streams into an operational layer for readiness and injury-prevention workflows (vendor-asserted outcomes; technical modeling details are not established in the provided sources) [[11]](https://www.kitmanlabs.com/), [[12]](https://www.kitmanlabs.com/blog/optimize-injury-prevention/).
- These systems show adoption demand, but they are often “insight dashboards” rather than step-by-step pedagogical tutors.

### 1.2 Research and practice gaps that remain (especially for multimodal fusion tutors)

**Gap 1 — From recognition to instruction (the “semantic gap”)**
- Wearable systems often excel at classifying actions, but coaching requires *diagnosing why performance is suboptimal* and generating *actionable correction*—BoxingPro explicitly identifies this gap and proposes sensor/video-to-text translation to help LLMs produce coach-like feedback [[1]](https://www.mdpi.com/2079-9292/14/21/4155).

**Gap 2 — Robustness in unconstrained environments**
- Vision-only guidance is limited by occlusion, viewpoint, lighting, blur, and calibration variability [[5]](https://link.springer.com/article/10.1007/s44163-025-00586-1), and by missing dynamics for high-speed skills [[1]](https://www.mdpi.com/2079-9292/14/21/4155). Multimodal fusion helps, but introduces synchronization, missing modality, and sensor placement variability problems.

**Gap 3 — Lack of open, context-rich sports datasets and weak longitudinal evidence**
- A systematic review of deep-learning human pose estimation in sport highlights that most studies use private datasets and bespoke pipelines, limiting reproducibility and practical uptake, and reports a “complete absence” of longitudinal studies testing long-term athlete development impact [[40]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12696263/).

**Gap 4 — Standardization and evaluation rigor**
- Broad AI-in-sports reviews point to inconsistent methodologies, limited reliability/real-time generalization, and lack of standard evaluation practices [[3]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12382096/), [[4]](https://www.mdpi.com/2076-3417/15/13/7254).
- For LLM-based coaching, a scoping review finds fragmented evaluation approaches and generally low rigor; benchmarks often test descriptive sports understanding rather than safety-critical, prescriptive coaching behavior (e.g., safe real-time form correction) [[49]](https://www.jmir.org/2025/1/e79217), [[93]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12520646/).

**Gap 5 — Safety, ethics, governance**
- Injury prediction and athlete monitoring raise recurring ethical issues: privacy, fairness, consent, autonomy, governance, and power asymmetry (especially for youth/Paralympic contexts) [[108]](https://www.mdpi.com/2673-2688/6/11/283). Consumer wearable privacy policies show large variability and frequent “high risk” ratings in transparency and vulnerability disclosure [[109]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12167361/).

---

## 2) Design targets: sports, users, contexts, and pedagogical foundations

### 2.1 Recommended scope: a tutor that generalizes across coaching “problem types”
A practical “sports intelligent tutoring and learning guidance system” should be designed as a **multi-task, multi-timescale** coach that can support:

1) **Technique learning (motor skill execution)**  
   Examples: striking/punching, racket swings, shooting mechanics, squats/jumps/landings.

2) **Tactics and decision-making (perception–action coupling)**  
   Examples: soccer tactical choices, team role reasoning, rule/foul interpretation (useful both for players and officials).

3) **Fitness and load management (training prescription + adaptation)**  
   Examples: endurance training plan guidance, fatigue/readiness-aware session modulation.

4) **Injury risk reduction (risk-aware constraints, not medical diagnosis)**  
   Examples: detecting risky movement patterns, recommending safer constraints/drills and rest—not claiming medical-grade prediction unless regulated/validated.

### 2.2 Users and contexts: design for variability, not a single athlete archetype
**User groups**
- **Novices/youth**: need clear KP/KR feedback, motivation support, safer constraints, and less sensor burden. The youth soccer AR trial indicates real-time feedback can improve both performance and intrinsic motivation [[62]](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1661936/full), [[63]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12714985/).
- **Intermediate/advanced**: need fine-grained error attribution, phase-specific cues, and personalization by style, body proportions, and goals.
- **Elite/team settings**: require interoperability with existing monitoring workflows and coach-in-the-loop review; may tolerate more instrumentation.

**Contexts**
- **Self-practice at home / gym** (mobile camera + optional wearables).
- **On-field training** (wearables + sparse video + GPS).
- **Remote coaching** (video upload + automated augmentation; coach editing workflows).
  - VisMimic shows how human–AI collaboration can produce augmented feedback videos that combine key poses and motion trajectories for comprehension [[2]](https://dl.acm.org/doi/10.1145/3746059.3747794).
- **PE / online PE**: the “feedback dilemma” is that OLPE often cannot close the perception–action–feedback loop; AI pose-based feedback is proposed as a remedy, grounded in motor learning + cognitive load + SDT motivation theory [[64]](https://link.springer.com/article/10.1186/s40561-025-00411-3).

### 2.3 Pedagogical models that should shape construction (and what they imply for system functions)

**A. Adaptive Instructional Systems / ITS concept**
- IEEE P2247 defines adaptive instructional systems as AI systems that tailor instruction and recommendations to learners’ goals/needs/preferences; ITS is the most common form, providing real-time, tailored feedback [[52]](https://sagroups.ieee.org/2247-1/wp-content/uploads/sites/316/2020/01/IEEE-Project-2247_one-pager.pdf).  
**Implication:** you need a learner model + pedagogical policy + assessment loop, not just a classifier.

**B. Motor learning feedback concepts (KR/KP)**
- Sports feedback needs both **Knowledge of Results (KR)** (outcomes: hit/miss, speed, accuracy) and **Knowledge of Performance (KP)** (movement form/coordination). The AR soccer work explicitly frames KP/KR and emphasizes timing/precision of feedback [[62]](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1661936/full), [[63]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12714985/).  
**Implication:** the system must produce (i) outcome metrics and (ii) form metrics, and decide *when* to deliver which.

**C. Constraints-led / ecological dynamics / nonlinear pedagogy**
- Ecological dynamics emphasizes athlete–environment interaction and positions the coach as a **learning environment designer** rather than a template enforcer [[57]](https://link.springer.com/article/10.1186/s40798-020-00268-5). Constraints-led coaching is presented as a prominent applied methodology [[58]](https://itfcoachingreview.com/index.php/journal/article/view/319).
- Evidence syntheses suggest nonlinear pedagogy can be particularly beneficial for tactical outcomes, with mixed results on technical outcomes and a need for better measurement tools [[61]](https://link.springer.com/article/10.1186/s40798-025-00893-y).  
**Implication:** the tutor should not only say “move your elbow here,” but also recommend **constraint manipulations** (task, environment, rules) that elicit the desired coordination pattern.

**D. Motivation, self-determination theory (SDT), self-regulation**
- AR feedback can increase intrinsic motivation and perceived competence/autonomy [[62]](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1661936/full), [[63]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12714985/).  
- A conceptual OLPE framework argues AI feedback reduces extraneous cognitive load and supports competence/autonomy, improving engagement and persistence [[64]](https://link.springer.com/article/10.1186/s40561-025-00411-3).  
- A self-regulation model stresses self-awareness, strategy selection, behaviors, and mental control, while warning about over-analysis and cognitive fatigue [[70]](https://www.sciencedirect.com/science/article/pii/S2211266925000337).  
**Implication:** feedback should be *actionable and minimal*, support goal-setting and reflection, and avoid overwhelming the learner.

**E. Mastery learning / deliberate practice (translated from ITS research)**
- ITS research (e.g., Trace Table Tutor) highlights modeling component skills + integrative skills, delivering step-by-step support and targeted repetition to build mastery efficiently [[71]](https://onlinelibrary.wiley.com/doi/abs/10.1111/jcal.12757).  
**Implication (sports translation):** decompose skills into phases/constraints/coordination elements, track mastery, and prescribe deliberate drills for weak subskills.

### 2.4 Measurable outcomes (what “learning guidance” must improve)
Your system should commit to measurable outcomes across timescales:
- **Immediate performance:** success rate, accuracy, consistency, timing error.
- **Technique quality:** joint angle/segment timing, smoothness/jerk, coordination metrics.
- **Retention and transfer:** delayed retention tests are essential; motor learning meta-analysis shows simple assumptions (e.g., reduced feedback frequency always helps learning) are not robustly supported [[65]](https://www.sciencedirect.com/science/article/abs/pii/S1469029222000334), so you must measure retention rather than assume it.
- **Tactical decision-making:** knowledge comprehension + decision quality; an AI-assisted football tactics study reports improved tactical comprehension/decision-making and satisfaction when combining language guidance with visualization [[67]](https://www.sciencedirect.com/science/article/pii/S0001691825007449).
- **Motivation/engagement:** intrinsic motivation (IMI subscales), adherence, practice volume [[62]](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1661936/full), [[63]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12714985/).
- **Safety proxies:** fatigue markers, risky pattern frequency, pain flags (with careful boundary-setting).

---

## 3) Multimodal data sources and how to collect/synchronize/label for robust fusion

### 3.1 Modalities: relevance vs feasibility (practical recommendations)
Below is a pragmatic prioritization for a multimodal sports tutoring system.

**Tier 1 (high value, feasible)**
1) **Video (RGB) + pose/skeleton**  
   - Core for technique understanding and feedback visualization.
   - Sports pose benchmarks show athletic motion is harder than standard datasets; domain adaptation can dramatically reduce errors (e.g., AthletePose3D reports large improvements after fine-tuning on athletic motions) [[23]](https://github.com/calvinyeungck/AthletePose3D).

2) **IMU wearables (wrist/ankle/torso; equipment-mounted when relevant)**
   - Adds dynamics, robustness to occlusion, and can capture impact-like events.
   - T3Set explicitly argues table tennis needs sensor-captured kinematic details (strength/acceleration) that video misses; it provides synchronized racket IMU + video and coach suggestions [[95]](https://dl.acm.org/doi/10.1145/3711896.3737407).

3) **Context metadata**
   - Skill level, drill type, intent, environment constraints, target outcomes.
   - MultiSenseBadminton includes skill level and rich annotations (stroke type, ball landing, hitting location, sound labels, surveys/interviews), illustrating how context supports coaching-grade analysis [[13]](https://www.nature.com/articles/s41597-024-03144-z), [[19]](https://github.com/dailyminiii/MultiSenseBadminton).

**Tier 2 (situational, higher burden but strong diagnostic value)**
4) **Pressure insoles / plantar pressure**
   - Valuable for footwork, balance, load distribution; MovePort demonstrates high-density insole pressure arrays combined with EMG/IMU/mocap [[26]](https://pubmed.ncbi.nlm.nih.gov/39024074/).  
5) **Audio**
   - Useful for impact timing, rhythm; MultiSenseBadminton includes hitting sound annotations [[13]](https://www.nature.com/articles/s41597-024-03144-z). Audio + video can improve event spotting in sports video understanding [[20]](https://arxiv.org/abs/2011.04258), [[21]](https://github.com/bastienvanderplaetse/SoccerNetMultimodalActionSpotting).  
6) **GPS/GNSS + accelerometry (field sports)**
   - For workload/positioning context; SoccerMon provides open objective GNSS tracking plus subjective wellness/load/injury reports at scale, showing how contextual “state” can be integrated with sensor streams [[28]](https://www.nature.com/articles/s41597-024-03386-x).

**Tier 3 (specialized / lab-like, or advanced users)**
7) **EMG**
   - High diagnostic value but complex setup; appears in MultiSenseBadminton and biomechanics datasets [[13]](https://www.nature.com/articles/s41597-024-03144-z), [[16]](https://pmc.ncbi.nlm.nih.gov/articles/PMC10740031/).  
8) **Markerless dynamics estimation (multi-phone)**
   - OpenCap estimates 3D kinematics and dynamics from multiple smartphones and validates against mocap/force plates, enabling quasi-lab measures without a lab [[29]](https://pmc.ncbi.nlm.nih.gov/articles/PMC10586693/).  
9) **Eye tracking / gaze**
   - Useful for perceptual skill; the natural urban walking dataset demonstrates synchronized full-body IMU + pressure insoles + gaze + first-person video with detailed annotations [[27]](https://www.nature.com/articles/s41597-022-01580-3)—an existence proof for “attention-aware” guidance.

### 3.2 Data collection and synchronization: proven patterns

**A. Hardware synchronization (best when you control the environment)**
- CMU Panoptic Studio demonstrates hardware clock synchronization across hundreds of cameras and multiple sensor types [[24]](http://domedb.perception.cs.cmu.edu/).  
**Use when:** building a high-quality reference dataset for model training/validation.

**B. Data-driven synchronization (best for consumer devices)**
- OpenCap synchronizes multi-phone video by cross-correlating keypoint velocities, then triangulates 3D points [[29]](https://pmc.ncbi.nlm.nih.gov/articles/PMC10586693/).  
**Use when:** users record with phones without shared clock.

**C. Wearable synchronization and sampling alignment**
- Multimodal fatigue detection work warns that sampling mismatch and timestamp loss can create large angular errors and distort intervals; it resamples modalities and emphasizes careful synchronization [[9]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12157230/).  
**Use when:** fusing IMU + EMG + other wearables.

**D. Streaming transport for real-time systems**
- A TinyML cloud-edge framework uses a WebRTC-based streaming pipeline for real-time action recognition and cloud-edge coordination [[84]](https://onlinelibrary.wiley.com/doi/abs/10.1002/itl2.70100).  
**Use when:** you need low-latency streaming from mobile devices to edge/cloud.

### 3.3 Labeling strategies that scale and support coaching-grade outputs

**A. Coarse-to-fine labeling with rule-based anchors**
- SoccerNet shows a scalable pipeline: parse coarse events from match reports then manually refine to rule-consistent second-level anchors; SoccerNet-v2 scales this to many more action classes with consistent anchor definitions based on soccer rules [[14]](https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w34/Giancola_SoccerNet_A_Scalable_CVPR_2018_paper.pdf), [[97]](https://openaccess.thecvf.com/content/CVPR2021W/CVSports/papers/Deliege_SoccerNet-v2_A_Dataset_and_Benchmarks_for_Holistic_Understanding_of_Broadcast_CVPRW_2021_paper.pdf).  
**Lesson:** define sport-specific anchor rules; refine systematically.

**B. Expert-driven multi-attribute annotations**
- SPORTU uses expert annotators, multilevel question design (easy→hard), and double review with removal of controversial clips to reduce mislabeling—important for rule/foul reasoning tasks and explanation evaluation [[94]](https://arxiv.org/html/2410.08474v3).  
**Lesson:** coaching-grade labels require “interpretation QA,” not only class tags.

**C. Coach suggestion corpora**
- T3Set provides not only aligned IMU+video but also **targeted suggestions from coaches** with a suggestion taxonomy—directly aligned to tutoring output requirements [[95]](https://dl.acm.org/doi/10.1145/3711896.3737407).  
**Lesson:** collect “what the coach would say,” not just motion labels.

**D. LLM-assisted labeling (bootstrapping)**
- A sensor-to-text framework in rehabilitation uses LLM-generated video descriptions to label sensor segments, addressing limited labeled sensor data [[85]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12467268/).  
**Lesson:** similar bootstrapping can label sports IMU segments using video + LLM, with human QC.

### 3.4 Quality control (QC) and validation practices you should institutionalize
- **Benchmark against gold standards:** SportsPose compares against a commercial marker-based system and reports quantified error [[17]](https://christianingwersen.github.io/SportsPose/), [[22]](https://orbit.dtu.dk/en/datasets/the-sportspose-dataset/); OpenCap reports errors for angles/forces/moments vs mocap/force plates [[29]](https://pmc.ncbi.nlm.nih.gov/articles/PMC10586693/).  
- **Version control and errata handling:** AthletePose3D documents preprocessing mistakes and corrected releases—your pipeline must track dataset versions and checksums [[23]](https://github.com/calvinyeungck/AthletePose3D).  
- **Reproducible organization/metadata:** Motion-BIDS argues motion data needs standardized metadata/file structure to be interoperable across labs and modalities, spanning IMUs, optical systems, and GPS trackers [[18]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11219788/).  
- **Learning activity logs interoperability:** xAPI and Caliper provide event models for capturing learning/feedback interactions and dashboards; they support instructor and team scenarios and real-time messaging for responsive engagement [[55]](https://www.imsglobal.org/initial-xapicaliper-comparison), [[56]](https://www.1edtech.org/standards/caliper).  
  *Practical suggestion:* store coaching interactions (feedback events, drill assignments, overrides) in xAPI/Caliper-like structures to support analytics and auditing.

---

## 4) Multimodal data fusion strategies and model families for sports tutoring tasks

A sports ITS must do more than “fuse features”—it must support four core inference tasks:
1) **Perception:** estimate pose/kinematics/events reliably.
2) **Assessment:** compute quality scores, detect errors, classify skill level.
3) **Diagnosis:** attribute errors to causes (timing, coordination, constraint mismatch, fatigue).
4) **Instruction:** generate actionable feedback, drills, and plans safely.

### 4.1 Fusion taxonomies and what to use when

**Early fusion (signal-level / representation-level)**
- Concatenate aligned features (e.g., IMU windows + pose features) into a shared encoder.
- Works well when synchronization is strong and missingness is rare.
- Risk: brittle under missing modalities and domain shift.

**Late fusion (decision-level)**
- Independent modality models produce scores/events; combine via weighted averaging, stacking, or policy gating.
- Works well when modalities are intermittently available (camera lost; IMU battery low).
- Risk: loses cross-modal interactions (e.g., subtle timing differences that require joint modeling).

**Hybrid fusion (recommended for tutoring)**
- Combine early fusion for *tightly coupled subproblems* (e.g., punch phase detection) with late fusion for *robust system decisions* (e.g., whether to issue a strong correction).
- Many recent systems implicitly adopt hybrid designs:
  - Dual-channel spatiotemporal Transformer + TCN with residual fusion (sports training pose estimation) [[41]](https://www.sciencedirect.com/science/article/pii/S1110016824009608).
  - ST‑GCN + Transformer + optimization components for spatiotemporal IoT sports analysis [[46]](https://www.sciencedirect.com/science/article/pii/S1110016825006702).
  - Video tokens + graph motion features + skill-level tokens fed into a multimodal generator for expert comments [[44]](https://www.mdpi.com/1424-8220/25/2/447).

### 4.2 Model families that matter most

#### A) Pose/skeleton modeling: graphs + temporal models
- Graph neural approaches model the body as a joint graph; spatiotemporal GCNs are common for action recognition and motion feature extraction (e.g., ST‑GCN in an explainable taekwondo ecosystem; STA‑GCN for motion features and skill classification) [[42]](https://arxiv.org/html/2510.18193v2), [[44]](https://www.mdpi.com/1424-8220/25/2/447).
- These features are especially valuable as **structured kinematics tokens** for downstream tutoring/LLM explanation.

#### B) Video understanding: foundation models and practical integration lessons
- InternVideo2 is a large video foundation model family trained with a progressive scheme combining masked video modeling, multimodal contrastive alignment, and next-token prediction, with explicit use of video-audio-speech captions for better alignment [[36]](https://arxiv.org/abs/2403.15377), [[37]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11491.pdf).  
- V‑JEPA 2 shows large-scale self-supervised video representation learning (over 1M hours) and downstream alignment with an LLM for question answering and anticipation tasks, illustrating how “foundation video encoders” can support higher-level reasoning [[38]](https://arxiv.org/html/2506.09985v1).
- Apollo (CVPR 2025) provides practical design findings for video-language models: fps sampling vs uniform sampling, token budgets, resampling (Perceiver resampler performing best in their experiments), and benefits of combining an image encoder with a video encoder [[39]](https://openaccess.thecvf.com/content/CVPR2025/papers/Zohar_Apollo__An_Exploration_of_Video_Understanding_in_Large_Multimodal_CVPR_2025_paper.pdf).  
**Implication for sports tutors:** you should treat video as expensive tokens; compress/resample intelligently and preserve temporal cues needed for technique timing.

#### C) Multimodal action quality assessment (AQA) and adaptive fusion
- AQA work shows that modality importance can vary across action phases; PAMFN proposes progressive and adaptive multimodal fusion policies across RGB/flow/audio for score regression [[6]](https://arxiv.org/abs/2402.09444).  
**Implication:** technique quality scoring should use phase-aware fusion (setup vs acceleration vs impact vs follow-through).

#### D) Sensor-language alignment and sensor-to-text generation
- SensorLLM proposes a two-stage sensor-language alignment approach with channel tokens and template-generated trend text to align time-series with a frozen LLM [[91]](https://arxiv.org/html/2410.10624v1). SensorLM proposes hierarchical captioning and very large-scale sensor-language pretraining for retrieval/recognition tasks [[92]](https://arxiv.org/html/2506.09108v1).  
- BoxingPro shows an applied “translation methodology” for converting kinematics into structured prompts to drive LLM reasoning for coaching feedback [[1]](https://www.mdpi.com/2079-9292/14/21/4155).  
**Implication:** for tutoring, language generation should be fed by *structured biomechanical facts + uncertainty*, not raw time-series.

#### E) Personalization and continual learning (critical in sport)
- Online continual learning for sensor-based HAR addresses evolving activity patterns and new activities under streaming unlabeled data [[47]](https://www.sciencedirect.com/science/article/pii/S1574119223000755).
- A dynamic mixture-of-experts continual learning approach aims to learn incrementally with resource constraints (important for on-device systems) [[48]](https://www.sciencedirect.com/science/article/pii/S1574119225000331).  
**Implication:** athletes change over time (fatigue, adaptation, injury, technique evolution). Your tutor needs safe, versioned personalization without catastrophic forgetting.

### 4.3 Explainability and uncertainty: required for coaching trust and safety
- FST.ai 2.0 describes combining pose-based recognition with uncertainty estimates and explainability overlays and dashboards, illustrating a pattern of **confidence-gated decisions** and human-inspectable rationale [[42]](https://arxiv.org/html/2510.18193v2). (Note: arXiv preprint; treat deployment claims cautiously.)
- Ontology-based recommendation modeling (OntoRecoModel) illustrates interpretable intent/component/content structures and rule-like reasoning for message generation [[89]](https://medinform.jmir.org/2022/6/e33847/).  
**Coaching implication:** separate *what you detected* (evidence), *why it matters* (principle/rule), and *what to do* (drill/cue), with confidence attached.

---

## 5) Construction: an end-to-end architecture for a multimodal sports ITS driven by fusion

### 5.1 Reference architecture (layered, hybrid edge–cloud, tutor-in-the-loop)

**Layer 0 — Sensing & capture**
- RGB video (single or multi-view), optional depth
- IMUs (body or equipment), pressure insoles, audio, GPS, HR/HRV (if available)
- Context inputs: sport, drill, goal, skill level, constraints, injury flags

**Layer 1 — Synchronization & stream management**
- Hardware sync when possible; otherwise data-driven sync (e.g., OpenCap’s keypoint-velocity correlation for multi-view video) [[29]](https://pmc.ncbi.nlm.nih.gov/articles/PMC10586693/).
- Resampling and alignment pipelines (as practiced in multimodal fatigue work) [[9]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12157230/).
- Real-time transport (e.g., WebRTC for streaming) [[84]](https://onlinelibrary.wiley.com/doi/abs/10.1002/itl2.70100).

**Layer 2 — Edge inference (low latency)**
- On-device pose estimation (mobile-friendly models as in BlazePose-based systems) [[79]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11013996/).
- Lightweight IMU segmentation (phases, reps, impacts).
- Immediate safety checks (e.g., unstable landing; excessive asymmetry).
- Edge runtime frameworks for streaming multimodal inference can follow patterns like NVIDIA Holoscan (graph-based pipeline, sensor bridge, GPU-accelerated operators) [[83]](https://www.nvidia.com/en-us/edge-computing/holoscan/).

**Layer 3 — Fusion & assessment services (edge server or cloud)**
- Hybrid fusion:
  - Pose graph encoder (ST/STA‑GCN features) [[44]](https://www.mdpi.com/1424-8220/25/2/447)
  - Video encoder (foundation model or distilled sports model) [[37]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11491.pdf), [[39]](https://openaccess.thecvf.com/content/CVPR2025/papers/Zohar_Apollo__An_Exploration_of_Video_Understanding_in_Large_Multimodal_CVPR_2025_paper.pdf)
  - IMU encoder (temporal CNN/Transformer)
  - Context encoder (skill level, drill plan, constraints)
- Outputs:
  - Rep segmentation + phase timing
  - Technique quality scores (per phase and overall)
  - Error classifiers (e.g., “early hip rotation,” “insufficient depth,” “late wrist snap”)
  - Skill level estimate (as in skill-level-aware comment generation) [[44]](https://www.mdpi.com/1424-8220/25/2/447)

**Layer 4 — Learner model & tutoring policy**
- Maintain a longitudinal **athlete/learner state**: mastery per subskill, variability, fatigue proxy, adherence, motivation signals.
- Policy chooses:
  - Feedback type: KP vs KR [[62]](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1661936/full), [[63]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12714985/)
  - Feedback timing: concurrent vs terminal vs summary
  - Drill recommendation: mastery learning / deliberate practice structure [[71]](https://onlinelibrary.wiley.com/doi/abs/10.1111/jcal.12757)
  - Constraints-led task modifications (environment/task constraints) [[57]](https://link.springer.com/article/10.1186/s40798-020-00268-5), [[58]](https://itfcoachingreview.com/index.php/journal/article/view/319), [[60]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11994730/)
- If deploying in XR or safety-critical contexts, adopt “guardrails + degrade safely” principles: under uncertainty, revert to defaults and avoid unsafe prescriptions (a key pattern in trustworthy adaptive training loops) [[77]](https://www.mdpi.com/2414-4088/10/1/11).

**Layer 5 — Feedback generation (multimodal, grounded, safe)**
- Generate feedback using:
  1) **Structured evidence** (metrics, detected errors, phase timings)
  2) **Coaching knowledge base** (rules, cues, contraindications)
  3) **LLM/NLG module** for natural language + personalization  
- BoxingPro demonstrates a concrete approach: convert multimodal numeric findings into structured prompts that LLMs can reason over step-by-step [[1]](https://www.mdpi.com/2079-9292/14/21/4155).  
- Use *confidence gating*: if confidence low, ask for re-recording, adjust camera placement, or provide conservative general guidance.

**Layer 6 — Interfaces (deliver actionable guidance)**
- **Real-time cues:** audio/haptic/visual overlays (AR), minimal and phase-specific.
- **Post-set review:** annotated video, motion-chain summaries, side-by-side with reference; VisMimic shows augmented feedback videos integrating key poses and trajectories and supporting multi-view and overview→detail explanation [[2]](https://dl.acm.org/doi/10.1145/3746059.3747794).
- **Coach dashboard:** progress, uncertainty, trends, drill adherence; optionally explanation overlays and audit logs (pattern seen in explainable ecosystems) [[42]](https://arxiv.org/html/2510.18193v2).
- **Conversational agent:** can be speech-to-speech with a multimodal model; GPT‑4o is positioned as a single model reasoning across audio/vision/text with low latency compared to three-model voice pipelines [[90]](https://openai.com/index/hello-gpt-4o/). (In practice, enterprise deployment still needs privacy and safety controls.)

### 5.2 Knowledge representation: how to encode “what a coach knows”
A robust sports ITS needs multiple knowledge layers:
1) **Skill decomposition model:** phases, key checkpoints, typical errors, prerequisites (mastery learning).
2) **Constraint model:** which constraints to manipulate to elicit adaptations (constraints-led approach) [[57]](https://link.springer.com/article/10.1186/s40798-020-00268-5), [[58]](https://itfcoachingreview.com/index.php/journal/article/view/319).
3) **Safety knowledge:** load progression rules, fatigue flags, “do not cue X if knee pain,” etc.
4) **Message model:** intent (correct, motivate, challenge), components (feedback, suggestion, rationale), content (spatial/temporal references). Ontology-style structures can help ensure interpretability and consistency [[89]](https://medinform.jmir.org/2022/6/e33847/).

### 5.3 Core application workflows (what the system should *do*)

**Workflow A — Technique tutoring loop (micro-loop, seconds)**
1) Capture rep (video + IMU).
2) Segment phases; compute KP metrics.
3) Decide whether to intervene now (latency constraints).
4) Deliver one actionable cue + one visualization.
5) Re-test: did the metric improve next rep?

**Workflow B — Deliberate practice plan (meso-loop, session)**
- Diagnose top 1–3 limiting subskills.
- Assign drills with constraints (e.g., reduce degrees of freedom, change target size, modify stance).
- Track success/failure and adapt difficulty (challenge point framing is referenced in AR feedback work) [[62]](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1661936/full), [[63]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12714985/).

**Workflow C — Longitudinal guidance (macro-loop, weeks)**
- Track mastery progression and variability.
- Adapt volume/intensity; incorporate readiness proxies (especially if GPS/wellness streams like SoccerMon are available) [[28]](https://www.nature.com/articles/s41597-024-03386-x).
- Provide reflection summaries (“what changed and why”), avoiding over-analysis fatigue risk highlighted in self-regulation literature [[70]](https://www.sciencedirect.com/science/article/pii/S2211266925000337).

**Workflow D — Remote coaching augmentation**
- Athlete uploads video; system generates:
  - Auto-annotations, key errors, suggested cues
  - Augmented feedback video draft
- Coach edits/approves (human-in-the-loop), similar in spirit to VisMimic’s coach-oriented workflow [[2]](https://dl.acm.org/doi/10.1145/3746059.3747794).

---

## 6) Evaluation, reporting, deployment: how to prove it works and ship responsibly

### 6.1 Benchmarks and datasets (what to use for development and ablation)

**Technique / motion understanding**
- **T3Set**: synchronized racket IMU (100 Hz, 16-dim) + video + thousands of coach targeted suggestions for table tennis techniques—directly valuable for “virtual coach” feedback generation and suggestion taxonomy learning [[95]](https://dl.acm.org/doi/10.1145/3711896.3737407).
- **MultiSenseBadminton**: multimodal badminton swings with eye tracking/body tracking/EMG/foot pressure + video and rich annotations (skill levels, outcomes, sound, ball landing), designed with coach interviews [[13]](https://www.nature.com/articles/s41597-024-03144-z), [[19]](https://github.com/dailyminiii/MultiSenseBadminton).
- **SportsPose / AthletePose3D**: sports-focused 3D pose resources, highlighting sports domain difficulty and adaptation needs [[17]](https://christianingwersen.github.io/SportsPose/), [[23]](https://github.com/calvinyeungck/AthletePose3D).
- **FineGym**: hierarchical fine-grained gymnastics actions; also documents that pose estimation failures during intense motion can undermine skeleton-based methods [[96]](https://sdolivia.github.io/FineGym/).

**Tactics / rules / explanation**
- **SPORTU**: multimodal benchmark designed for sports rules understanding and foul detection, includes open-ended explanation ground truth and expert QC procedures [[94]](https://arxiv.org/html/2410.08474v3).
- **SoccerNet / SoccerNet-v2**: long broadcast videos with large-scale timestamped action annotations and consistent rule-defined anchors [[14]](https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w34/Giancola_SoccerNet_A_Scalable_CVPR_2018_paper.pdf), [[97]](https://openaccess.thecvf.com/content/CVPR2021W/CVSports/papers/Deliege_SoccerNet-v2_A_Dataset_and_Benchmarks_for_Holistic_Understanding_of_Broadcast_CVPRW_2021_paper.pdf).

**General skeleton action recognition**
- **NTU RGB+D (60/120)**: large RGB/depth/IR/skeleton dataset with some sports-like actions; note academic-only licensing and privacy constraints [[98]](https://rose1.ntu.edu.sg/dataset/actionRecognition/).

**Athlete monitoring context**
- **SoccerMon**: open GNSS/objective streams + subjective wellness/load/injury reports for large-scale readiness modeling and missingness-aware learning [[28]](https://www.nature.com/articles/s41597-024-03386-x).

### 6.2 Experimental design: what “good evidence” looks like for a sports ITS

**A. Technical evaluation (model-level)**
- Accuracy/F1 for segmentation, error detection, skill-level classification.
- Score regression metrics for AQA-style technique scoring.
- Calibration and uncertainty (so you can gate feedback safely).

**B. Multimodal ablations (fusion validity)**
- Compare:
  - video-only vs IMU-only vs fused
  - pose-only vs pose+IMU
  - early vs late vs hybrid fusion
- Test missing modality robustness (camera occluded; IMU dropout).

**C. Real-time system evaluation**
- Measure end-to-end latency (capture→feedback). Real-time pose tracking work like DeepLabCut‑Live! provides a model for reporting latency and benchmarking across hardware, including forward prediction to offset delay [[99]](https://pmc.ncbi.nlm.nih.gov/articles/PMC7781595/), [[100]](https://elifesciences.org/articles/61909). (Different domain, but good measurement discipline.)
- Report frame rate, dropped frames, synchronization error.

**D. Human-subject learning evaluation (the ITS part)**
- Use designs that include retention/transfer, not only immediate gains:
  - The AR youth soccer study uses pre/post/follow-up and motivation instruments [[62]](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1661936/full), [[63]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12714985/).
  - Nonlinear vs linear pedagogy studies use retention (4-week) and show retention differences can diverge from immediate acquisition [[60]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11994730/).
  - A motor learning meta-analysis finds weak evidence for simplistic “feedback frequency” rules, reinforcing why retention testing is crucial [[65]](https://www.sciencedirect.com/science/article/abs/pii/S1469029222000334).
- For tactics, an AI-assisted instruction crossover design with washout shows how to evaluate learning comprehension and decision-making with satisfaction measures [[67]](https://www.sciencedirect.com/science/article/pii/S0001691825007449).

**E. Coach/athlete interaction quality**
- Consider “working alliance” style outcomes when using conversational agents; an RCT comparing perceived alliance with AI vs human coaches (Wizard-of-Oz) suggests alliance can be comparable in a session, implying relationship metrics are measurable and relevant [[107]](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1364054/full).

### 6.3 Reporting guidelines to adopt (even if you’re not “medical”)
Because sports tutors influence physical behavior and sometimes touch health-like decisions, use rigorous reporting norms:

- **TRIPOD+AI** for predictive models (e.g., injury risk proxies, readiness scores) and transparent reporting of development/validation [[101]](https://www.bmj.com/content/385/bmj-2023-078378).
- **TRIPOD‑LLM** concepts for generative components: hallucination/omission risks, evaluation metric limitations, privacy and bias concerns [[102]](https://www.nature.com/articles/s41591-024-03425-5).
- **CONSORT‑AI** when running RCT-like evaluations of AI interventions (clear description of AI’s role, human–AI interaction, error analysis) [[103]](https://www.nature.com/articles/s41591-020-1034-x), [[104]](https://www.equator-network.org/reporting-guidelines/consort-artificial-intelligence/).
- **DECIDE‑AI** for early-stage “live” evaluations emphasizing safety, human factors, dataset shift, versioning, and real-world workflow integration [[105]](https://www.bmj.com/content/377/bmj-2022-070904).  
- A Nature Communications review shows AI RCTs often under-report algorithm version/accessibility/protocol links, which are equally important for sports ITS reproducibility and safety monitoring [[106]](https://www.nature.com/articles/s41467-024-45355-3).

### 6.4 Deployment considerations: privacy, minors, bias, security, regulatory boundary

**A. Privacy and data governance**
- Consumer wearable privacy policies vary widely; transparency and vulnerability disclosure are frequent high-risk areas [[109]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12167361/). If your tutor integrates vendor wearables, you inherit risk—mitigate via data minimization, local processing, encryption, and clear consent/retention policies.

**B. Athlete autonomy and ethics**
- Ethical concerns in AI-driven injury prediction include privacy, fairness, consent, autonomy, governance, and power asymmetries (athletes vs institutions), especially salient in youth and Paralympic contexts [[108]](https://www.mdpi.com/2673-2688/6/11/283).  
**Practical mitigations:** opt-out options, clear data ownership terms, contestability of AI outputs, role-based access control, and separation between performance coaching and medical decision-making.

**C. Bias/fairness**
- Pose estimation and CV can be sensitive to lighting, apparel, skin tone, camera quality; sports systems also face domain shifts (venues, uniforms, body types). Your evaluation must stratify performance by subgroup and environment. (Be cautious about over-relying on non-peer-reviewed fairness claims; the key point is to measure, not assume.)

**D. Regulatory boundary (wellness vs medical device)**
- If the system claims diagnosis/treatment/prevention of injury in a medical sense, it may cross into regulated territory. FDA guidance on Clinical Decision Support clarifies distinctions between non-device CDS functions and device functions, depending on intended use and user context (patient/caregiver vs professional) [[110]](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/clinical-decision-support-software).  
**Recommendation:** position as performance/wellness guidance unless you pursue regulatory pathways and clinical validation.

**E. EU AI Act obligations (if operating in the EU)**
- The AI Act uses risk tiers and imposes transparency requirements for generative AI and labeling of AI-generated/modified content (deepfakes) [[111]](https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence). High-risk categories can include education/vocational contexts and certain biometric/emotion recognition uses [[111]](https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence), [[112]](https://artificialintelligenceact.eu/annex/3/).  
**Recommendation:** design for human oversight, traceability, and clear labeling of generated content; avoid prohibited manipulative designs, especially involving minors [[111]](https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence).

---

## 7) Future directions (what a “next-generation” multimodal sports ITS should prioritize)

1) **From “feedback” to “curriculum”:** integrate mastery tracking, constraints-led drill generation, and long-term progression policies (micro/meso/macro loops) [[57]](https://link.springer.com/article/10.1186/s40798-020-00268-5), [[71]](https://onlinelibrary.wiley.com/doi/abs/10.1111/jcal.12757).  
2) **Grounded generative coaching:** standardize sensor/pose-to-text representations (as in BoxingPro-style prompting) and evaluate hallucination resistance and safety [[1]](https://www.mdpi.com/2079-9292/14/21/4155), [[102]](https://www.nature.com/articles/s41591-024-03425-5).  
3) **Open, context-rich datasets + reproducibility:** address the private-dataset bottleneck and lack of longitudinal studies highlighted in sport pose estimation research [[40]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12696263/). Expand datasets like T3Set/MultiSenseBadminton with more environments, camera viewpoints, and demographic diversity [[95]](https://dl.acm.org/doi/10.1145/3711896.3737407), [[13]](https://www.nature.com/articles/s41597-024-03144-z).  
4) **Uncertainty-aware, guardrailed adaptation:** incorporate confidence gating and safe fallback policies (especially for XR real-time cues) [[77]](https://www.mdpi.com/2414-4088/10/1/11), [[42]](https://arxiv.org/html/2510.18193v2).  
5) **Personalization without fragility:** adopt continual learning and mixture-of-experts approaches that can update safely over time and operate within on-device constraints [[47]](https://www.sciencedirect.com/science/article/pii/S1574119223000755), [[48]](https://www.sciencedirect.com/science/article/pii/S1574119225000331).  
6) **Evaluation maturity:** move beyond accuracy to retention/transfer, motivation/adherence, and real-world deployment studies with strong reporting discipline [[62]](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1661936/full), [[63]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12714985/), [[65]](https://www.sciencedirect.com/science/article/abs/pii/S1469029222000334), [[93]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12520646/).

---

### Bottom line construction principle
A sports intelligent tutoring and learning guidance system “driven by multimodal data fusion” should be engineered as a **closed-loop adaptive instructional system**: multimodal sensing → synchronized perception → hybrid fusion assessment → learner model → pedagogy/safety policy → grounded feedback (often LLM-mediated) → measurable learning outcomes—validated with rigorous multimodal ablations, real-time latency tests, and longitudinal human learning evaluations, while meeting privacy/ethics/regulatory constraints [[52]](https://sagroups.ieee.org/2247-1/wp-content/uploads/sites/316/2020/01/IEEE-Project-2247_one-pager.pdf), [[1]](https://www.mdpi.com/2079-9292/14/21/4155), [[62]](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1661936/full), [[63]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12714985/), [[77]](https://www.mdpi.com/2414-4088/10/1/11), [[93]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12520646/), [[108]](https://www.mdpi.com/2673-2688/6/11/283), [[109]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12167361/).
