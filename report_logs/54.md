Query ID: 54
============================================================

QUESTION:
In the field of FinTech, machine learning algorithms are now widely applied to asset allocation and investment decisions. Examples include classic models like Mean-Variance and Black-Litterman, as well as emerging deep learning models. While these models have shown certain advantages under different market conditions, each also has its limitations. For instance, the Mean-Variance model assumes asset returns follow a normal distribution, which often doesn't align with actual market conditions. The Black-Litterman model relies on subjective view inputs, introducing a degree of subjectivity. Although deep learning models can handle complex non-linear relationships, they suffer from poor interpretability. So, what are the core differences between these various models in terms of risk measurement, return prediction, and asset allocation? And is it possible to combine their strengths to build a more general-purpose and effective modeling framework?

============================================================

RESEARCH TASKS (6 total):
  #1: Research and summarize the core formulation of classical Mean-Variance (Markowitz) portfolio theory for asset allocation, focusing on: assumptions about return distributions (normality/elliptical), risk measurement (variance/covariance, alternatives like semivariance/CVaR in extensions), estimation error sensitivity, constraints (long-only, leverage), and practical mitigations (shrinkage covariance, robust optimization, resampled efficient frontier). Include key references and any post-2020 evidence on robustness under heavy tails/regime shifts.
  #2: Research and summarize Black–Litterman (BL) portfolio model differences vs mean-variance: Bayesian combination of equilibrium implied returns with investor views, role of tau and view uncertainty (Omega), how risk is measured (covariance) and how BL affects allocations/stability, typical ways to generate systematic views using factors/ML, and modern variants (hierarchical Bayes, robust BL, entropy pooling). Include practical implementation details and references, especially any recent (post-2020) papers or practitioner notes.
  #3: Research deep learning / machine learning approaches to portfolio construction and asset allocation, covering at least three families: (1) return-forecasting models feeding an optimizer, (2) end-to-end differentiable portfolio networks (e.g., Deep Portfolio Theory, differentiable convex optimization layers), and (3) reinforcement learning / policy gradient allocation. Focus on how they define risk (e.g., variance, CVaR, drawdown, utility), how they predict returns (point/quantile/distributional forecasts), allocation mapping (constraints, turnover/cost), interpretability and overfitting issues, and best-practice evaluation (walk-forward, cost-aware). Include post-2020 empirical comparisons and key references.
  #4: Research hybrid/general-purpose frameworks that combine classical portfolio models (Mean-Variance, Black–Litterman) with machine learning/deep learning: e.g., ML-generated views in BL, Bayesian priors + ML signals, distributional/quantile forecasting feeding CVaR/robust optimization, differentiable optimization layers for constrained allocation, ensemble/regime-switching meta-models. Summarize best practices, typical architectures, and key references (preferably 2020-2026).
  #5: Research risk measurement in modern portfolio construction beyond variance: downside risk, VaR/CVaR, drawdown, regime-conditional risk, transaction cost/liquidity risk, and how these are used in classical vs ML models (including RL). Include post-2020 empirical or methodological evidence about robustness under heavy tails/non-normality/structural breaks and how robust/distributionally robust methods address this.
  #6: Research evaluation and governance best practices for ML-driven asset allocation: walk-forward / purged CV, data leakage (look-ahead, survivorship), realistic transaction cost and market impact modeling, benchmark selection, model risk management, interpretability techniques (e.g., attention explanations, attribution, constraint-based models), and stress testing/scenario analysis. Include recent references and practical checklists.

============================================================

ANSWER:
## 1) Mean–Variance (Markowitz) vs Black–Litterman vs Deep Learning: what is *structurally* different?

### 1.1 Mean–Variance (MV): “optimize a quadratic form given point estimates”
**Canonical formulation.** MV portfolio choice reduces the problem to two inputs:

- **Expected returns** \( \mu \)
- **Covariance matrix** \( \Sigma \)

and then chooses weights \(w\) to trade off expected return vs variance (or equivalently trace the efficient frontier) [[1]](http://www.ledoit.net/honey.pdf). Risk is captured entirely by the quadratic form \(w^\top \Sigma w\) (or tracking-error \(x^\top\Sigma x\) in benchmark-relative form) [[1]](http://www.ledoit.net/honey.pdf).

**Structural constraint (behavioral implication).**
- MV is a *deterministic optimizer* acting on *estimated* moments. The optimizer is “smart,” but it treats inputs as truth.
- Because the objective is quadratic and constraints are typically linear, the solution often pushes toward **extreme weights** when the estimated inputs contain noise (the optimizer exploits estimation error) [[1]](http://www.ledoit.net/honey.pdf)[[2]](https://newfrontieradvisors.com/media/rxbld4hq/estimation-error-and-portfolio-optimization-12-05.pdf).

**Key failure mode: estimation-error amplification.**
- Covariance and especially mean estimates are noisy; MV “latches onto” extreme sample covariances/means and over-bets them (“error maximization”) [[1]](http://www.ledoit.net/honey.pdf)[[2]](https://newfrontieradvisors.com/media/rxbld4hq/estimation-error-and-portfolio-optimization-12-05.pdf).
- Heavy tails worsen instability and create high turnover/transaction costs because estimated moments fluctuate more violently [[3]](https://www.sciencedirect.com/science/article/abs/pii/S0304407623000180)[[4]](https://arxiv.org/abs/2212.13996).

**Practical stabilization levers (within MV worldview).**
- Add constraints (long-only, bounds) [[1]](http://www.ledoit.net/honey.pdf).
- Improve covariance estimation (shrinkage / nonlinear shrinkage) [[1]](http://www.ledoit.net/honey.pdf)[[9]](http://www.ledoit.net/Analytical_AoS_2020.pdf).
- Robust estimation / robust optimization (median-of-means, distributionally robust) [[3]](https://www.sciencedirect.com/science/article/abs/pii/S0304407623000180)[[4]](https://arxiv.org/abs/2212.13996)[[5]](https://arxiv.org/html/2112.09959v3)[[10]](https://www.sciencedirect.com/science/article/pii/S0304407625001721).
- Resampling (Michaud) to average across parameter uncertainty and reduce corner solutions [[2]](https://newfrontieradvisors.com/media/rxbld4hq/estimation-error-and-portfolio-optimization-12-05.pdf)[[11]](https://newfrontieradvisors.com/media/xu4csq3q/out-of-sample-test.pdf).

---

### 1.2 Black–Litterman (BL): “Bayesian expected returns + MV allocation”
BL keeps the **MV risk model** (still based on \(\Sigma\)), but changes how expected returns are produced.

**Core idea.** BL treats expected returns as a Bayesian object: combine
- a **prior** (typically *market-implied equilibrium returns* \(\Pi\) via reverse optimization)
- with **views** \(Q\) on linear combinations of assets \(P\mu\),
- with **view uncertainty** \(\Omega\),
to get **posterior expected returns** \(E(R)\) [[12]](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html)[[14]](https://cims.nyu.edu/~ritter/kolm2021black.pdf)[[15]](https://people.duke.edu/~charvey/Teaching/BA453_2006/Idzorek_onBL.pdf).

Standard posterior mean formula (one common form) is [[12]](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html)[[15]](https://people.duke.edu/~charvey/Teaching/BA453_2006/Idzorek_onBL.pdf):
\[
E(R)=\left[(\tau\Sigma)^{-1}+P^\top\Omega^{-1}P\right]^{-1}\left[(\tau\Sigma)^{-1}\Pi+P^\top\Omega^{-1}Q\right]
\]

**Structural constraint (behavioral implication).**
- BL is a *regularized* MV: it anchors expected returns to an equilibrium prior, so the optimizer is less likely to treat noisy mean estimates as arbitrage.
- Views on a subset of assets propagate to the entire universe through correlation structure, reducing corner solutions [[12]](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html)[[14]](https://cims.nyu.edu/~ritter/kolm2021black.pdf).

**Key trade-off: subjectivity and specification burden.**
- BL improves stability, but introduces **new modeling degrees of freedom**: how to set \(\tau\), how to encode \(P,Q\), and especially how to specify \(\Omega\) (confidence) [[12]](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html)[[16]](https://hudsonthames.org/bayesian-portfolio-optimisation-the-black-litterman-model/).
- In practice, BL quality depends heavily on view quality and calibration—so “subjectivity” is real unless you systematize view generation.

**Modern direction: reduce subjectivity with data-driven views.**
- A 2026 Expert Systems with Applications paper proposes “objective” BL views generated by a deep learning forecasting pipeline (CEEMDAN decomposition + GA-optimized LSTM + nonlinear aggregation) and reports improved Sharpe and drawdown metrics in their tests [[41]](https://www.sciencedirect.com/science/article/abs/pii/S0957417425024856).
- A 2025 arXiv preprint similarly uses denoising + multivariate decomposition + TCN forecasting to feed BL views and reports improved risk/return vs MV/equal-weight/market-weight for a NASDAQ-100 subset [[42]](https://arxiv.org/html/2505.01781v1).
- An ICML 2025 poster proposes treating BL’s \((Q,\Omega)\) as *latent variables learned from data* within one Bayesian network, claiming improved Sharpe and reduced turnover vs Markowitz/index baselines [[43]](https://icml.cc/virtual/2025/poster/43691) (promising, but note this is a poster-level source).

**Multi-period BL matters if your investment horizon and view horizon differ.**
Classical BL is mostly single-period. “Dynamic BL” (2024 arXiv) conditions a continuous-time return process on views (Brownian-bridge style) and derives an explicit dynamic optimal policy; it reports lower turnover and less sensitivity to rebalancing interval than repeatedly solving one-period BL each rebalancing date [[44]](https://arxiv.org/pdf/2404.18822).

---

### 1.3 Deep learning / ML portfolio models: three families with different “decision plumbing”
A useful taxonomy is [[23]](https://arxiv.org/pdf/2507.01918):

1) **Predict-then-optimize** (two-stage): forecast returns/risk → feed an optimizer  
2) **End-to-end differentiable**: directly learn mapping from features/history → weights, optimizing a portfolio objective  
3) **Reinforcement learning**: learn a rebalancing policy maximizing a sequential reward (return–risk–cost)

These differ from MV/BL mainly in *where learning happens* and *what the loss function aligns with*.

#### (A) Predict-then-optimize
- Stage 1 trains a model to predict inputs (e.g., \(\hat\mu, \hat\Sigma\)) under prediction losses (MSE, etc.).
- Stage 2 solves a classical optimization (MV, risk parity, CVaR, etc.).

**Main structural issue:** prediction loss is not decision loss—better MSE does not imply better portfolios [[27]](https://arxiv.org/html/2503.13544v6). This is exactly why “decision-focused learning” emerged.

A concrete example of “risk-forecast then optimize” is deep learning CVaR forecasting used inside a CVaR-adjusted utility portfolio for crypto; CVaR is predicted by LSTM and then used in allocation, with reported improvements in risk-adjusted outcomes and drawdowns vs MV/min-var/naive [[28]](https://onlinelibrary.wiley.com/doi/10.1002/ijfe.70012).

#### (B) End-to-end differentiable portfolio networks
Here the model outputs weights directly and is trained on a portfolio objective (Sharpe, variance, utility, drawdown proxy, etc.), often enforcing constraints by construction.

Examples:
- Deep learning that directly maximizes Sharpe and uses **softmax** to enforce long-only fully-invested weights [[24]](https://www.oxford-man.ox.ac.uk/wp-content/uploads/2020/06/Deep-Learning-for-Portfolio-Optimisation.pdf).
- A “Portfolio Transformer” that also optimizes Sharpe, allows controlled shorting via a signed-softmax construction, and **includes transaction costs in the training objective** via an L1 turnover penalty [[25]](https://discovery.ucl.ac.uk/id/eprint/10172666/1/2206.03246.pdf).
- End-to-end risk budgeting with a *model-based* implicit optimization layer, trained to maximize Sharpe, reporting better out-of-sample Sharpe than pure risk parity / equal-weight in their tests [[26]](https://ideas.repec.org/a/spr/annopr/v339y2024i1d10.1007_s10479-023-05539-4.html)[[48]](https://arxiv.org/abs/2107.04636).
- A 2025 GMV-oriented end-to-end architecture that mirrors the analytical GMV form while learning “covariance cleaning”; it reports lower realized volatility, smaller max drawdown, and higher Sharpe than leading analytical covariance filters including nonlinear shrinkage, with a realistic execution framework (slippage/fees/financing) [[23]](https://arxiv.org/pdf/2507.01918).

**Structural advantage:** the training objective can match what investors care about (net Sharpe, drawdown, variance, costs), not intermediate prediction metrics.

**Structural risk:** non-convex optimization, overfitting, and instability of direct Sharpe optimization (sensitive to initialization/hyperparameters). A 2025 “Decision by Supervised Learning (DSL)” approach argues direct end-to-end Sharpe objectives can be unstable and proposes convex surrogate learning with ensembling to reduce allocation variance [[27]](https://arxiv.org/html/2503.13544v6).

#### (C) Reinforcement learning (RL)
RL learns a rebalancing policy over time. It is inherently multi-period and cost-aware *if you define reward appropriately*.

Examples:
- A 2026 attention-enhanced RL allocation approach uses a **Dirichlet policy** so weights are feasible “by construction,” supports tradability masks, and evaluates on a long S&P 500 panel with purged walk-forward backtesting; it reports improved terminal wealth / Sharpe / Sortino with realistic turnover and drawdown profiles vs baselines [[31]](https://www.sciencedirect.com/science/article/pii/S2667305325001486).
- A 2023 crypto RL framework uses cross-asset attention and policy gradients and reports lower drawdown/volatility and improved turnover vs many alternatives [[32]](https://www.sciencedirect.com/science/article/abs/pii/S030645732200348X).
- A 2023 comparative study emphasizes fair comparisons to MV by aligning objectives (Sharpe) and reports DRL outperformance on Sharpe, drawdown, returns, turnover [[33]](https://icaps23.icaps-conference.org/papers/finplan/FinPlan23_paper_4.pdf).
- A TD3-based DRL portfolio framework explicitly embeds **risk aversion and transaction costs** into an extended Markowitz mean–variance reward function [[59]](https://www.sciencedirect.com/science/article/pii/S1044028324000887).
- FinRL-Meta stresses reproducibility and highlights common pitfalls: survivorship bias and backtesting overfitting are endemic in DRL trading research [[34]](https://papers.neurips.cc/paper_files/paper/2022/file/0bf54b80686d2c4dc0808c2e98d430f7-Paper-Datasets_and_Benchmarks.pdf).

---

## 2) Core differences by dimension: **risk measurement**, **return prediction**, **allocation mapping**

### 2.1 Risk measurement: what “risk” means in each model family

#### Mean–Variance / Black–Litterman: risk = variance (usually)
- MV directly uses portfolio variance \(w^\top\Sigma w\) (or tracking-error \(x^\top\Sigma x\)) [[1]](http://www.ledoit.net/honey.pdf).
- BL typically **keeps the same covariance-based risk model** and modifies expected returns through Bayesian updating [[12]](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html)[[14]](https://cims.nyu.edu/~ritter/kolm2021black.pdf). (Some implementations also compute posterior covariance, but the risk metric is still quadratic variance-like.)

**Why variance is limiting.**
- Variance treats upside and downside symmetrically; if returns are skewed/heavy-tailed, minimizing standard deviation is not equivalent to minimizing investor-perceived risk [[5]](https://arxiv.org/html/2112.09959v3)[[56]](https://www.sciencedirect.com/science/article/abs/pii/S0950705125014959).
- Financial returns are heavy-tailed; sample-moment plug-in MV is unstable and turnover-intensive, with heavy tails explicitly identified as a cause of erratic weight changes [[3]](https://www.sciencedirect.com/science/article/abs/pii/S0304407623000180)[[4]](https://arxiv.org/abs/2212.13996).

#### Extensions: CVaR / downside risk / drawdown risk
- **CVaR optimization** (Rockafellar–Uryasev) is convex and can be solved efficiently; CVaR is coherent (unlike VaR, which can be non-convex and not subadditive) [[7]](https://sites.math.washington.edu/~rtr/papers/rtr179-CVaR1.pdf).
- Drawdown-oriented measures like **CDaR** are motivated as capturing path-dependent loss accumulation that variance/VaR can miss [[56]](https://www.sciencedirect.com/science/article/abs/pii/S0950705125014959).

**But downside measures can worsen estimation error.**
- Mean-semivariance and other downside measures require estimating tail/downside dependence; these inputs can be noisier than covariance, making performance even more sensitive to parameter uncertainty [[57]](https://link.springer.com/article/10.1007/s10479-022-04736-x).

**Mitigations:** dimension reduction and robust estimation.
- A 2022 method reduces estimation error in semivariance optimization by PCA on downside correlation with a Minimum Average Partial rule; it reports improved out-of-sample performance and “closing the gap” versus covariance-based methods [[57]](https://link.springer.com/article/10.1007/s10479-022-04736-x).

#### ML / DL / RL: risk becomes part of the objective function and reward design
Deep portfolio methods frequently use:
- **Sharpe** (variance-based, but risk-adjusted) [[24]](https://www.oxford-man.ox.ac.uk/wp-content/uploads/2020/06/Deep-Learning-for-Portfolio-Optimisation.pdf)[[25]](https://discovery.ucl.ac.uk/id/eprint/10172666/1/2206.03246.pdf)[[26]](https://ideas.repec.org/a/spr/annopr/v339y2024i1d10.1007_s10479-023-05539-4.html)
- **Realized variance minimization** (GMV-like) [[23]](https://arxiv.org/pdf/2507.01918)
- **Sortino, drawdown, turnover penalties** (often as evaluation metrics; sometimes embedded) [[31]](https://www.sciencedirect.com/science/article/pii/S2667305325001486)[[32]](https://www.sciencedirect.com/science/article/abs/pii/S030645732200348X)[[33]](https://icaps23.icaps-conference.org/papers/finplan/FinPlan23_paper_4.pdf)[[25]](https://discovery.ucl.ac.uk/id/eprint/10172666/1/2206.03246.pdf)
- **CVaR/EVaR proxies** in robust objectives (e.g., worst-window penalties) [[50]](https://arxiv.org/html/2601.05975v1)

A notable 2026 macro-futures deep portfolio manager explicitly targets regime robustness using a differentiable worst-window penalty as a proxy for EVaR and emphasizes heavy tails, regime shifts, and trading frictions [[50]](https://arxiv.org/html/2601.05975v1). This is qualitatively different from MV/BL where robustness is usually added via constraints/regularizers rather than being the primary training signal.

#### Distributionally robust optimization (DRO): risk = worst-case over plausible distributions
DRO reframes risk as *ambiguity* about the return distribution itself, optimizing worst-case performance over an ambiguity set (e.g., Wasserstein/optimal transport balls) [[54]](https://www.cambridge.org/core/journals/acta-numerica/article/distributionally-robust-optimization/5B4E65E3A5A2AEF24E218A6B34E6EAA2).

Portfolio-relevant DRO results in the sources include:
- Distributionally robust mean–variance that becomes empirical variance minimization plus a regularization term under Wasserstein ambiguity, with backtests on S&P 500 mentioned [[63]](https://pubsonline.informs.org/doi/10.1287/mnsc.2021.4155).
- “Robustifying conditional portfolio decisions via optimal transport” integrates side information + conditional estimation + DRO; for mean-variance or mean-CVaR it can be reformulated as tractable conic programs, with empirical advantages reported for US equities [[49]](https://arxiv.org/abs/2103.16451).
- Regime-switching DRO-CVaR frameworks designed for nonstationarity and crisis regime changes, with strong empirical claims of responsiveness in the 2008 crisis [[61]](https://ideas.repec.org/a/inm/ormsom/v25y2023i5p1779-1795.html).

**Bottom line on “risk”:**
- **MV/BL**: risk is mainly second-moment (variance/covariance).
- **Modern practice & ML**: risk is increasingly *tail-, drawdown-, cost-, and regime-aware*, sometimes via coherent risk measures (CVaR/EVaR) or worst-case ambiguity sets (DRO).

---

### 2.2 Return prediction: explicit vs implicit and what is being predicted

#### Mean–Variance
- Requires explicit \(\mu\). In practice, expected return estimates are notoriously noisy; MV sensitivity to mean error is a major cause of out-of-sample failure [[2]](https://newfrontieradvisors.com/media/rxbld4hq/estimation-error-and-portfolio-optimization-12-05.pdf)[[3]](https://www.sciencedirect.com/science/article/abs/pii/S0304407623000180).

#### Black–Litterman
- Does not remove the need for expected returns; it **replaces raw historical \(\mu\)** with a posterior that blends:
  - equilibrium prior \(\Pi\) (reverse optimized from market weights) [[12]](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html)[[15]](https://people.duke.edu/~charvey/Teaching/BA453_2006/Idzorek_onBL.pdf)[[16]](https://hudsonthames.org/bayesian-portfolio-optimisation-the-black-litterman-model/)
  - views \(Q\) with confidences \(\Omega\) [[12]](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html)[[14]](https://cims.nyu.edu/~ritter/kolm2021black.pdf)[[15]](https://people.duke.edu/~charvey/Teaching/BA453_2006/Idzorek_onBL.pdf)

So BL is a *return estimation framework* more than a “new optimizer.” Its stability comes from shrinking toward a market-consistent prior and spreading view impacts through correlations [[12]](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html)[[14]](https://cims.nyu.edu/~ritter/kolm2021black.pdf).

**Modern BL: views can be generated by ML**
- Deep learning forecasting pipelines used to create “objective views” and improve performance are explicitly proposed and empirically tested in recent work [[41]](https://www.sciencedirect.com/science/article/abs/pii/S0957417425024856)[[42]](https://arxiv.org/html/2505.01781v1).
- A latent-variable BL approach tries to learn views and uncertainties directly from data within one Bayesian model [[43]](https://icml.cc/virtual/2025/poster/43691).

#### Deep learning / RL
- Many end-to-end methods **avoid predicting returns explicitly** and instead optimize portfolio-level objectives (Sharpe/variance/utility) directly [[24]](https://www.oxford-man.ox.ac.uk/wp-content/uploads/2020/06/Deep-Learning-for-Portfolio-Optimisation.pdf)[[25]](https://discovery.ucl.ac.uk/id/eprint/10172666/1/2206.03246.pdf).
- Other pipelines predict **risk quantities** (e.g., CVaR) or conditional volatilities and then optimize [[28]](https://onlinelibrary.wiley.com/doi/10.1002/ijfe.70012)[[52]](https://www.sciencedirect.com/science/article/pii/S2405918821000155)[[53]](https://arxiv.org/abs/2003.00656).
- Regime-robust deep portfolio systems focus on representation learning + robust objective rather than point return forecasts [[50]](https://arxiv.org/html/2601.05975v1).

**Decision-focused learning (bridge between prediction and allocation).**
A 2024 ICAIF paper shows how decision-focused learning (DFL) for MV changes the return predictor: the gradient effectively tilts prediction errors by \(\Sigma^{-1}\), so the model learns forecasts that improve portfolio decisions rather than MSE [[46]](https://dl.acm.org/doi/full/10.1145/3768292.3770423). This is a key conceptual bridge: it explains *why* a “worse predictor” by MSE might be a *better allocator*.

---

### 2.3 Allocation mapping: how forecasts/risk become portfolio weights (and how constraints are handled)

#### MV / BL: explicit constrained optimization
- Weights are the solution to a quadratic program with constraints like:
  - fully invested \(w^\top \mathbf{1}=1\) [[1]](http://www.ledoit.net/honey.pdf)
  - long-only \(w\ge 0\) [[1]](http://www.ledoit.net/honey.pdf)
  - position bounds (e.g., \(w_i \le c\)) [[1]](http://www.ledoit.net/honey.pdf)
- Constraints stabilize but may reduce theoretical optimality; still, constraints are ubiquitous in real management [[1]](http://www.ledoit.net/honey.pdf)[[2]](https://newfrontieradvisors.com/media/rxbld4hq/estimation-error-and-portfolio-optimization-12-05.pdf).

**Stability tools inside classical optimization**
- Shrinkage covariance estimators directly reduce instability and improve out-of-sample behavior [[1]](http://www.ledoit.net/honey.pdf)[[9]](http://www.ledoit.net/Analytical_AoS_2020.pdf).
- Robust Markowitz methods (median-of-means + projected gradient ideas) report reduced turnover and preserved/improved performance relative to shrinkage/constrained portfolios, explicitly attributing instability to heavy tails [[4]](https://arxiv.org/abs/2212.13996)[[3]](https://www.sciencedirect.com/science/article/abs/pii/S0304407623000180).
- Resampled efficient frontier averages across parameter uncertainty and tends to produce diversified, smoother portfolios with lower risk at similar returns in simulations [[2]](https://newfrontieradvisors.com/media/rxbld4hq/estimation-error-and-portfolio-optimization-12-05.pdf)[[11]](https://newfrontieradvisors.com/media/xu4csq3q/out-of-sample-test.pdf).

#### End-to-end deep allocation: constraints by construction
- Long-only fully invested: softmax on raw outputs [[24]](https://www.oxford-man.ox.ac.uk/wp-content/uploads/2020/06/Deep-Learning-for-Portfolio-Optimisation.pdf).
- Controlled long-short: signed-softmax with L1 normalization of gross exposure [[25]](https://discovery.ucl.ac.uk/id/eprint/10172666/1/2206.03246.pdf).
- Costs/turnover included directly: subtract \(C\sum_i|w_{t-1}-w_{t-2}|\) from return stream in the training objective [[25]](https://discovery.ucl.ac.uk/id/eprint/10172666/1/2206.03246.pdf).
- Model-based implicit allocation layers (risk budgeting) embed structure and constraints inside the network [[26]](https://ideas.repec.org/a/spr/annopr/v339y2024i1d10.1007_s10479-023-05539-4.html)[[48]](https://arxiv.org/abs/2107.04636).

#### RL allocation: feasibility and frictions depend on action parameterization and reward
- Dirichlet policies naturally output simplex weights and can support tradability masks [[31]](https://www.sciencedirect.com/science/article/pii/S2667305325001486).
- Reward shaping can incorporate transaction costs, risk penalties, drawdown proxies, and can adapt by market state (some recent work highlights the need for adaptive reward designs) [[62]](https://www.sciencedirect.com/science/article/pii/S2666764926000081)[[59]](https://www.sciencedirect.com/science/article/pii/S1044028324000887).

---

## 3) Where each approach tends to work best—and why it fails

### 3.1 MV tends to work when:
- Covariance estimation is reasonably stable (large samples, lower dimension, or strong shrinkage/structure).
- The objective is risk-centric (e.g., GMV) rather than relying heavily on mean estimates.
- You accept strong constraints and turnover controls.

**Fails when:**
- Mean estimates are noisy, especially with many assets → corner solutions [[2]](https://newfrontieradvisors.com/media/rxbld4hq/estimation-error-and-portfolio-optimization-12-05.pdf)[[3]](https://www.sciencedirect.com/science/article/abs/pii/S0304407623000180).
- Heavy tails/regime shifts cause moment instability and turnover [[4]](https://arxiv.org/abs/2212.13996)[[3]](https://www.sciencedirect.com/science/article/abs/pii/S0304407623000180).

### 3.2 BL tends to work when:
- You want a **stable baseline** anchored to market equilibrium and a systematic way to incorporate additional information.
- You can specify or learn **credible views and uncertainties**.

**Fails when:**
- Views are poor, overconfident, or inconsistently calibrated (subjectivity).
- The single-period Gaussian/linear-view assumptions are materially wrong (nonlinear payoffs; tail events) [[14]](https://cims.nyu.edu/~ritter/kolm2021black.pdf).

### 3.3 Deep learning / RL tends to work when:
- There is enough data and enough structure/regularization to avoid overfitting.
- You incorporate costs, constraints, and realistic execution into the objective and evaluation (otherwise paper alpha evaporates).
- You evaluate across multiple regimes with leakage-resistant protocols.

**Fails when:**
- You optimize non-convex objectives (Sharpe) naively and get unstable results; sensitivity to initialization/hyperparameters is real [[27]](https://arxiv.org/html/2503.13544v6).
- You inadvertently leak information (look-ahead, survivorship) or overfit backtests [[23]](https://arxiv.org/pdf/2507.01918)[[34]](https://papers.neurips.cc/paper_files/paper/2022/file/0bf54b80686d2c4dc0808c2e98d430f7-Paper-Datasets_and_Benchmarks.pdf).
- Interpretability and governance requirements are ignored (hard to deploy in institutional settings).

---

## 4) Can we combine the strengths? Yes—an effective “general-purpose” framework is usually **modular + Bayesian + robust + cost-aware + well-governed**

The most credible path is not “pick one model,” but build a **stack** where each layer does what it’s best at:

### Layer 1 — Data & evaluation hygiene (non-negotiable)
**Goal:** stop fooling yourself before choosing any model.

Key practices from the sources:
- Use **purged walk-forward** / purged CV to reduce leakage; add embargo where relevant [[67]](https://en.wikipedia.org/wiki/Purged_cross-validation).
- Control survivorship bias (e.g., use survivorship-safe universes; FinRL-Meta highlights this as a major RL pitfall) [[34]](https://papers.neurips.cc/paper_files/paper/2022/file/0bf54b80686d2c4dc0808c2e98d430f7-Paper-Datasets_and_Benchmarks.pdf).
- Include transaction costs, turnover, and (if possible) slippage/financing in both training and evaluation; several deep portfolio studies explicitly embed or simulate costs [[25]](https://discovery.ucl.ac.uk/id/eprint/10172666/1/2206.03246.pdf)[[23]](https://arxiv.org/pdf/2507.01918).
- Avoid backtest overfitting; treat repeated tuning as a model risk problem [[34]](https://papers.neurips.cc/paper_files/paper/2022/file/0bf54b80686d2c4dc0808c2e98d430f7-Paper-Datasets_and_Benchmarks.pdf)[[68]](https://portfoliooptimizationbook.com/slides/slides-backtesting.pdf).

### Layer 2 — Risk model (covariance) as a “shared service”
**Recommendation:** treat covariance estimation/cleaning as its own module used by MV, BL, and ML.

Options with strong grounding in the sources:
- **Shrinkage / nonlinear shrinkage** covariance estimation to stabilize MV-type optimizers [[1]](http://www.ledoit.net/honey.pdf)[[9]](http://www.ledoit.net/Analytical_AoS_2020.pdf).
- **Robust Markowitz-style covariance/gradient estimation** for heavy-tailed data to reduce turnover [[4]](https://arxiv.org/abs/2212.13996)[[3]](https://www.sciencedirect.com/science/article/abs/pii/S0304407623000180).
- **End-to-end learned covariance cleaning** (GMV-focused) if you can validate thoroughly; one 2025 model claims it outperforms state-of-the-art analytical nonlinear shrinkage out-of-sample with realistic execution assumptions [[23]](https://arxiv.org/pdf/2507.01918).

### Layer 3 — Return (or “alpha”) model with uncertainty, not just point forecasts
Here you choose one of three paradigms depending on your use case:

#### (A) Bayesian prior + ML “views” (BL as the fusion engine)
This is often the most institutionally palatable hybrid because it is interpretable:
- Prior: market-implied equilibrium returns \(\Pi\) [[12]](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html)[[15]](https://people.duke.edu/~charvey/Teaching/BA453_2006/Idzorek_onBL.pdf).
- Views: from ML forecasts (deep models, factor models, macro signals) [[41]](https://www.sciencedirect.com/science/article/abs/pii/S0957417425024856)[[42]](https://arxiv.org/html/2505.01781v1).
- View confidence: from model uncertainty calibration (or Idzorek-style mapping if you must) [[12]](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html)[[15]](https://people.duke.edu/~charvey/Teaching/BA453_2006/Idzorek_onBL.pdf).

Recent work explicitly implements this “objective views” BL hybrid with deep learning and reports improvements [[41]](https://www.sciencedirect.com/science/article/abs/pii/S0957417425024856)[[42]](https://arxiv.org/html/2505.01781v1). A further step is learning \((Q,\Omega)\) as latent variables in a unified Bayesian model to reduce manual specification (promising but early-stage) [[43]](https://icml.cc/virtual/2025/poster/43691).

#### (B) Decision-focused learning (DFL) for MV / risk-based optimization
If you insist on MV as the allocator, DFL helps align the forecaster with the portfolio outcome:
- Learn \(\hat\mu\) using gradients that pass through the optimizer; in MV this induces \(\Sigma^{-1}\)-weighted learning signals [[46]](https://dl.acm.org/doi/full/10.1145/3768292.3770423).
- This can intentionally bias forecasts in ways that improve allocations even if MSE worsens [[46]](https://dl.acm.org/doi/full/10.1145/3768292.3770423).

#### (C) Distributional forecasting for tail-aware optimization
If your mandate cares about tail risk/drawdown:
- Forecast quantiles/tails or directly forecast risk functionals (CVaR), then optimize CVaR/utility [[28]](https://onlinelibrary.wiley.com/doi/10.1002/ijfe.70012).
- Or skip explicit forecasting and use distributionally robust optimization (next layer).

### Layer 4 — Allocation engine: convex optimization, robust optimization, or differentiable constrained layers
A robust “general-purpose” allocator should support constraints, costs, and risk limits.

**Option 1: Classical convex optimizer (MV / CVaR / CDaR / risk budgeting)**
- MV is quadratic (fast).
- CVaR is convex and can be solved efficiently; it avoids VaR’s non-convexity pitfalls [[7]](https://sites.math.washington.edu/~rtr/papers/rtr179-CVaR1.pdf).
- Drawdown-aware models (e.g., mean-CDaR) can be used when path risk is central [[56]](https://www.sciencedirect.com/science/article/abs/pii/S0950705125014959).

**Option 2: Distributionally Robust Optimization (DRO)**
When nonstationarity/heavy tails/model misspecification are dominant:
- Optimal transport / Wasserstein DRO gives worst-case control and can reduce to regularized forms; conditional DRO can be reformulated into tractable conic programs for mean-variance and mean-CVaR [[49]](https://arxiv.org/abs/2103.16451)[[63]](https://pubsonline.informs.org/doi/10.1287/mnsc.2021.4155).
- Regime-switching DRO frameworks explicitly handle structural breaks and claim improved crisis responsiveness [[61]](https://ideas.repec.org/a/inm/ormsom/v25y2023i5p1779-1795.html).

**Option 3: Differentiable optimization layers (for hybrid deep + hard constraints)**
To combine deep signals with guaranteed feasibility:
- Use differentiable convex optimization layers (e.g., CVXPYlayers) so the network outputs parameters and the layer outputs constrained optimal weights, while gradients flow end-to-end [[22]](https://github.com/cvxpy/cvxpylayers)[[21]](https://arxiv.org/abs/1910.12430).
- This is a practical way to get *both*: deep feature extraction + hard constraints + transparent KKT/dual diagnostics (CVXPYlayers can output dual variables) [[22]](https://github.com/cvxpy/cvxpylayers).

### Layer 5 — Robust objective: include frictions and “bad times” explicitly
A major weakness of many ML portfolios is optimizing average performance while ignoring concentrated losses in a few windows.

Tools seen in the sources:
- Embed turnover costs in the objective (e.g., Portfolio Transformer’s explicit L1 turnover cost) [[25]](https://discovery.ucl.ac.uk/id/eprint/10172666/1/2206.03246.pdf).
- Use **worst-window / tail-sensitive penalties**; a 2026 macro-futures deep portfolio manager uses a differentiable worst-window penalty as an EVaR proxy for regime robustness [[50]](https://arxiv.org/html/2601.05975v1).
- If using downside measures, mitigate estimation risk (dimension reduction for semivariance [[57]](https://link.springer.com/article/10.1007/s10479-022-04736-x), performance-based regularization for downside moments [[64]](https://www.sciencedirect.com/science/article/pii/S0927538X26000302)).

### Layer 6 — Ensemble / regime switching meta-controller (optional but powerful)
Markets switch regimes; one allocator is rarely optimal everywhere.
- Regime-switching DRO explicitly encodes this into the ambiguity set and reports improved crisis response [[61]](https://ideas.repec.org/a/inm/ormsom/v25y2023i5p1779-1795.html).
- Regime-switching strategy research shows state models can improve drawdown/Sharpe under realistic frictions [[58]](https://arxiv.org/abs/2402.05272).
- Hybrid systems combining HMM-style regimes with neural signals and then BL optimization appear in recent preprints [[51]](https://arxiv.org/html/2407.19858v6) (treat as exploratory until independently validated).

A pragmatic approach is a **mixture-of-experts**:
- Expert A: robust GMV / low-risk
- Expert B: BL + macro views
- Expert C: trend/momentum
- Gating network/regime model chooses blend weights, under turnover constraints.

---

## 5) A concrete “combined strengths” blueprint (implementable)

Here is a practical architecture that directly synthesizes the best pieces:

### Step 1 — Covariance / risk estimation (robust + stable)
- Start with shrinkage / nonlinear shrinkage covariance estimation [[1]](http://www.ledoit.net/honey.pdf)[[9]](http://www.ledoit.net/Analytical_AoS_2020.pdf).
- If heavy tails are severe, consider robust Markowitz-style methods aimed at turnover stabilization under heavy tails [[4]](https://arxiv.org/abs/2212.13996)[[3]](https://www.sciencedirect.com/science/article/abs/pii/S0304407623000180).

### Step 2 — Return model as Bayesian fusion (BL) with ML-generated views
- Compute equilibrium prior \(\Pi = \delta \Sigma w_{mkt}\) [[12]](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html)[[15]](https://people.duke.edu/~charvey/Teaching/BA453_2006/Idzorek_onBL.pdf).
- Generate views \(Q\) using ML forecasts (could be deep time-series models, decomposition ensembles, factor models) [[41]](https://www.sciencedirect.com/science/article/abs/pii/S0957417425024856)[[42]](https://arxiv.org/html/2505.01781v1).
- Calibrate \(\Omega\) from forecast uncertainty (preferred) or confidence mapping methods when necessary [[12]](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html)[[15]](https://people.duke.edu/~charvey/Teaching/BA453_2006/Idzorek_onBL.pdf).
- Produce posterior expected returns using BL [[12]](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html)[[15]](https://people.duke.edu/~charvey/Teaching/BA453_2006/Idzorek_onBL.pdf).
This retains interpretability (prior + views) while reducing pure subjectivity.

### Step 3 — Allocation engine: robust + constraint-aware
Choose based on mandate:
- If variance-centric: MV with strong constraints + turnover penalty (or post-trade smoothing).
- If tail/drawdown-centric: CVaR (coherent, convex) [[7]](https://sites.math.washington.edu/~rtr/papers/rtr179-CVaR1.pdf), CDaR-style objectives [[56]](https://www.sciencedirect.com/science/article/abs/pii/S0950705125014959).
- If uncertainty/model risk-centric: conditional DRO with optimal transport ambiguity; mean-variance/mean-CVaR become conic programs [[49]](https://arxiv.org/abs/2103.16451).

### Step 4 — Make the allocator differentiable (optional but increasingly standard)
If you want end-to-end training (decision-focused learning or deep features):
- Implement the optimizer as a differentiable convex layer (CVXPYlayers) [[22]](https://github.com/cvxpy/cvxpylayers)[[21]](https://arxiv.org/abs/1910.12430).
- Train the upstream model on decision objectives (net Sharpe, drawdown proxy, turnover-aware utility), not pure MSE [[27]](https://arxiv.org/html/2503.13544v6)[[46]](https://dl.acm.org/doi/full/10.1145/3768292.3770423).

### Step 5 — Cost, turnover, and execution realism in training *and* evaluation
- Penalize turnover explicitly (as in PT) [[25]](https://discovery.ucl.ac.uk/id/eprint/10172666/1/2206.03246.pdf).
- Validate under realistic slippage/fees/financing where possible (as in the GMV deep model’s “realistic implementation framework”) [[23]](https://arxiv.org/pdf/2507.01918).
- If your environment is asynchronous (global futures), ensure features are causally measurable; one 2026 system argues naive cross-asset attention can create subtle look-ahead bias and proposes directed delay mechanisms [[50]](https://arxiv.org/html/2601.05975v1).

### Step 6 — Governance and model risk management (institutional requirement)
If this is FinTech production, you need a model risk framework:
- SR 11-7 defines model risk broadly and requires conceptual soundness review, ongoing monitoring, and outcomes analysis/backtesting, with independent validation and governance proportional to materiality [[72]](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm).
- IOSCO highlights expanding AI use cases and stresses governance, data/model issues, third-party dependency, and human–AI interaction risks [[73]](https://www.iosco.org/library/pubdocs/pdf/IOSCOPD788.pdf).

---

## 6) Summary: the “core differences” in one comparative lens

### Risk measurement
- **MV**: variance/covariance (quadratic risk); brittle under non-normal heavy tails unless robustified [[1]](http://www.ledoit.net/honey.pdf)[[3]](https://www.sciencedirect.com/science/article/abs/pii/S0304407623000180)[[4]](https://arxiv.org/abs/2212.13996).
- **BL**: same covariance risk, but more stable expected returns via Bayesian shrinkage to equilibrium [[12]](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html)[[14]](https://cims.nyu.edu/~ritter/kolm2021black.pdf).
- **Deep learning/RL**: risk is whatever you put in the loss/reward (Sharpe/variance/CVaR/drawdown/turnover); can be regime-robust if designed so [[24]](https://www.oxford-man.ox.ac.uk/wp-content/uploads/2020/06/Deep-Learning-for-Portfolio-Optimisation.pdf)[[25]](https://discovery.ucl.ac.uk/id/eprint/10172666/1/2206.03246.pdf)[[31]](https://www.sciencedirect.com/science/article/pii/S2667305325001486)[[50]](https://arxiv.org/html/2601.05975v1).
- **DRO/robust optimization**: risk = worst-case distribution within ambiguity set; tractable for mean-variance/mean-CVaR under optimal transport formulations [[49]](https://arxiv.org/abs/2103.16451)[[54]](https://www.cambridge.org/core/journals/acta-numerica/article/distributionally-robust-optimization/5B4E65E3A5A2AEF24E218A6B34E6EAA2)[[63]](https://pubsonline.informs.org/doi/10.1287/mnsc.2021.4155).

### Return prediction
- **MV**: requires explicit \(\mu\) (most fragile input) [[2]](https://newfrontieradvisors.com/media/rxbld4hq/estimation-error-and-portfolio-optimization-12-05.pdf)[[3]](https://www.sciencedirect.com/science/article/abs/pii/S0304407623000180).
- **BL**: \(\mu\) is posterior from prior + views; views can be subjective or ML-generated; confidence calibration is central [[12]](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html)[[15]](https://people.duke.edu/~charvey/Teaching/BA453_2006/Idzorek_onBL.pdf)[[41]](https://www.sciencedirect.com/science/article/abs/pii/S0957417425024856).
- **End-to-end deep**: often no explicit return forecasts—directly learns weights to maximize portfolio objective [[24]](https://www.oxford-man.ox.ac.uk/wp-content/uploads/2020/06/Deep-Learning-for-Portfolio-Optimisation.pdf)[[25]](https://discovery.ucl.ac.uk/id/eprint/10172666/1/2206.03246.pdf).
- **RL**: does not forecast returns explicitly; learns policy from reward feedback, but is sensitive to environment realism and leakage [[34]](https://papers.neurips.cc/paper_files/paper/2022/file/0bf54b80686d2c4dc0808c2e98d430f7-Paper-Datasets_and_Benchmarks.pdf).

### Asset allocation mapping
- **MV/BL**: explicit optimization with constraints; stable if covariance is stable and constraints are realistic [[1]](http://www.ledoit.net/honey.pdf).
- **Deep end-to-end**: mapping is learned; constraints via softmax/signed-softmax; costs can be included in objective [[24]](https://www.oxford-man.ox.ac.uk/wp-content/uploads/2020/06/Deep-Learning-for-Portfolio-Optimisation.pdf)[[25]](https://discovery.ucl.ac.uk/id/eprint/10172666/1/2206.03246.pdf).
- **RL**: mapping is a policy; feasibility via Dirichlet/simplex parameterizations; costs must be in reward to matter [[31]](https://www.sciencedirect.com/science/article/pii/S2667305325001486)[[59]](https://www.sciencedirect.com/science/article/pii/S1044028324000887).

---

## 7) Direct answer to your final question: “Can we combine strengths into a general-purpose effective framework?”
Yes—but the winning “general-purpose” framework is rarely a single monolithic model. It is typically:

1) **Bayesian anchoring (BL-style priors)** to prevent mean-estimate noise from dominating  
2) **Robust risk estimation / DRO** to handle heavy tails and misspecification  
3) **ML-generated views or decision-focused learning** to extract nonlinear signals from rich features  
4) **Constrained optimization (possibly differentiable)** to guarantee feasibility, exposure limits, turnover control  
5) **Cost-aware objectives and regime-aware evaluation** to survive in live trading  
6) **Governance** consistent with model risk management expectations (validation, monitoring, documentation, independent challenge) [[72]](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)[[73]](https://www.iosco.org/library/pubdocs/pdf/IOSCOPD788.pdf)

If you want one “template” sentence:

> **A robust, production-grade allocator in 2026 is often “BL (or BL-like Bayesian fusion) + robust covariance/DRO + tail/cost-aware constrained optimization, with ML used to generate views or to learn representations, and differentiable optimization used to align learning with the decision objective—validated under leakage-resistant walk-forward protocols and governed under a formal model risk framework.”**  

This combination directly addresses the weaknesses you listed:
- MV’s distributional/estimation fragility → shrinkage/robust Markowitz/DRO [[1]](http://www.ledoit.net/honey.pdf)[[4]](https://arxiv.org/abs/2212.13996)[[49]](https://arxiv.org/abs/2103.16451)  
- BL’s subjectivity → ML/latent-variable views + calibrated \(\Omega\) [[41]](https://www.sciencedirect.com/science/article/abs/pii/S0957417425024856)[[43]](https://icml.cc/virtual/2025/poster/43691)  
- Deep learning interpretability/stability → constrained layers, modular architectures, robust objectives, ensembles, and governance [[22]](https://github.com/cvxpy/cvxpylayers)[[27]](https://arxiv.org/html/2503.13544v6)[[72]](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)  

If you share your target asset universe (size, asset classes), rebalance horizon, constraints, and primary risk metric (variance vs drawdown vs CVaR), I can propose a specific end-to-end design (mathematical objective + constraints + training/evaluation protocol) consistent with these principles.
