Query ID: 19
============================================================

QUESTION:
prometheus 的高流失率会造成什么影响，有什么系统的方案可以解决？各家云厂商有没有现有方案？

============================================================

RESEARCH TASKS (6 total):
  #1: 调研 Prometheus 高流失率（high churn，指标/时间序列 churn）对系统的具体影响与故障模式：对 TSDB（WAL、head block、compaction）、内存/GC、抓取与写入吞吐、查询与规则评估、告警稳定性、remote_write 队列、以及 Thanos/Cortex/Mimir 等下游的放大效应。请给出可操作的观察指标、典型症状、以及来自官方文档/社区的权威表述与建议。
  #2: 调研并整理一套系统化的 Prometheus 高 churn 治理/缓解方案，覆盖指标设计与治理（label 规范、避免高基数/动态标签、Recording Rules 预聚合、白黑名单、drop/keep relabeling、采样与聚合、调试指标分层）、抓取与实例工程化（分片/水平拆分、多 Prometheus 分区、agent 模式、资源与 TSDB 参数、容量模型与压测）、以及对 Kubernetes 高频变更目标的最佳实践。请尽量引用官方文档与成熟实践。
  #3: 调研截至 2026-02 各主要云厂商的托管 Prometheus/监控方案在高 churn（高基数/高写入速率/高 series churn）方面的现有能力、配额/限制与治理工具：AWS Amazon Managed Service for Prometheus(AMP)、GCP Managed Service for Prometheus、Azure Monitor managed Prometheus、阿里云/腾讯云/华为云托管 Prometheus；重点关注 ingestion/active series 限制、cardinality/churn 诊断、成本控制、自动扩缩容、remote_write、以及自建 vs 托管选型建议。
  #4: 补充调研：Prometheus 高 churn 对查询性能、规则评估（recording/alerting rules）、告警稳定性（抖动/延迟/丢告警）有哪些具体影响？请重点查找官方文档、官方博客、runbooks、社区权威文章/issue 中对 query engine、rule evaluation、alertmanager 行为的描述，以及可用的自监控指标（如 prometheus_engine_query_duration_seconds、prometheus_rule_evaluation_duration_seconds、prometheus_rule_group_last_duration_seconds 等）与典型阈值/排障思路。
  #5: 补充调研：在 Prometheus 中系统降低 churn 的工程手段与具体配置抓手，尤其是 target_relabel_configs、metric_relabel_configs、labeldrop/labelkeep、drop/keep、hashmod/sharding、federation 与多 Prometheus 分区、以及 Kubernetes 场景（Pod/Job 高频变化、服务发现标签映射）最佳实践。请尽量给出来自官方文档/Prometheus Operator 文档的具体配置片段或明确语义。
  #6: 进一步调研除 AWS/GCP/Azure 之外云厂商托管 Prometheus 在高 churn/高基数治理方面的明确限制与能力：腾讯云 TMP（active series/ingestion/丢弃策略、限额配置与告警）、阿里云 Managed Service for Prometheus V2（后端 SLS Metricstore 的写入/索引/活跃序列限制与治理工具）、华为云 AOM Prometheus（是否有 series/写入配额、丢弃/限流指标、cardinality 管理功能）。优先查找官方“limits/quotas/计费/最佳实践/排障”文档。

============================================================

ANSWER:
下面把问题拆成三块：**(A) Prometheus 的“高流失率/高 churn”到底是什么、怎么量化**；**(B) 高 churn 会带来哪些具体影响/故障模式**；**(C) 一套可系统落地的治理方案**；最后再对比 **(D) 各云厂商托管 Prometheus 在高 churn/高基数方面的现有能力与限制**。

---

## A. 什么是 Prometheus 的高 churn？如何量化？

### 1) 概念：高 churn ≠ 仅仅“高基数”
- **高基数（cardinality）**：同一时刻或较长时间范围内，系统中存在非常多的时间序列（series）。
- **高 churn（流失率/更替率）**：时间序列集合在时间维度上变化很快——**新 series 不断出现、旧 series 很快消失或变成不活跃**。Robust Perception 直说 Prometheus 资源效率的核心问题就是“你有多少 time series，以及这些 time series 集合变化得有多频繁（churn）”[[1]](https://www.robustperception.io/using-tsdb-analyze-to-investigate-churn-and-cardinality)。

典型 churn 来源：
- **动态/无界 label**：把 user_id、request_id、pod_uid、container_id、随机字符串等放进 label；官方明确警告高基数 label（例如 user ID、email 等）会显著增加存储数据量[[13]](https://prometheus.io/docs/practices/naming/)，而“频繁变化的属性/标签”会进一步造成 churn。
- **K8s 高频重建**：Pod/Job/CronJob 大量短生命周期实例，`pod`/`instance` 等 label 随重建变化。
- **OTel 资源属性提升为 label**：Prometheus OTLP 接收端如果开启 `promote_all_resource_attributes`，官方明确警告：收到的 attributes 变化可能导致 time series churn 并让 Prometheus 内存飙升[[15]](https://prometheus.io/docs/prometheus/latest/configuration/configuration/)。
- **扩缩容/分片方式不当导致的再分布**：例如 Thanos Receive 文档明确指出 hashmod 算法在新增 receiver 节点时会引发 series churn 与内存尖峰，不推荐用于新部署[[9]](https://thanos.io/tip/components/receive.md/)。

---

### 2) 观测 churn 的“最实用三件套”

#### (1) `scrape_series_added`：抓取侧新增 series（定位“谁在 churn”最直接）
Prometheus 2.10 引入 `scrape_series_added`，含义是“一次 scrape 新创建了多少 series”[[2]](https://www.robustperception.io/finding-churning-targets-in-prometheus-with-scrape_series_added)。它的价值在于：**只看 `scrape_samples_scraped` 往往看不出 churn**，但 `scrape_series_added` 能直接暴露“每小时新增多少 series”[[2]](https://www.robustperception.io/finding-churning-targets-in-prometheus-with-scrape_series_added)。

官方/社区常用定位语句（Robust Perception 给了现成例子）：
```promql
topk(10, sum without(instance) (sum_over_time(scrape_series_added[1h])))
```
用于找 1 小时内 churn 最大的 targets[[2]](https://www.robustperception.io/finding-churning-targets-in-prometheus-with-scrape_series_added)。

注意事项：该指标可能 under/over-report；失败 scrape 的边界情况也可能影响计数；重启 Prometheus 或变更 scrape config 会影响 scrape cache[[2]](https://www.robustperception.io/finding-churning-targets-in-prometheus-with-scrape_series_added)。

#### (2) `prometheus_tsdb_head_series_created_total` + `prometheus_tsdb_head_series`：TSDB 视角的 churn 与“头部膨胀”
维护者在排查内存增长时建议直接看：
- `rate(prometheus_tsdb_head_series_created_total[5m])`：是否持续创建新序列（churn）[[38]](https://github.com/prometheus/prometheus/issues/10925)
- `prometheus_tsdb_head_series`：head 里当前 series 数，是否能在 GC 周期下降[[38]](https://github.com/prometheus/prometheus/issues/10925)[[5]](https://github.com/prometheus/prometheus/issues/10982)

维护者解释了正常机制：samples 进内存 head，**每 2 小时写一个 block**，之后 head GC 会修剪不再更新的 chunks，因此“与 churn 相关的内存堆积应在 2 小时后减少；`tsdb_head_series` 应每两小时出现下降”[[38]](https://github.com/prometheus/prometheus/issues/10925)[[5]](https://github.com/prometheus/prometheus/issues/10982)。如果一直不降，就要怀疑 compaction/写块/GC 没正常工作或资源不足等[[5]](https://github.com/prometheus/prometheus/issues/10982)。

#### (3) `promtool tsdb analyze`：离线找“哪些 labels/metrics 在 churn 或基数过高”
Robust Perception 的 `tsdb analyze` 输出能把 churn 与高基数按 label pairs / label names / metric names 排名，帮助你定位“到底是哪些维度导致 churn/稀疏”[[1]](https://www.robustperception.io/using-tsdb-analyze-to-investigate-churn-and-cardinality)。适合做**治理前审计**和**治理后复盘**。

---

## B. 高 churn 会造成什么影响？（按链路拆解：采集 → TSDB → 查询/规则/告警 → remote_write → 下游存储）

### 1) 对 Prometheus TSDB：内存、WAL、写块/压缩、GC 压力显著上升

#### (1) Head 内存更高、更难降下来（尤其是“短命 series”）
Prometheus head block 保存最近 1–3 小时数据在内存，通常每 2 小时 flush 成持久化 block[[4]](https://grafana.com/blog/new-in-prometheus-v2-19-0-memory-mapping-of-full-chunks-of-the-head-block-reduces-memory-usage-by-as-much-as-40/)。Prometheus 的 chunk 每个最多 120 个样本；低 churn + 短 scrape interval 时 chunk 很快填满，可以更早 flush/mmap，从而显著省内存[[4]](https://grafana.com/blog/new-in-prometheus-v2-19-0-memory-mapping-of-full-chunks-of-the-head-block-reduces-memory-usage-by-as-much-as-40/)。

但 Grafana 对 Prometheus v2.19 的内存机制分析明确指出：
- **高 churn 时，一些 series 可能永远填不满 chunk（达不到 120 样本）**，因此“在整个过程中占用更多内存”，导致内存优化收益更小[[4]](https://grafana.com/blog/new-in-prometheus-v2-19-0-memory-mapping-of-full-chunks-of-the-head-block-reduces-memory-usage-by-as-much-as-40/)。
- 同时若对最近几小时做 heavy queries，会触发更多 mmap chunk 被加载，实际内存会高于“静态估计”[[4]](https://grafana.com/blog/new-in-prometheus-v2-19-0-memory-mapping-of-full-chunks-of-the-head-block-reduces-memory-usage-by-as-much-as-40/)。

#### (2) “两小时节奏”很关键：若写块/compaction/GC 异常，churn 会把系统更快推向 OOM
维护者描述的正常回收路径是：2 小时写块 → head GC 修剪 stale chunks → `tsdb_head_series` 周期性下降[[38]](https://github.com/prometheus/prometheus/issues/10925)[[5]](https://github.com/prometheus/prometheus/issues/10982)。  
真实案例中，如果 compaction/写块长期不触发，head series 会持续增长、内存持续上涨直至 OOM；可用 `prometheus_tsdb_compactions_triggered_total`、日志是否出现周期性 write block 来排查[[5]](https://github.com/prometheus/prometheus/issues/10982)。

> 结论：**高 churn 本身不一定立刻“打死”Prometheus，但它会显著放大任何写块/GC 延迟、磁盘/CPU 不足、bug 或配置问题的后果**。

#### (3) 磁盘与保留的压力：WAL/chunks_head 峰值更明显
Prometheus 本地存储目录包含 WAL、chunks_head、blocks 等[[3]](https://prometheus.io/docs/prometheus/latest/storage/)；磁盘规划官方给了粗略公式：  
`needed_disk_space = retention_seconds * ingested_samples_per_second * bytes_per_sample`（平均 1–2 bytes/样本）[[3]](https://prometheus.io/docs/prometheus/latest/storage/)。降低摄取速率的两条路：减少 series 或增大 scrape interval，且“减少 series 通常更有效”[[3]](https://prometheus.io/docs/prometheus/latest/storage/)。  
高 churn 往往伴随更多活跃 series/元数据变化，WAL 与 head 峰值更难控；建议启用 WAL 压缩（`--storage.tsdb.wal-compression`，通常可将 WAL 体积减半，CPU 增量小；2.20 起默认开启）[[3]](https://prometheus.io/docs/prometheus/latest/storage/)。

---

### 2) 对查询与规则评估：更慢、更不稳定，进而影响告警准确性

#### (1) 查询变慢、甚至影响 Prometheus 自身稳定性
在高查询负载场景，`prometheus_engine_query_duration_seconds` 会明显恶化；社区案例里 0.9 分位达到 20–80s，甚至导致 `/-/healthy` 健康检查超时被 K8s 重启（HTTP 连接/资源竞争问题）[[40]](https://github.com/prometheus/prometheus/issues/10686)。高 churn 往往意味着查询需要扫描/聚合更多 series（尤其是范围查询 + 聚合），会显著放大这一问题。

Prometheus Mixin 也给了一个“查询接近并发上限”的告警范式：当
`avg_over_time(prometheus_engine_queries[5m]) / max_over_time(prometheus_engine_queries_concurrent_max[5m]) > 0.8`
持续 15m，可判定为高 query load[[42]](https://monitoring.mixins.dev/prometheus/)。

#### (2) 规则评估变慢/漏评估：直接导致“指标缺失或告警不准确”
kube-prometheus 的 runbook 明确写了：**规则组评估慢会导致 missed evaluations，影响是“指标与告警可能缺失或不准确”**[[41]](https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusmissingruleevaluations/)。Prometheus Mixin 也把它固化成告警：
- `increase(prometheus_rule_group_iterations_missed_total[5m]) > 0`（MissingRuleEvaluations）[[42]](https://monitoring.mixins.dev/prometheus/)
- `increase(prometheus_rule_evaluation_failures_total[5m]) > 0`（RuleFailures）[[42]](https://monitoring.mixins.dev/prometheus/)

Thanos Ruler 文档进一步给出非常直观的判断：如果  
`prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds`  
说明评估时间超过调度间隔，会导致某些间隔的数据缺失[[45]](https://thanos.io/tip/components/rule.md/)。

高 churn 常见的触发方式：
- 规则表达式按 `pod`/`container`/`instance` 等高 churn label 展开，导致评估向量巨大；
- 在短时间窗口里不断产生新 series，使规则评估每次面对的“实体集合”不断变化，缓存命中率与计算稳定性更差。

#### (3) 告警更容易抖动（flapping）或“重复/无法聚合”
Prometheus 告警实例由表达式返回的每个 label set 决定；告警支持 `for`（pending→firing）与 `keep_firing_for`（防抖/防误恢复）机制，官方明确说 `keep_firing_for` 可用于防止 flapping、或因缺失数据导致的错误恢复[[36]](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/)。  
而 PromQL 对 staleness/lookback 有明确语义：只有“最近样本距评估时间不超过 lookback”的 series 才会被选中，否则直接不出现在结果里[[37]](https://prometheus.io/docs/prometheus/latest/querying/basics/)。因此 **高 churn + 缺失/迟到** 会让告警表达式更容易出现“时有时无”的返回，从而抖动。

HA 体系里还有一个非常现实的重复告警来源：如果 HA Prometheus 的 external_labels 带了 replica 区分标签，且未在发往 Alertmanager 前移除，Alertmanager 会把两份告警当作不同告警，无法去重；维护者在邮件列表里明确指出要 drop replica label，并给了排障方式[[46]](https://groups.google.com/g/prometheus-users/c/Cb7lUaqWnbc)。高 churn 若发生在告警标签集（例如把 pod_uid 之类带进告警标签），也会造成“看起来像全新告警”，导致聚合/静默失效，体感上就是“告警风暴”。

Alertmanager 在大量告警下出现重复通知的案例也有人报告（HA 模式下去重/状态传播的边界问题），但该 issue 仍在讨论中，部分技术断言尚未被维护者最终确认；可将其视作“告警量爆炸时的潜在风险线索”而非定论[[47]](https://github.com/prometheus/alertmanager/issues/4152)。

---

### 3) 对 remote_write：**内存放大 + 背压停摆 + 2 小时数据丢失边界**

Prometheus 官方 remote_write 调优文档把关键故障模式说得非常直白：

- remote_write 每个目标都有队列，从 WAL 读取→写入各 shard 队列→发送远端[[7]](https://prometheus.io/docs/practices/remote_write/)。
- **只要某个 shard 堵塞并填满队列，Prometheus 会阻塞从 WAL 读取到任何 shard，导致所有吞吐停止**[[7]](https://prometheus.io/docs/practices/remote_write/)（这是一个“单点背压 → 全局停摆”的机制）。
- 远端若持续不可用超过 2 小时：WAL 会 compaction，未发送数据将丢失[[7]](https://prometheus.io/docs/practices/remote_write/)。
- **remote_write 会缓存 seriesID→labels 映射；大量 series churn 会显著增加内存使用**[[7]](https://prometheus.io/docs/practices/remote_write/)（这是官方对 churn 与 remote_write 内存放大的直接结论）。

并且队列参数与内存强相关：内存开销 ~ `shards * (capacity + max_samples_per_send)`；默认每 shard < 2MB；调大 capacity / batch 可能提升吞吐，但也更容易 OOM，需要联动控制 `max_shards`[[7]](https://prometheus.io/docs/practices/remote_write/)。

社区也有在高负载/backlog 下“加 shards 吞吐不线性增长、capacity 影响吞吐”的案例讨论，但属于特定环境观测，尚未形成统一结论；不过它再次提示 remote_write 的瓶颈常在 WAL 读取争用、队列背压链路上[[8]](https://github.com/prometheus/prometheus/issues/17277)。

---

### 4) 对下游（Thanos / Cortex / Mimir 等）：churn 会被“分布式/多租户”进一步放大

#### (1) Thanos Receive
- Thanos Receive 基于 Prometheus TSDB，remote_write 写入，默认每 2 小时上传 block 并提供近实时查询[[9]](https://thanos.io/tip/components/receive.md/)。
- 关键点：**hashmod 分布算法扩容会导致 series churn 与内存尖峰，不推荐用于新安装；推荐 ketama**[[9]](https://thanos.io/tip/components/receive.md/)。这类“扩容引发再分布 churn”在高 churn 上游下会叠加，放大抖动与资源峰值。

#### (2) Cortex Compactor / block 体系
Cortex Compactor 通过垂直+水平 compaction 减少 block 数、去重并降低索引与 store-gateway 的 index-header 内存，从而提升查询效率[[10]](https://cortexmetrics.io/docs/blocks-storage/compactor/)。高 churn/高基数通常意味着索引更大、block 更多，compaction 跟不上会导致查询扫描更多块、性能更差（机制上高度相关）[[10]](https://cortexmetrics.io/docs/blocks-storage/compactor/)。

#### (3) Grafana Mimir（典型多租户远端存储）
Mimir ingester 也是 TSDB head + WAL + 2 小时 block 上传的模型[[11]](https://grafana.com/docs/mimir/latest/references/architecture/components/ingester/)。Mimir runbook 提到一个常见放大场景：故障恢复后，Prometheus remote-write 有 lag，会以更高速率补发，导致入站流量突增，需要水平扩容 ingester[[12]](https://grafana.com/docs/mimir/latest/manage/mimir-runbooks/)。

更关键的是 Mimir 的“series limit”故障链路（非常适合作为多租户平台的 churn 典型风险）：
- 达到单 ingester `max_series` 时：**新 series 写入失败，但已有 series 继续成功**；
- 错误为 5xx，会触发发送端重试；重试会阻塞数据流，发送端排队；
- 条件解除可无损恢复；
- 扩容后 series 数下降可能需要最长约 4 小时，因为要等 TSDB head compaction 清理 stale series[[12]](https://grafana.com/docs/mimir/latest/manage/mimir-runbooks/)。

这正是“高 churn（不断产生新 series）→触发限制→重试风暴/背压→全链路拥塞”的经典放大回路。

---

## C. 有什么系统方案可以解决？（从“治理 + 护栏 + 架构 + 运维”四层落地）

下面给一套可以在大多数组织里**工程化落地**的方案框架。核心思路：**先可观测，再治理源头，再加护栏，最后用架构把不可避免的 churn 隔离/摊薄**。

---

### 0) 先建立“churn SLO”与责任边界（否则永远救火）
建议至少定义并按 team/service 归因：
- 每小时新增 series（`scrape_series_added` over_time）[[2]](https://www.robustperception.io/finding-churning-targets-in-prometheus-with-scrape_series_added)
- head 新建 series 速率（`prometheus_tsdb_head_series_created_total` rate）[[38]](https://github.com/prometheus/prometheus/issues/10925)
- remote_write backlog（`prometheus_remote_storage_samples_pending`）[[7]](https://prometheus.io/docs/practices/remote_write/)
- 规则漏评估（`prometheus_rule_group_iterations_missed_total`）[[42]](https://monitoring.mixins.dev/prometheus/)
- 写块/compaction 节奏（`prometheus_tsdb_compactions_triggered_total`）[[5]](https://github.com/prometheus/prometheus/issues/10982)

这些指标能把“是平台不行”还是“某团队/某 exporter 乱打标签”快速分开。

---

### 1) 指标设计与标签治理（最有效、成本最低）

#### (1) 明确禁止的 label 模式（从源头减少 churn）
Prometheus 官方命名与标签实践明确警告：每个唯一 label 组合都会生成新的 time series；不要使用 user ID、email 等高基数标签[[13]](https://prometheus.io/docs/practices/naming/)。在 churn 语境下，建议把这条扩展为**“禁止无界、近似唯一、或频繁变化的标签值”**，典型如：
- 请求级：request_id、trace_id、span_id
- 实例级且短命：pod_uid、container_id、docker_hash
- 时间相关：timestamp、build_time
- 资源属性全量提升（OTLP 的 `promote_all_resource_attributes` 风险已被官方点名）[[15]](https://prometheus.io/docs/prometheus/latest/configuration/configuration/)

#### (2) 统一 label 维度模型：把“调试维度”与“监控维度”分层
- **监控/告警用（稳定维度）**：cluster、namespace、workload/service、env、region、status_code 等（值域相对稳定）
- **调试用（高变动维度）**：pod、instance、container、endpoint_path（可能很多）、异常栈等  
调试维度如果必须存在，尽量只在少数 debug metrics 中保留，并通过采集过滤/更短保留期隔离成本。

---

### 2) 采集侧过滤与硬护栏（把“错误指标”挡在 TSDB 之外）

#### (1) 用 metric relabeling 做“白名单/黑名单”过滤（最直接的降基数/降 churn 手段）
Prometheus Operator 文档明确：
- **target relabeling**：抓取前改写/筛选目标 label set[[49]](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/running-exporters.md)
- **metric relabeling**：ingestion 前最后一步，对 samples 做 drop/keep 等[[49]](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/running-exporters.md)，且不会作用于自动生成的 `up` 等序列[[49]](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/running-exporters.md)

Operator 给了非常具体的例子（可直接套用）：
- 丢弃某个指标名：对 `__name__` 做 regex + `action: drop`[[49]](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/running-exporters.md)
- 丢弃某类 time series：对某个 label（如 `id`）做 regex + drop[[49]](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/running-exporters.md)
- 通过 `action: labeldrop` 删除某些 label（例如 `pod`）从而减少序列身份抖动[[49]](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/running-exporters.md)

腾讯云 TMP 的最佳实践文档也给了同类示例：通过 `metricRelabelings`（CRD）或 `metric_relabel_configs`（原生 scrape_config）对 `__name__` 做 keep，仅保留所需指标，其余全部 drop；并强调要用 metricRelabel（不是 relabelings）[[55]](https://www.tencentcloud.com/ind/document/product/457/47004)。

#### (2) 为“异常爆炸”设置上限：sample/label/target 各类 limit
Prometheus 配置里有一组非常实用的“护栏”：
- `sample_limit`：单次 scrape 最大样本数，超过则该次 scrape 直接失败[[15]](https://prometheus.io/docs/prometheus/latest/configuration/configuration/)
- `label_limit` / label name/value length limit：限制每个样本的 label 数与长度，超限直接失败[[15]](https://prometheus.io/docs/prometheus/latest/configuration/configuration/)
- `target_limit`：每个 scrape config 允许的 target 数上限（target relabel 后计算），超限则 targets 失败不抓取[[15]](https://prometheus.io/docs/prometheus/latest/configuration/configuration/)
- `body_size_limit`：响应体大小限制，避免 exporter 异常输出压垮 Prometheus[[15]](https://prometheus.io/docs/prometheus/latest/configuration/configuration/)

这些护栏的策略建议：
- **对高风险 job（kube-state-metrics、cAdvisor、黑盒探测、多租户入口）单独配置更严格的上限**；
- 让“错误配置”尽早失败，避免把整套 Prometheus 拖死。

---

### 3) Recording Rules 预聚合与分层（把“高维原始数据”变成“稳定可用的中间层”）

Prometheus 官方 recording rules 最佳实践强调：
- 用 `level:metric:operations` 做分层命名；
- 聚合时显式 `without(...)`，明确聚合掉哪些 label，避免冲突并保留有用维度[[14]](https://prometheus.io/docs/practices/rules/)；
- ratio 先分别聚合分子分母再相除，避免平均的平均等统计错误[[14]](https://prometheus.io/docs/practices/rules/)。

在高 churn 下的落地方式：
- **边缘/本地先做聚合**：例如按 `pod` 的原始指标只在本地保留，向上游/远端只写 `sum without(pod, instance, container)` 的聚合序列（这是把 churn label “吸收”掉）。
- GCP GMP 的成本控制文档甚至把这作为 self-deployed collection 的重要能力：本地聚合后只导出聚合结果到托管后端，以降低基数与成本[[26]](https://docs.cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls)（代价是远端看不到原始明细）。

---

### 4) 架构层隔离：分片、分区、多层 Prometheus、Agent 模式

#### (1) Prometheus 水平分片（sharding）与多实例分区
Prometheus Operator 文档给出了很明确的分片能力：
- 当单 Prometheus 承载不了 targets 时，可用 `spec.shards` 将 targets 按 `__address__` 做 hash/mod 分片；Operator 会创建 `shards * replicas` 个 Pod[[52]](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/shards-and-replicas.md)。
- 但扩/缩 shards 不会自动重分布旧数据；要全局查询通常要配合 Thanos 或 remote_write 到中心[[52]](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/shards-and-replicas.md)。

分区（partition）的常见方法：
- 按 namespace/team 分 Prometheus（软隔离）
- 按指标类型分（infra 与 app 分开；高 churn job 单独 Prometheus）
- 按合规/成本域分（prod/non-prod 分开）

#### (2) Federation：把上层 Prometheus 只喂“聚合后的稳定序列”
Prometheus 官方 federation 允许上层从下层的 `/federate` 抓取“选定的时间序列当前值”，用 `match[]` 选择器控制集合[[51]](https://prometheus.io/docs/prometheus/latest/federation/)。典型模式是：
- 下层 Prometheus 保留 instance/pod 级细节；
- 上层 Prometheus 只抓 `job:` 聚合后的 recording rules 结果，实现全局视图同时控制基数[[51]](https://prometheus.io/docs/prometheus/latest/federation/)。

#### (3) Agent mode：把 Prometheus 变成“抓取+转发”，大幅降低本地 TSDB 成本
Prometheus 官方 agent mode：
- 保留 scraping/discovery/remote_write 能力；
- **禁用 TSDB、告警、规则评估**，用定制 WAL 做短期缓冲，成功写出就删除，本地资源消耗远低于 server mode[[17]](https://prometheus.io/docs/prometheus/latest/prometheus_agent/)；
- downsides：不能本地做 recording rules 预聚合、不能本地告警（要在远端做）[[17]](https://prometheus.io/docs/prometheus/latest/prometheus_agent/)。

Operator 也专门为此引入 PrometheusAgent CRD，避免用户用 Prometheus CR 硬塞 agent 参数导致 rules/alerts 段不兼容的问题[[18]](https://prometheus-operator.dev/docs/proposals/implemented/prometheus-agent/)。

适用建议：
- 如果你的主要目标是“把指标进一个远端多租户平台（Mimir/AMP/GMP/Azure）”，且你愿意把规则/告警统一挪到远端：Agent mode 很合适。
- 如果你必须在边缘做大量预聚合：保留 server mode 或采用“边缘 server + 只 remote_write 聚合结果”。

---

### 5) remote_write 专项治理（高 churn 环境里经常是第一瓶颈）

按官方文档把关键点做成“检查清单”：

1. **先看 backlog 指标**：`prometheus_remote_storage_samples_pending`[[7]](https://prometheus.io/docs/practices/remote_write/)  
2. 记住最危险机制：**任一 shard 队列满 → 阻塞 WAL 读取 → 全部吞吐停止**[[7]](https://prometheus.io/docs/practices/remote_write/)  
3. 明确 churn 的内存放大：remote_write 会缓存 seriesID→labels，大量 series churn 会显著增加内存[[7]](https://prometheus.io/docs/practices/remote_write/)  
4. 调参原则：
   - `capacity` 建议为 `max_samples_per_send` 的 3–10 倍[[7]](https://prometheus.io/docs/practices/remote_write/)
   - `max_shards` 不是越大越好：会线性增加内存并可能压垮远端；必要时反而要降[[7]](https://prometheus.io/docs/practices/remote_write/)
   - 关注 CPU/网络是否饱和[[7]](https://prometheus.io/docs/practices/remote_write/)

另外，要把“远端不可用 2 小时数据丢失边界”纳入演练：远端故障持续超过 2h，WAL compaction 后未发送数据会丢[[7]](https://prometheus.io/docs/practices/remote_write/)。

---

### 6) 告警稳定性专项：防抖、去重、避免“churn label”进入告警维度

- 合理使用 `for`（避免瞬时尖峰触发）与 `keep_firing_for`（防 flapping、数据缺失误恢复）[[36]](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/)。
- HA Prometheus + Alertmanager：确保发往 Alertmanager 的告警 label 集一致；如果包含 replica 区分 external label，需要在发送前 drop，否则会重复告警[[46]](https://groups.google.com/g/prometheus-users/c/Cb7lUaqWnbc)。
- 用 Prometheus Mixin 的通知队列/错误率告警做底座（例如队列预测跑满、发送错误率）[[42]](https://monitoring.mixins.dev/prometheus/)，避免 churn 导致告警量暴涨时“告警系统自己先坏”。

---

## D. 各家云厂商有没有现有方案？（截至 2026-02，按“限制/诊断/治理能力”对比）

> 这里的“现有方案”通常体现为三类能力：  
> 1) **硬配额/限流**（active series、写入速率、请求大小等）；  
> 2) **可观测性/诊断**（谁的 series/吞吐在增长、丢弃原因）；  
> 3) **治理工具**（按标签/项目/命名空间做隔离与限制，或提供指标排除/过滤）。

### 1) AWS — Amazon Managed Service for Prometheus (AMP)
**配额/限制：**
- 默认每 workspace **Active series 50,000,000**，series 若过去 2 小时有样本则视为 active[[20]](https://docs.aws.amazon.com/prometheus/latest/userguide/AMP_quotas.html)；可申请提升，最大可到 10 亿[[20]](https://docs.aws.amazon.com/prometheus/latest/userguide/AMP_quotas.html)。
- Ingestion rate 默认 **1,666,666 samples/sec**（可升配），且会自动调整为 active series limit 的 1/30（封顶 1,666,666）[[20]](https://docs.aws.amazon.com/prometheus/latest/userguide/AMP_quotas.html)。
- RemoteWrite API ops：3,000 TPS / workspace[[20]](https://docs.aws.amazon.com/prometheus/latest/userguide/AMP_quotas.html)。
- AMP 还有动态容量调节与“突增过快会 throttling”的行为说明，并建议逐步提升写入避免 throttling[[20]](https://docs.aws.amazon.com/prometheus/latest/userguide/AMP_quotas.html)。

**治理能力（很关键）：Label-based active series limits**
- 2025 起 AMP 支持在 workspace 内配置“基于 label 的 active series 限额”，用于隔离 noisy neighbor：当某个应用/团队激增时，只 throttle 命中同一 label set 的序列，不影响其他 producer[[21]](https://aws.amazon.com/about-aws/whats-new/2025/04/amazon-managed-service-prometheus-label-based-series-limits/)[[22]](https://aws.amazon.com/blogs/mt/optimizing-metrics-ingestion-with-amazon-managed-service-for-prometheus/)。
- 并提供 CloudWatch 指标：
  - `ActiveSeriesPerLabelSet`、`ActiveSeriesLimitPerLabelSet`
  - `IngestionRatePerLabelSet`
  - `DiscardedSamplesPerLabelSet`（并区分原因：labelset/per-metric/per-user 等）[[22]](https://aws.amazon.com/blogs/mt/optimizing-metrics-ingestion-with-amazon-managed-service-for-prometheus/)  
这对治理高 churn/高基数非常实用：你能直接看到**“谁导致丢弃、丢弃原因是什么”**[[22]](https://aws.amazon.com/blogs/mt/optimizing-metrics-ingestion-with-amazon-managed-service-for-prometheus/)。

**结论**：AMP 在“多租户隔离 + 按标签限额 + 丢弃原因可观测”这块做得相对完整[[21]](https://aws.amazon.com/about-aws/whats-new/2025/04/amazon-managed-service-prometheus-label-based-series-limits/)[[22]](https://aws.amazon.com/blogs/mt/optimizing-metrics-ingestion-with-amazon-managed-service-for-prometheus/)。

---

### 2) GCP — Managed Service for Prometheus (GMP) / Cloud Monitoring
**限制口径：**
- GMP 文档宣称“对 active/total time series 没有限制”[[23]](https://docs.cloud.google.com/stackdriver/docs/managed-prometheus)，但它共享 Cloud Monitoring 的写入/读取配额：
  - 默认写入配额：**500 QPS / project**，每次写入最多 200 samples，等效 **100,000 samples/sec**[[23]](https://docs.cloud.google.com/stackdriver/docs/managed-prometheus)。
  - 写入请求最多包含 **200 time series/points**（每个 TS 每次最多 1 点）[[24]](https://docs.cloud.google.com/monitoring/quotas)。
  - 单 time series 写入频率：点的 end time 至少间隔 5 秒[[24]](https://docs.cloud.google.com/monitoring/quotas)。
  - 还有“单 monitored resource 下 Prometheus active time series 的系统安全上限 1,000,000（过去 24 小时活跃）”[[24]](https://docs.cloud.google.com/monitoring/quotas)。

**诊断/治理能力：Metrics Management + Exclusion rules**
- Cloud Monitoring 的 Metrics Management 页面能报告摄取量、labels/metrics 的 cardinality、reads、写入错误率，并支持 **metric-exclusion rules**（按指标名或正则排除），被排除指标不计费[[24]](https://docs.cloud.google.com/monitoring/quotas)[[25]](https://docs.cloud.google.com/monitoring/docs/metrics-management)。
- GMP 成本控制文档明确：托管按 samples 计费时应主动过滤；可通过 relabeling 过滤不需要指标、增大 scrape interval 降 samples；self-deployed 场景下可本地 recording rules 聚合并只导出聚合结果到托管后端（降低基数/成本）[[26]](https://docs.cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls)。

**结论**：GMP 更强调“用 Cloud Monitoring 的配额体系与 Metrics Management 做治理/成本控制”，而不是像 AMP 那样突出 workspace 内按 label set 的 series 限额[[24]](https://docs.cloud.google.com/monitoring/quotas)[[25]](https://docs.cloud.google.com/monitoring/docs/metrics-management)[[26]](https://docs.cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls)。

---

### 3) Azure — Azure Monitor managed service for Prometheus
**核心限制（workspace 侧 + DCR/DCE 侧）：**
- Azure Monitor workspace：active time series 上限 **1,000,000**（过去约 12 小时窗口），可申请提升[[28]](https://learn.microsoft.com/en-us/azure/azure-monitor/fundamentals/service-limits)。
- Events per minute ingested：1,000,000，可申请提升[[28]](https://learn.microsoft.com/en-us/azure/azure-monitor/fundamentals/service-limits)。
- 但 DCR 层有不可提升硬限制：**15,000 req/min**、**50 GB/min**[[28]](https://learn.microsoft.com/en-us/azure/azure-monitor/fundamentals/service-limits)。

**可观测性与告警：**
- Azure 提供 workspace 利用率指标：`Active Time Series % Utilization`、`Events Per Minute Received % Utilization`，并给出推荐告警阈值（75%/95% 持续 5 分钟）用于预防 throttling[[29]](https://learn.microsoft.com/en-us/azure/azure-monitor/metrics/azure-monitor-workspace-monitor-ingest-limits)。

**remote_write 接入注意：**
- Azure 明确支持 self-managed Prometheus remote_write 到 Azure Monitor workspace（迁移期或长期混合）[[27]](https://learn.microsoft.com/en-us/azure/azure-monitor/metrics/prometheus-metrics-overview)。
- 并给出 remote write container 的吞吐上限：最多处理 **150,000 unique time series**；超过可能因并发连接数高而报错；可通过把 remote batch size 从 500 调到 1,000 来减少连接数[[28]](https://learn.microsoft.com/en-us/azure/azure-monitor/fundamentals/service-limits)。

**结论**：Azure 的“active time series 1M + 利用率监控告警 + DCR 层硬上限”是你做容量规划时必须优先核算的三件事[[28]](https://learn.microsoft.com/en-us/azure/azure-monitor/fundamentals/service-limits)[[29]](https://learn.microsoft.com/en-us/azure/azure-monitor/metrics/azure-monitor-workspace-monitor-ingest-limits)。

---

### 4) 腾讯云 — Tencent Managed Service for Prometheus (TMP)
腾讯云在“高基数/高 churn”限制与诊断上给得很具体。

**实例级硬限制：**
- 每实例最多 **450 万 series**（免费试用 200 万）；可工单申请调整[[53]](https://www.tencentcloud.com/document/product/1116/49140)。
- 上报吞吐：付费实例 **300,000 datapoints/s**（试用 100,000）[[53]](https://www.tencentcloud.com/document/product/1116/49140)。
- 单 metric 最多 32 个 labels；同一 metric 下 label 组合数不超过 100,000[[53]](https://www.tencentcloud.com/document/product/1116/49140)。
- 查询侧限制：单次查询涉及 series 不超过 100,000、数据量不超过 100MB；这些限制同样适用于告警规则与录制规则，并给出“拆分查询/先拆分后聚合/对 recording 结果再聚合”等建议[[53]](https://www.tencentcloud.com/document/product/1116/49140)。

**诊断能力：Instance Diagnosis**
- 诊断页会展示实例上报速率、实例 series 上限、单 metric series 上限、label 长度/数量限制、时间戳窗口限制（不允许 out-of-order）、以及“series 数 top10 metrics”等，并明确超限会导致 discard（丢弃）[[54]](https://www.tencentcloud.com/document/product/1116/62373)。

**治理建议：Streamlining（精简指标）**
- 官方给出通过 `metricRelabelings` / `metric_relabel_configs` 对 `__name__` 做 keep/drop 的过滤示例，以及通过增大 scrape interval 大幅降数据点量的建议[[55]](https://www.tencentcloud.com/ind/document/product/457/47004)。

**结论**：TMP 的特点是“配额小于 AMP 的量级，但限制、诊断、丢弃语义写得很落地”，适合快速按上限做工程约束与归因[[53]](https://www.tencentcloud.com/document/product/1116/49140)[[54]](https://www.tencentcloud.com/document/product/1116/62373)[[55]](https://www.tencentcloud.com/ind/document/product/457/47004)。

---

### 5) 阿里云 — Managed Service for Prometheus V2（基于 SLS MetricStore）
阿里云 MSP V2 的限制体系大量下沉到 SLS（日志服务）的 MetricStore。

**采集侧（agent）限制：**
- 阿里云文档给出每 agent replica 的数据点、targets 等上限（例如单副本 3.5 million 数据点、targets 5,000 等），并通过扩容 replicas（最多 20）提升能力；单 target 数据量超 512MB 会丢弃且不可调[[30]](https://www.alibabacloud.com/help/en/prometheus/product-overview/service-limits)。

**后端存储（SLS MetricStore）的吞吐/扩缩容关键：Shard**
- SLS 明确：**1 Shard 写入 5MB/s、500 次/s；读取 10MB/s、100 次/s**；每 MetricStore 最多 10 Shard、每 Project 最多 200 Shard；可开启自动分裂，必要时最多自动分裂到 256 个 readwrite Shard[[58]](https://help.aliyun.com/zh/sls/manage-a-metricstore)。  
这基本决定了你在高 churn/高写入下的“后端极限与扩容路径”。

**异常写入治理（时间窗口丢弃）：**
- MetricStore 支持配置写入时间窗口：合法写入时间范围为【到达时间-左区间，到达时间+右区间】，超出则丢弃；且该特性仅对 Prometheus remote write 写入生效[[58]](https://help.aliyun.com/zh/sls/manage-a-metricstore)。这对时钟漂移/延迟回放导致的“乱序/脏数据”很实用。

**配额监控与超限排障：**
- CloudLens for SLS 可以基于错误日志与监控指标做 Project 级配额使用与超配监控，并能检索到多类 QuotaExceed / WriteQuotaExceed / ShardWriteQuotaExceed 等错误线索[[61]](https://sls.alibabacloud.com/doc/en/cloudlen/slsquota.html)。

**结论**：阿里云 MSP V2 更像“Prometheus 托管 + SLS 时序库”，你需要用 SLS 的 Shard/Project 配额模型来做容量与限流治理[[58]](https://help.aliyun.com/zh/sls/manage-a-metricstore)[[61]](https://sls.alibabacloud.com/doc/en/cloudlen/slsquota.html)。

---

### 6) 华为云 — AOM Prometheus
华为云的公开资料口径相对分散，需要更谨慎解读。

**能力侧描述：**
- Overview 文档宣称“支持云存储、没有可存储数据限制（Unlimited data）”，并强调采集存储分离、水平扩展、动态扩缩容[[34]](https://support.huaweicloud.com/intl/en-us/usermanual-aom2/mon_01_0083.html)。
- 支持 remote_write 接入：给出 remote_write 地址与 bearer_token 配置流程[[35]](https://support.huaweicloud.com/intl/en-us/usermanual-aom2/mon_01_0076.html)。
- 提供 recording rules 用于改善查询效率（部分实例类型支持）[[34]](https://support.huaweicloud.com/intl/en-us/usermanual-aom2/mon_01_0083.html)。

**但同时存在明确“使用限制”口径：**
- FAQ 文档给出 AOM 指标总数上限（例如单账号最多 400,000，小规格 100,000）、单 metric 最多 20 dimensions、存储天数（基础版 7 天、专业版 30 天）等限制[[63]](https://support.huaweicloud.com/intl/en-us/aom_faq/aom_03_0002.html)。这与“Unlimited data”表述存在表面冲突，可能是不同产品层/版本/规格口径，需要你按实际购买的 AOM Prometheus 实例类型与计费规格进一步核对[[34]](https://support.huaweicloud.com/intl/en-us/usermanual-aom2/mon_01_0083.html)[[63]](https://support.huaweicloud.com/intl/en-us/aom_faq/aom_03_0002.html)。

**可观测性线索：**
- CCE 的 Prometheus Server view 列出 remote_write dropped/failed/retried/enqueue_retries 等 Prometheus 内置指标，可用于排查写入背压与丢弃[[64]](https://support.huaweicloud.com/eu/usermanual-cce/cce_10_0764.html)。

**结论**：华为云 AOM Prometheus 的“高 churn 配额与治理”在公开材料里不如 AMP/TMP/Azure 清晰；你应当在 PoC 时重点验证：active series/写入吞吐/丢弃语义、是否能按租户/实例做限流、以及对应的可观测性指标面板[[34]](https://support.huaweicloud.com/intl/en-us/usermanual-aom2/mon_01_0083.html)[[63]](https://support.huaweicloud.com/intl/en-us/aom_faq/aom_03_0002.html)[[64]](https://support.huaweicloud.com/eu/usermanual-cce/cce_10_0764.html)。

---

## 最后：落地建议（从“最快见效”到“长期演进”）

1) **第一周就能做的止血**
- 用 `scrape_series_added` topk 找 churn 来源[[2]](https://www.robustperception.io/finding-churning-targets-in-prometheus-with-scrape_series_added)；同时看 `rate(prometheus_tsdb_head_series_created_total[5m])` 验证是否持续造新序列[[38]](https://github.com/prometheus/prometheus/issues/10925)。
- 对最异常的 job 立刻加 `metricRelabelings`：先白名单保留关键指标、其余 drop（Operator/TMP 文档都有成熟示例）[[49]](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/running-exporters.md)[[55]](https://www.tencentcloud.com/ind/document/product/457/47004)。
- 加 `sample_limit`/`label_limit`/`body_size_limit` 护栏，避免单 job 拖垮全局[[15]](https://prometheus.io/docs/prometheus/latest/configuration/configuration/)。

2) **一个月内完成的体系化治理**
- 建立标签规范（禁止无界/短命 label），把 OTLP `promote_all_resource_attributes` 作为高风险项审计[[15]](https://prometheus.io/docs/prometheus/latest/configuration/configuration/)。
- 用 recording rules 分层聚合，把 pod/instance 级 raw 指标下沉在边缘或缩短保留；远端只存聚合层[[14]](https://prometheus.io/docs/practices/rules/)[[26]](https://docs.cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls)。
- 把规则评估与告警稳定性纳入监控：MissingRuleEvaluations/RuleFailures/HighQueryLoad 等 mixin 告警直接启用[[42]](https://monitoring.mixins.dev/prometheus/)，并用 `keep_firing_for` 降低 flapping[[36]](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/)。

3) **长期（平台化）**
- 架构上做分区/分片（Operator `spec.shards`）并配 Thanos/Mimir 统一查询；或在“只转发到远端”的场景改 Agent mode[[52]](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/shards-and-replicas.md)[[17]](https://prometheus.io/docs/prometheus/latest/prometheus_agent/)。
- 多租户平台优先选支持“按标签/租户限额 + 丢弃原因可观测”的产品形态（AMP 的 label set limits 是很强的参考实现）[[21]](https://aws.amazon.com/about-aws/whats-new/2025/04/amazon-managed-service-prometheus-label-based-series-limits/)[[22]](https://aws.amazon.com/blogs/mt/optimizing-metrics-ingestion-with-amazon-managed-service-for-prometheus/)；否则必须在采集侧/准入侧自己做配额与隔离。

如果你愿意补充两类信息，我可以把上述方案进一步“具体化到配置与阈值”：
- 你目前的环境：K8s 规模、scrape 间隔、当前 active series、remote_write 目标（是否 Mimir/AMP/GMP 等）。
- churn 的主要来源：是 exporter（如 kube-state-metrics/cAdvisor）还是应用自定义指标/OTel attributes。
