Query ID: 90
============================================================

QUESTION:
Analyze the complex issue of liability allocation in accidents involving vehicles with advanced driver-assistance systems (ADAS) operating in a shared human-machine driving context. Your analysis should integrate technical principles of ADAS, existing legal frameworks, and relevant case law to systematically examine the boundaries of responsibility between the driver and the system. Conclude with proposed regulatory guidelines or recommendations.

============================================================

RESEARCH TASKS (6 total):
  #1: Research the technical-operational landscape of ADAS in shared control (SAE Levels 0–3, especially Level 2): core functions (ACC, LKA, AEB, TJA), ODD and fallback behavior, takeover request timing, driver monitoring, and common failure modes and human factors that lead to crashes. Summarize key technical principles relevant to assigning responsibility.
  #2: Research relevant case law, settlements, and regulatory enforcement actions involving ADAS/shared control (especially Level 2 systems such as Tesla Autopilot/FSD Beta, GM Super Cruise, etc.) including NHTSA investigations/recalls, notable U.S. lawsuits, EU/UK cases if any, and how fault/responsibility was argued or allocated (misrepresentation, failure to warn, design defect, driver negligence, foreseeable misuse, data/EDR evidence).
  #3: Research current (through early 2026) legal frameworks for allocating liability in ADAS-related crashes in major jurisdictions (U.S. negligence & product liability; EU product liability directive updates + motor insurance; UK; Germany; China/Japan if notable). Include relevant regulations/standards used as benchmarks (UNECE regs, ISO 26262, ISO 21448 SOTIF, UL 4600, ISO/SAE 21434) and any ADAS/ALKS-specific rules.
  #4: Research evidentiary and governance requirements for adjudicating ADAS/shared-control crashes: event data recorders (EDR) fields, ADS/ADAS logging, over-the-air update logs, driver monitoring data, discovery and spoliation issues, privacy/data protection constraints (GDPR, CCPA), chain-of-custody, access rights, and any standardization efforts (IEEE/ISO, UNECE, EU GSR EDR requirements). Provide practical implications for liability allocation.
  #5: Research additional relevant case law and regulatory actions (through early 2026) beyond Tesla: U.S. and international lawsuits, jury verdicts, settlements, and agency actions involving AEB failures, lane keeping, driver monitoring, and hands-free Level 2 systems (GM Super Cruise, Ford BlueCruise, Mercedes Drive Pilot Level 3, etc. where available). Focus on how courts/regulators reason about driver vs OEM responsibility and the role of warnings/ODD/DSM.
  #6: Deepen the comparative legal framework survey for ADAS crashes (U.S., EU, UK, Germany, Japan/China if salient): negligence/comparative fault, product liability (design defect, failure to warn, post-sale duty), motor insurance regimes, EU Product Liability Directive revision (adopted 2024) and its digital/software/AI aspects, UNECE ALKS R157 and other regulations, and how safety standards (ISO 26262, ISO 21448 SOTIF, ISO/SAE 21434, UL 4600) are used in litigation/regulation.

============================================================

ANSWER:
### 1) Why liability allocation is uniquely hard in “shared” ADAS driving

**Shared human–machine driving is not “automation replaces the driver.”** For the systems most consumers encounter (especially **SAE Level 2**), the vehicle may simultaneously control **lateral** and **longitudinal** motion, yet the **human remains the driver** and is still expected to perform key parts of the Dynamic Driving Task (DDT)—notably **Object and Event Detection and Response (OEDR)**—and to supervise the system continuously [[1]](https://wiki.unece.org/download/attachments/128418539/SAE%20J3016_202104.pdf). SAE emphasizes these “roles” are based on **design intent and expected use**, not on what a particular driver actually did in the moment (e.g., neglect does not magically reassign the “driver” role to the system) [[1]](https://wiki.unece.org/download/attachments/128418539/SAE%20J3016_202104.pdf).

That creates the central legal/technical tension:

- **The safety concept of Level 2 depends on continuous human supervision** (eyes-on/brain-on), but
- **Real-world Level 2 implementations can predictably produce** overreliance, distraction, mode confusion, and delayed intervention—failure modes that are both human and system-mediated [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf)[[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf)[[10]](https://pmc.ncbi.nlm.nih.gov/articles/PMC12329158/)[[11]](https://www.sciencedirect.com/science/article/abs/pii/S0925753520302423).

Regulators and investigators increasingly frame this as a *systems problem* rather than solely “bad driver behavior,” especially where **HMI design, driver monitoring, ODD permissiveness, and marketing** make misuse foreseeable [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf)[[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf).

---

## 2) Technical principles of ADAS that matter for assigning responsibility

### 2.1 SAE Levels 0–3: responsibility follows *DDT* + *fallback* allocation
SAE J3016 is the backbone for explaining “who was supposed to do what”:

- **Level 0:** driver performs the entire DDT (even if crash-avoidance features intervene momentarily) [[1]](https://wiki.unece.org/download/attachments/128418539/SAE%20J3016_202104.pdf).
- **Level 1:** system sustains either lateral *or* longitudinal motion control; driver does the rest of DDT [[1]](https://wiki.unece.org/download/attachments/128418539/SAE%20J3016_202104.pdf).
- **Level 2 (partial automation):** system sustains **both lateral and longitudinal motion control**, but the **driver must perform OEDR and supervise** the system [[1]](https://wiki.unece.org/download/attachments/128418539/SAE%20J3016_202104.pdf).
- **Level 3 (conditional automation):** the ADS performs the **entire DDT** within its **ODD**, but the **DDT fallback-ready user** must respond to takeover requests and relevant failures [[1]](https://wiki.unece.org/download/attachments/128418539/SAE%20J3016_202104.pdf).

**Key liability implication:** at **Level 2**, the driver’s duty is not “be ready when asked,” it is **continuous supervision**; at **Level 3**, the system owes the user **sufficient time, effective takeover prompting, and fallback behavior** (e.g., minimum-risk maneuvers) [[1]](https://wiki.unece.org/download/attachments/128418539/SAE%20J3016_202104.pdf)[[37]](https://unece.org/sites/default/files/2023-12/R157e.pdf).

---

### 2.2 Operational Design Domain (ODD) is the technical boundary line
ODD is the set of conditions in which the system is designed to function (road type, speed range, weather, lighting, lane markings, etc.) [[1]](https://wiki.unece.org/download/attachments/128418539/SAE%20J3016_202104.pdf). Investigators repeatedly find crashes where Level 2 systems are used **outside a defensible ODD**, often because the product is not meaningfully restricted to its intended domain.

The NTSB has explicitly criticized partial automation deployments where:
- drivers use Level 2 outside its ODD,
- manufacturers do not restrict it,
- and safeguards are inadequate to ensure safe operation [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf).

**Practical allocation point:** if a system is used **outside ODD**, a driver may be negligent; but if the system’s *availability design* makes out-of-ODD use easy and predictable, that can support a claim of **defective design / inadequate safeguards** [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf)[[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf).

---

### 2.3 Typical Level 2 “stack” and why “control” ≠ “responsibility”
Many Level 2 systems are essentially:
- **ACC/TACC** (longitudinal control),
- **lane-centering / Autosteer** (lateral control),
with the driver expected to watch for hazards and unusual geometry [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf).

Two recurring crash mechanisms show why a driver can remain legally responsible while the system is technically “driving”:

1) **Perception limitations (what the system won’t reliably detect)**  
In the Mountain View crash, NTSB found Tesla’s collision avoidance systems were **not designed to detect** the crash attenuator; FCW and AEB did not activate and ACC accelerated toward set speed [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf). That is an OEDR limitation: the system continued control but did not understand the hazard.

2) **Lane-geometry / path-selection edge cases**  
NTSB found Autopilot steered into a gore area due to limitations in vision processing and did so without alerting the driver [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf). That is a “within-lane” control system failing at “which lane/path is correct,” again an OEDR-adjacent failure.

---

### 2.4 Driver monitoring (DMS/DSM) is the critical “safety contract enforcement” layer
Because Level 2 depends on continuous human supervision, the technical question becomes: **does the system effectively verify supervision?**

- NTSB: steering wheel torque as a proxy is an **ineffective surrogate** for engagement and standards are needed for effective driver monitoring for Level 2 systems [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf).
- On-road study: hands-on sensors are easy to game; drivers used strategies to satisfy them; unintentional disengagements and **mode confusion** occurred due to insufficient cues [[11]](https://www.sciencedirect.com/science/article/abs/pii/S0925753520302423).
- NHTSA (EA22-002): in Tesla crash reviews, drivers crashed **despite meeting Tesla’s pre-recall engagement monitoring criteria**, and often with hazards visible for many seconds and little/no braking or steering response [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf). NHTSA concluded “insufficient controls can lead to foreseeable driver disengagement” [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf).

**Liability relevance:** if a manufacturer sells Level 2 as requiring constant supervision, yet the system can be operated for long periods while the driver is not truly engaged, plaintiffs argue the system is **unreasonably unsafe as designed**, even if the driver was also negligent [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf)[[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf).

---

### 2.5 Mode confusion and “partial disengagement” are predictable shared-control hazards
A particularly thorny category is **mixed-mode operation**: e.g., lateral support drops while longitudinal remains, or vice versa.

NHTSA described “inadvertent override” crashes where drivers may unknowingly deactivate Autosteer while TACC remains active; most crashes occurred within **<5 seconds** after override, consistent with disengagement + design-induced misunderstanding [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf). A real-world on-road study similarly found unintentional automation disengagements and drivers incorrectly thinking automation remained active due to insufficient cues [[11]](https://www.sciencedirect.com/science/article/abs/pii/S0925753520302423).

**Allocation implication:** when the *system design* creates a high risk of mode confusion (ambiguous HMI, easy inadvertent override, poor alerts), responsibility becomes less purely “driver failed to supervise” and more plausibly **shared**.

---

### 2.6 Takeover timing science: why “the driver should have taken over” is not always realistic
Even though Level 2 assumes continuous monitoring, real systems still:
- disengage unexpectedly,
- face edge cases,
- demand rapid driver intervention.

Takeover research (Level 3 context) suggests drivers distracted by non-driving tasks often need:
- ~3–4 seconds to look back to the road,
- ~6–8 seconds for physical readiness and system-off,
- ~12–15 seconds for broader situation awareness indicators [[3]](https://www-esv.nhtsa.dot.gov/Proceedings/25/25ESV-000027.pdf).

If a Level 2 system’s alerts occur too late—or the system enables out-of-the-loop behavior—“driver should have intervened” may become technically unrealistic, shifting scrutiny to **warning design, monitoring, and fallback behavior** [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf)[[3]](https://www-esv.nhtsa.dot.gov/Proceedings/25/25ESV-000027.pdf).

---

## 3) What current investigations and incidents show about “boundaries of responsibility”

### 3.1 NTSB: Williston (2016) and Mountain View (2018)
NTSB does not assign legal fault and its reports have limits on use in civil litigation [[4]](https://www.ntsb.gov/investigations/accidentreports/reports/har1702.pdf), but its causal narratives heavily influence how courts, regulators, and experts frame responsibility.

- **Williston (2016 Model S under-trailer crash):** probable cause included truck driver’s failure to yield *and* Tesla driver inattention due to **overreliance** on automation; contributing factor was Tesla’s operational design permitting prolonged disengagement and use inconsistent with warnings [[4]](https://www.ntsb.gov/investigations/accidentreports/reports/har1702.pdf).
- **Mountain View (2018 Model X gore/attenuator):** probable cause included Autopilot steering into gore due to system limitations + driver distraction/overreliance; NTSB criticized ineffective driver engagement monitoring and insufficient alert timing; also noted collision avoidance didn’t detect attenuator so FCW/AEB didn’t trigger and ACC accelerated [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf).

**Boundary lesson:** even when the driver remains obligated (Level 2), NTSB treats **ODD safeguards, engagement monitoring, alert timing, and system limitations** as manufacturer responsibilities that can contribute materially to crashes [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf)[[4]](https://www.ntsb.gov/investigations/accidentreports/reports/har1702.pdf).

---

### 3.2 NHTSA EA22-002 → Recall 23V-838: “foreseeable misuse” becomes a defect theory
NHTSA’s EA22-002 is a key modern example of a regulator examining Level 2 responsibility as a *product design* issue:

- ODI reviewed hundreds of Autopilot-involved crashes with logs/video/EDR; identified a trend of **avoidable** crashes with hazards visible to attentive drivers [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf).
- In many frontal-plane crashes, hazards were visible **≥5 seconds** before impact; drivers often didn’t brake/steer until <1 second before impact (or not at all) [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf).
- NHTSA stated “insufficient controls can lead to foreseeable driver disengagement and avoidable crashes” and Tesla recalled vehicles to add controls/alerts and lockouts [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf)[[15]](https://static.nhtsa.gov/odi/rcl/2023/RCLRPT-23V838-8276.PDF).

Tesla’s recall framing is also revealing:
- It reiterated drivers are responsible and must keep hands on wheel and remain attentive,
- but conceded that in some circumstances the **prominence and scope of controls** may be insufficient to prevent misuse [[15]](https://static.nhtsa.gov/odi/rcl/2023/RCLRPT-23V838-8276.PDF).

**Boundary lesson:** regulators may treat *a driver’s failure to supervise* as foreseeable misuse that a reasonable Level 2 design must mitigate—without converting Level 2 into Level 3.

---

### 3.3 NHTSA opens similar scrutiny of other Level 2 systems (BlueCruise)
NHTSA upgraded an investigation of **Ford BlueCruise** to Engineering Analysis EA25001 to examine frontal collisions and “system limitations” and to evaluate **drivers’ ability to respond** when scenarios exceed system limitations [[45]](https://static.nhtsa.gov/odi/inv/2025/INIM-EA25001-13728.pdf).

**Boundary lesson:** the driver/system boundary is increasingly being evaluated as:  
- what limits exist,  
- whether they are communicated/enforced, and  
- whether humans can realistically compensate.

---

## 4) Legal frameworks (and how they map to technical facts)

### 4.1 United States (general doctrines + ADAS-specific enforcement posture)
**Doctrinally**, U.S. crash litigation involving ADAS usually blends:
- **driver negligence** (inattention, speeding, impairment, misuse),
- **product liability** (design defect, failure to warn, manufacturing defect),
- sometimes **fraud/negligent misrepresentation** (marketing claims),
- plus **comparative fault** allocations (jurisdiction-dependent).

While the provided sources are mainly regulatory and case-specific (not treatises on U.S. tort doctrine), they show how these theories appear in real disputes.

#### (a) Marketing / misrepresentation as responsibility-shifting conduct
- California DMV accused Tesla’s “Autopilot” and “Full Self-Driving Capability” marketing of implying autonomy (“no action required by the person in the driver’s seat”), and asserted disclaimers did not cure misleading headline claims [[20]](https://www.thedrive.com/wp-content/uploads/2022/08/08/21-02189-California-DMV-Tesla-Autopilot-Complaint.pdf)[[21]](https://ca-times.brightspotcdn.com/18/c1/a05dcc814f8abc76bef3677c61a9/tesla-manufacturer-accusation.pdf). California DMV later announced a decision finding the terminology misleading and imposing license-suspension remedies conditioned on corrective action [[49]](https://www.dmv.ca.gov/portal/news-and-media/news-releases/dmv-finds-tesla-violated-california-state-law/).
- A Florida federal order (summary judgment / Daubert context) recounts plaintiffs relying on Musk/Tesla statements asserting dramatic safety and near-autonomy, using these to support negligent misrepresentation and failure-to-warn theories [[17]](https://cases.justia.com/federal/district-courts/florida/flsdce/1:2021cv21940/593426/428/0.pdf).
- China’s MIIT reportedly moved to restrict advertising terms like “autonomous driving” / “smart driving” for driver-assistance features, tightening OTA upgrade controls [[70]](https://www.reuters.com/business/autos-transportation/china-bans-smart-autonomous-driving-terms-vehicle-ads-2025-04-17/).

**Legal relevance:** capability marketing can expand manufacturer responsibility by:
- shaping the “reasonable user” expectations,
- supporting foreseeability of misuse/overreliance,
- undermining warning defenses,
- and supporting punitive damages narratives (as in the Florida jury report below).

#### (b) A litigated example of shared fault: Florida 2019 crash (jury allocation reported)
BBC reported a Florida jury found Tesla partly liable for a 2019 Autopilot-related crash, allocating Tesla one-third of compensatory damages and all punitive damages (subject to caps/appeal), with arguments focused on Tesla’s choice not to restrict Autopilot to controlled-access highways and alleged “self-driving hype,” while Tesla argued driver distraction and override (accelerator) made the driver solely responsible [[18]](https://www.bbc.com/news/articles/c93dqpkwx4xo).

Even as secondary reporting, it illustrates the **typical allocation battlefield**:
- **Driver fault:** distraction, misuse, overriding system, failure to brake/steer.
- **OEM fault:** foreseeable misuse, inadequate restrictions, inadequate warnings/monitoring, misleading capability claims.

#### (c) Regulatory standards as de facto “reasonableness” baselines
NHTSA’s defect analysis emphasizes that Level 2 requires constant supervision, yet “insufficient controls” may enable disengagement [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf). While not a courtroom standard, it functions as a powerful benchmark for “reasonable design” and “foreseeable misuse” arguments.

---

### 4.2 European Union: strict product liability modernized for software + regulatory type approval ecosystem

#### (a) Product Liability Directive 2024/2853 (PLD) extends strict liability into software/AI/updates
Directive (EU) **2024/2853** repeals the 1985 PLD and must be transposed by **9 December 2026** [[57]](https://eur-lex.europa.eu/eli/dir/2024/2853/oj/eng)[[58]](https://www.reedsmith.com/articles/the-new-eu-product-liability-key-implications-autonomous-vehicle/). Commentaries note it explicitly includes **software (embedded, stand-alone, cloud), OTA updates, and AI systems**, and recognizes cybersecurity vulnerabilities and failures to provide necessary updates as potential defect triggers [[58]](https://www.reedsmith.com/articles/the-new-eu-product-liability-key-implications-autonomous-vehicle/). It also introduces **rebuttable presumptions** (defect and/or causation) and stronger **disclosure** mechanisms where technical complexity creates excessive difficulty for claimants [[58]](https://www.reedsmith.com/articles/the-new-eu-product-liability-key-implications-autonomous-vehicle/); Parliament research similarly highlights expanded product definitions and evidentiary presumptions [[59]](https://www.europarl.europa.eu/RegData/etudes/STUD/2025/776426/IUST_STU(2025)776426_EN.pdf). (The proposed AI Liability Directive was later withdrawn by the Commission in early 2025 amid lack of agreement [[60]](https://www.twobirds.com/en/insights/2025/proposed-eu-ai-liability-rules-withdrawn).)

**Why it matters for ADAS:** shared-control crashes often hinge on hard-to-prove software behavior, sensor fusion, and post-sale updates. The PLD’s disclosure and presumption tools are designed to reduce asymmetry in exactly those settings [[58]](https://www.reedsmith.com/articles/the-new-eu-product-liability-key-implications-autonomous-vehicle/)[[59]](https://www.europarl.europa.eu/RegData/etudes/STUD/2025/776426/IUST_STU(2025)776426_EN.pdf).

#### (b) EU type-approval and safety regulation: ADAS becomes mandated baseline; EDR becomes standardized
The EU’s General Safety Regulation framework (Regulation (EU) 2019/2144, consolidated version available) underpins mandatory safety features and type approval [[32]](https://eur-lex.europa.eu/eli/reg/2019/2144/oj/eng). The Commission public summary notes that by **July 2024** many ADAS features become required in new vehicles and that cars/vans should be equipped with lane keeping, automated braking, and **event data recorders** [[13]](https://single-market-economy.ec.europa.eu/news/mandatory-drivers-assistance-systems-expected-help-save-over-25000-lives-2038-2024-07-05_en). Delegated Regulation (EU) 2022/545 provides detailed EDR technical rules under that framework [[33]](https://eur-lex.europa.eu/eli/reg_del/2022/545/oj/eng).

**Liability implication:** as ADAS becomes ubiquitous, courts may increasingly treat failures as:
- either a **defect** (if system performs unreasonably),
- or **expected limitation** (if within declared limits),
but with better EDR data to adjudicate driver inputs and system status.

---

### 4.3 UK: insurer-first compensation model for “automated driving” + new authorization regime

- The **Automated and Electric Vehicles Act 2018 (AEVA)** creates a model where, when an insured automated vehicle is “driving itself,” **the insurer is initially liable** to compensate victims, then can pursue recovery from responsible parties (e.g., manufacturer) [[65]](https://www.brownejacobson.com/insights/the-word-september-2023/automated-vehicle-insurance-and-the-automated-and-electric-vehicles-act-2018)[[66]](https://www.stewartslaw.com/news/unanswered-questions-following-automated-electric-vehicles-act-2018/). AEVA includes exclusions tied to **prohibited software alterations** and failure to install **safety-critical updates** the insured knew/ought to know were safety-critical [[66]](https://www.stewartslaw.com/news/unanswered-questions-following-automated-electric-vehicles-act-2018/), and applies contributory negligence principles [[66]](https://www.stewartslaw.com/news/unanswered-questions-following-automated-electric-vehicles-act-2018/).
- The **Automated Vehicles Act 2024** establishes a broader authorization and enforcement framework (self-driving capability, authorized features/entities, transition demands, information powers, sanctions) [[63]](https://www.legislation.gov.uk/ukpga/2024/10/contents).

**Boundary effect:** AEVA’s structure intentionally prevents victims from having to litigate “driver vs OEM software” before being compensated; it turns many disputes into insurer subrogation battles where high-quality logs, update compliance, and authorization status become critical.

---

### 4.4 Germany: explicit statutory accommodation of automation + higher strict holder liability caps
Germany’s framework (as summarized by CMS and the Library of Congress) combines:
- general traffic liability rules (strict holder liability; presumed driver negligence),
- AV-specific rules for Level 3/4 operation,
- and structured approval/ODD/technical supervisor concepts for Level 4 [[67]](https://cms.law/en/int/expert-guides/cms-expert-guide-to-autonomous-vehicles-avs/germany)[[68]](https://www.loc.gov/item/global-legal-monitor/2021-08-09/germany-road-traffic-act-amendment-allows-driverless-vehicles-on-public-roads/).

Key points from the sources:
- 2017 StVG changes reportedly **doubled maximum strict liability** for highly/fully automated functions to **€10M personal injury** and **€2M property** [[67]](https://cms.law/en/int/expert-guides/cms-expert-guide-to-autonomous-vehicles-avs/germany).
- Germany’s 2021 Autonomous Driving Act enables Level 4 operation in specified operating areas with a **technical supervisor** and includes extensive **data storage obligations** (including geographic position, activation/deactivation, software status, environmental conditions, etc.) accessible to authorities for monitoring safe operation [[68]](https://www.loc.gov/item/global-legal-monitor/2021-08-09/germany-road-traffic-act-amendment-allows-driverless-vehicles-on-public-roads/).
- Manufacturers have explicit cybersecurity-related duties; failures can drive liability [[67]](https://cms.law/en/int/expert-guides/cms-expert-guide-to-autonomous-vehicles-avs/germany)[[68]](https://www.loc.gov/item/global-legal-monitor/2021-08-09/germany-road-traffic-act-amendment-allows-driverless-vehicles-on-public-roads/).

**Boundary effect:** Germany is moving toward a world where “who was in control” is not just argued—it is **logged** and **regulated**.

---

### 4.5 UNECE regulations as cross-border “design intent” and evidence frameworks
UNECE regulations matter because they encode core principles relevant to shared control:

- **UNECE R79 (steering):** “Advanced Driver Assistance Steering Systems” are assistance where the **driver remains at all times in primary control** and can override by deliberate action [[12]](https://unece.org/sites/default/files/2024-04/R079r5e.pdf). This aligns with Level 2 responsibility: driver primacy.
- **UNECE R157 (ALKS, Level 3 type approval):** formalizes **transition demands**, escalation timing, **minimum risk manoeuvres**, and mandates a **Driver Availability Recognition System** with criteria/timing requirements [[37]](https://unece.org/sites/default/files/2023-12/R157e.pdf). It also includes DSSAD concepts for determining driver/system interaction [[37]](https://unece.org/sites/default/files/2023-12/R157e.pdf).
- **UNECE R156 (software updates / SUMS):** requires governance/traceability of updates, version identification, compatibility checks, assessments of safety impacts, and retention/documentation for regulatory scrutiny [[41]](https://unece.org/sites/default/files/2024-03/R156e%20%282%29.pdf).
- UNECE press releases emphasize cybersecurity and software update regulations as binding audited systems (CSMS/SUMS) and describe OTA update integrity, user information, and safety assurance expectations [[61]](https://unece.org/sustainable-development/press/un-regulations-cybersecurity-and-software-updates-pave-way-mass-roll).
- UNECE has endorsed **DSSAD guidance** explicitly designed to establish whether driver or system was requested to be in control, and who actually performed the driving task [[50]](https://unece.org/sustainable-development/press/unece-adopts-new-regulation-emergency-lane-keeping-system-and).

**Boundary effect:** UNECE frameworks increasingly treat liability-adjacent questions (control state, takeover, update governance) as *regulated engineering requirements*, not merely contractual disclaimers.

---

## 5) A structured liability-allocation framework for shared human–machine crashes

Below is a **decision-point framework** that translates technical facts into legal responsibility boundaries. It is designed for Level 2 primarily, but shows Level 3 boundary shifts.

### Step 1 — Identify the automation mode and the *expected role*
Use **SAE J3016** definitions to classify the engaged function:
- Was it Level 2 partial automation (system controls both axes; driver does OEDR)? [[1]](https://wiki.unece.org/download/attachments/128418539/SAE%20J3016_202104.pdf)
- Was it Level 3 ALKS-like conditional automation (system does OEDR; user is fallback-ready)? [[1]](https://wiki.unece.org/download/attachments/128418539/SAE%20J3016_202104.pdf)[[37]](https://unece.org/sites/default/files/2023-12/R157e.pdf)

**Legal consequence:** the baseline duty of care shifts dramatically between L2 and L3.

---

### Step 2 — Was the system operating within its ODD (and was ODD enforceable)?
**ODD compliance questions:**
- Road class (controlled-access highway vs city streets), lane markings, traffic patterns
- Weather/visibility
- Speed range
- Excluded conditions (construction zones, fog, heavy rain, etc.)

Regulators explicitly use **ODD gating** for Level 3 deployment permits (e.g., California DMV’s Drive Pilot permit limited to daylight highways ≤40 mph and excluding construction zones and adverse weather) [[48]](https://www.dmv.ca.gov/portal/news-and-media/california-dmv-approves-mercedes-benz-automated-driving-system-for-certain-highways-and-conditions/).

**Allocation principle:**
- **Within ODD:** system performance and HMI/monitoring adequacy become central.
- **Outside ODD:** driver negligence increases—but OEM exposure can remain if out-of-ODD use was *foreseeable and insufficiently restricted* (a major NTSB critique of Level 2 permissiveness) [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf).

---

### Step 3 — What was the driver’s engagement state, and how was it measured?
Evidence from investigations shows that “hands on wheel torque” is a weak proxy [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf), drivers can meet criteria and still be disengaged [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf), and mode confusion can arise [[11]](https://www.sciencedirect.com/science/article/abs/pii/S0925753520302423).

**Allocation principle (Level 2):**
- If objective evidence shows the driver was distracted (no braking/steering response with hazards visible for many seconds), **driver negligence** is strong [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf).
- If the system’s monitoring is objectively ineffective such that disengagement is predictable and common, **design defect / foreseeable misuse** arguments strengthen [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf)[[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf).

---

### Step 4 — Did the system create or fail to mitigate a hazardous trajectory consistent with known limitations?
Two sub-questions:

1) **Perception/avoidance scope:** was the hazard one the system was designed to detect/mitigate?  
NTSB found non-detection of the Mountain View attenuator prevented FCW/AEB action and ACC accelerated [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf).

2) **Path/lane choice:** did lane-keeping mis-handle splits/gore areas without alert?  
NTSB found Autopilot steered into gore due to vision processing limitations without alert [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf).

**Allocation principle:**
- If the system behaves within disclosed limits and the driver failed to supervise → driver share rises.
- If the system’s behavior is unreasonably unsafe within typical use (including foreseeable edge cases on common highways) and warnings/monitoring/ODD constraints are inadequate → OEM share rises.

---

### Step 5 — Mode transition integrity (especially “partial disengagement”)
NHTSA’s “inadvertent override” category highlights a design-sensitive boundary problem: Autosteer off + TACC on + ambiguous cues → rapid crash risk [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf). On-road research found drivers sometimes believed they were still in automated mode due to insufficient cues [[11]](https://www.sciencedirect.com/science/article/abs/pii/S0925753520302423).

**Allocation principle:** if mode changes are easy to trigger unintentionally and not clearly communicated, responsibility becomes structurally shared: the driver must remain attentive, but the OEM may have created an unreasonably confusing shared-control interface [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf)[[11]](https://www.sciencedirect.com/science/article/abs/pii/S0925753520302423).

---

### Step 6 — Software versioning, OTA updates, and post-sale governance
Because behavior changes over time, the “product at time of crash” includes its **software configuration**.

- UNECE R156 requires rigorous update governance, traceability of software versions (RXSWIN), compatibility assessments, safety impact assessments, and recordkeeping [[41]](https://unece.org/sites/default/files/2024-03/R156e%20%282%29.pdf).
- EU product liability modernization explicitly pulls OTA updates and software into strict liability scope [[58]](https://www.reedsmith.com/articles/the-new-eu-product-liability-key-implications-autonomous-vehicle/).

**Allocation principle:**
- If an OEM update introduces/regresses unsafe behavior → stronger OEM responsibility.
- If a user fails to install safety-critical updates under regimes that allocate duty (e.g., AEVA exclusions) → user/insurer defenses strengthen [[66]](https://www.stewartslaw.com/news/unanswered-questions-following-automated-electric-vehicles-act-2018/).

---

## 6) Evidence: what must be proven, and what data is needed

### 6.1 EDR: short window, high value, increasingly richer
NHTSA describes EDRs as capturing seconds of pre/during/post crash technical data (vehicle dynamics, driver inputs, restraint status), not audio/video [[29]](https://www.nhtsa.gov/research-data/event-data-recorder). Part 563 sets standardized requirements for vehicles equipped with EDRs [[30]](https://www.federalregister.gov/documents/2025/11/28/2025-21506/event-data-recorders). NHTSA finalized a rule increasing pre-crash capture to **20 seconds at 10 Hz** (from 5 seconds at 2 Hz), aimed at better capturing initiation of maneuvers and correlating driver commands with vehicle response [[30]](https://www.federalregister.gov/documents/2025/11/28/2025-21506/event-data-recorders).

Some EDRs (even if not required) may record ADAS-related statuses like ACC, AEB, FCW, LDW, etc. [[31]](https://www.federalregister.gov/documents/2024/12/18/2024-29862/event-data-recorders)—which is crucial for shared-control disputes.

**Key limitation:** EDR alone may not capture sustained ADAS behavior minutes earlier, nor nuanced driver monitoring state.

---

### 6.2 DSSAD and “who was driving” logging: purpose-built for control attribution
UNECE describes DSSAD guidance as recording timestamped flags during Level 3–5 automated sequences to clarify driver/system interactions and determine who was requested to be in control and who actually was [[50]](https://unece.org/sustainable-development/press/unece-adopts-new-regulation-emergency-lane-keeping-system-and). OICA materials describe DSSAD as complementary to EDR: EDR is crash-triggered seconds; DSSAD is longer-window, flag-based “who was in control” evidence [[38]](https://unece.org/DAM/trans/doc/2019/wp29grva/GRVA-02-20e.pdf)[[39]](https://wiki.unece.org/download/attachments/87621710/EDR-DSSAD-01-04%20%28OICA%29%20Positions%20on%20EDR%20and%20DSSAD.pdf). R157 embeds DSSAD concepts and takeover/MRM logic [[37]](https://unece.org/sites/default/files/2023-12/R157e.pdf).

**Why it matters:** many contested events (near-misses, minor impacts, non-airbag crashes) may not trigger EDR in a meaningful way; DSSAD-like logs fill the gap.

---

### 6.3 Privacy constraints are not optional—especially for driver monitoring
EU data protection authorities emphasize that EDR and driver monitoring data are personal data; fatigue/drowsiness may be **health data** under GDPR Article 9, requiring enhanced safeguards [[34]](https://www.europarl.europa.eu/cmsdata/158076/2018_11_29_wiewiorowski_event_recorders.pdf). California regulators similarly treat connected vehicles as “computers on wheels” collecting sensitive data, emphasizing rights to know/delete/opt out under CCPA enforcement priorities [[35]](https://cppa.ca.gov/announcements/2023/20230731.html).

**Design implication:** regulators should not require safety logging without also specifying:
- lawful access routes,
- minimization,
- security,
- retention limits,
- and standardized “post-crash disclosure” processes.

---

## 7) Where courts and regulators are drawing the responsibility line (patterns)

### Pattern A — “Driver failed to supervise” is powerful when data shows non-response to visible hazards
NHTSA quantified many crashes where hazards were visible for multiple seconds and drivers did not brake/steer until the last instant [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf). This is strong evidence for driver negligence.

### Pattern B — “Foreseeable misuse” becomes OEM responsibility when system controls/monitoring are inadequate
NHTSA explicitly tied Tesla recall to concerns about foreseeable disengagement from insufficient controls [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf)[[15]](https://static.nhtsa.gov/odi/rcl/2023/RCLRPT-23V838-8276.PDF). NTSB similarly emphasized design choices that permit prolonged disengagement and enable overreliance [[4]](https://www.ntsb.gov/investigations/accidentreports/reports/har1702.pdf).

### Pattern C — Misleading marketing can shift the “reasonable reliance” baseline
California DMV treated autonomy-implying claims as unlawful even with disclaimers [[20]](https://www.thedrive.com/wp-content/uploads/2022/08/08/21-02189-California-DMV-Tesla-Autopilot-Complaint.pdf)[[21]](https://ca-times.brightspotcdn.com/18/c1/a05dcc814f8abc76bef3677c61a9/tesla-manufacturer-accusation.pdf)[[49]](https://www.dmv.ca.gov/portal/news-and-media/news-releases/dmv-finds-tesla-violated-california-state-law/). EU/German concerns about “Autopilot” naming similarly framed the term as potentially misleading about attention needs [[25]](https://www.reuters.com/article/business/dutch-regulator-weighs-teslas-use-of-autopilot-name-idUSKBN12H1EK/). China reportedly moved to ban “autonomous driving” marketing terms for driver assistance features [[70]](https://www.reuters.com/business/autos-transportation/china-bans-smart-autonomous-driving-terms-vehicle-ads-2025-04-17/).

### Pattern D — For Level 3, the boundary shifts toward the system during engaged operation—*but only inside a tight ODD with formal takeover/MRM safeguards*
California DMV’s Drive Pilot permit shows the compliance model: narrow ODD + mandatory consumer education + highway-only low speed constraints [[48]](https://www.dmv.ca.gov/portal/news-and-media/california-dmv-approves-mercedes-benz-automated-driving-system-for-certain-highways-and-conditions/). UNECE R157 formalizes takeover timing, driver availability monitoring, and minimum risk maneuvers [[37]](https://unece.org/sites/default/files/2023-12/R157e.pdf).

---

## 8) Regulatory guidelines and recommendations (2026-forward)

What follows is a concrete, implementable package intended to reduce ambiguity and make post-crash allocation more consistent across jurisdictions.

### 8.1 Standardize capability labels and prohibit autonomy-implying names for Level 2
**Rule:**
- If the system is **SAE Level 2**, marketing and HMI must include “Driver Assistance / Supervised” language and must not imply autonomy.
- Regulators should treat contradictory disclaimers as insufficient where headline claims imply “no driver action” (consistent with CA DMV posture) [[20]](https://www.thedrive.com/wp-content/uploads/2022/08/08/21-02189-California-DMV-Tesla-Autopilot-Complaint.pdf)[[21]](https://ca-times.brightspotcdn.com/18/c1/a05dcc814f8abc76bef3677c61a9/tesla-manufacturer-accusation.pdf)[[49]](https://www.dmv.ca.gov/portal/news-and-media/news-releases/dmv-finds-tesla-violated-california-state-law/).

**Rationale:** naming affects mental models; NHTSA observed “Autopilot” terminology may invite overtrust [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf), and multiple regulators have pursued misleading advertising theories [[20]](https://www.thedrive.com/wp-content/uploads/2022/08/08/21-02189-California-DMV-Tesla-Autopilot-Complaint.pdf)[[21]](https://ca-times.brightspotcdn.com/18/c1/a05dcc814f8abc76bef3677c61a9/tesla-manufacturer-accusation.pdf)[[25]](https://www.reuters.com/article/business/dutch-regulator-weighs-teslas-use-of-autopilot-name-idUSKBN12H1EK/)[[49]](https://www.dmv.ca.gov/portal/news-and-media/news-releases/dmv-finds-tesla-violated-california-state-law/)[[70]](https://www.reuters.com/business/autos-transportation/china-bans-smart-autonomous-driving-terms-vehicle-ads-2025-04-17/).

---

### 8.2 Mandate measurable driver monitoring performance standards for Level 2
**Rule:**
- Replace torque-only proxies with **camera-based attention monitoring** or equivalent, with defined performance metrics (false accept / false reject rates; robustness to sunglasses, glare, head pose; time-to-detect prolonged eyes-off-road).
- Require **graduated escalation**: prompt → persistent alert → controlled disengagement → lockout for repeated violations (mirroring recall remedies and industry trends) [[15]](https://static.nhtsa.gov/odi/rcl/2023/RCLRPT-23V838-8276.PDF).
- Require that **mode confusion risks** (e.g., Autosteer override while ACC remains) be mitigated by:
  - unambiguous HMI status,
  - distinctive alerts,
  - and safer transition logic (avoid “silent partial disengagement”) [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf)[[11]](https://www.sciencedirect.com/science/article/abs/pii/S0925753520302423).

**Rationale:** NTSB calls torque-based monitoring ineffective [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf); NHTSA documented crashes with disengaged drivers meeting criteria [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf); mode confusion is empirically observed [[11]](https://www.sciencedirect.com/science/article/abs/pii/S0925753520302423).

---

### 8.3 Require explicit ODD declarations *and* enforceable ODD gating for Level 2 “high authority” functions
**Rule:**
- For any Level 2 feature providing sustained lane-centering + ACC at highway speeds, OEMs must:
  - publish an ODD specification (road types, speed ranges, weather/visibility constraints, lane marking quality assumptions),
  - implement **ODD gating** (geofencing for road type; speed constraints; weather detection thresholds where feasible),
  - and provide a “confidence/availability” display to the driver when the system is operating near its limits.

**Rationale:** NTSB criticized lack of restriction and use outside ODD [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf); California DMV’s Level 3 deployment model relies on tight ODD constraints [[48]](https://www.dmv.ca.gov/portal/news-and-media/california-dmv-approves-mercedes-benz-automated-driving-system-for-certain-highways-and-conditions/).

---

### 8.4 Standardize takeover / disengagement protocols (even for Level 2)
While Level 2 does not “hand over” in the Level 3 sense, systems still disengage and drivers still need time.

**Rule:**
- Mandate minimum alert lead times and escalation behaviors when the system detects it cannot maintain function (lane loss, sensor occlusion, severe curvature).
- If disengagement occurs, require:
  - a distinct “control returned to driver” cue,
  - a short “stabilization” behavior (e.g., controlled deceleration) where safe to reduce the severity of sudden reversion.
- For Level 3 systems, align with UNECE R157 takeover timing and minimum risk manoeuvres [[37]](https://unece.org/sites/default/files/2023-12/R157e.pdf).

**Rationale:** takeover readiness is not instantaneous; delayed awareness can require many seconds [[3]](https://www-esv.nhtsa.dot.gov/Proceedings/25/25ESV-000027.pdf). R157 provides a reference architecture for takeover and fallback [[37]](https://unece.org/sites/default/files/2023-12/R157e.pdf).

---

### 8.5 Software update governance as a safety obligation (OTA ≠ informal experimentation)
**Rule:**
- Adopt/align with UNECE R156-style SUMS obligations: update traceability, compatibility checks, safety impact assessments, user notification, rollback/recovery, and regulatory auditability [[41]](https://unece.org/sites/default/files/2024-03/R156e%20%282%29.pdf).
- Require a public-facing “safety-relevant change log” for ADAS behavior changes (high-level, not trade secrets), enabling informed use and post-crash reconstruction.

**Rationale:** R156 is an established governance model [[41]](https://unece.org/sites/default/files/2024-03/R156e%20%282%29.pdf); China’s regulators are reportedly tightening OTA upgrade permissions for ADAS [[70]](https://www.reuters.com/business/autos-transportation/china-bans-smart-autonomous-driving-terms-vehicle-ads-2025-04-17/); EU PLD expands strict liability to software/updates [[58]](https://www.reedsmith.com/articles/the-new-eu-product-liability-key-implications-autonomous-vehicle/).

---

### 8.6 Minimum logging package for shared-control crashes (EDR + ADAS + driver monitoring + versioning)
**Rule:**
- Expand standardized crash logging beyond core EDR to include:
  - ADAS engagement status (ACC, lane-centering, AEB/FCW states),
  - driver monitoring state (attention availability flag, not raw biometric video),
  - takeover/disengagement warnings and timestamps,
  - software version identifiers and recent update history.
- Harmonize with UNECE DSSAD guidance aims (“who was requested to control, who actually controlled”) [[50]](https://unece.org/sustainable-development/press/unece-adopts-new-regulation-emergency-lane-keeping-system-and) while respecting minimization and privacy [[34]](https://www.europarl.europa.eu/cmsdata/158076/2018_11_29_wiewiorowski_event_recorders.pdf).

**Rationale:** Part 563 improvements increase EDR usefulness [[30]](https://www.federalregister.gov/documents/2025/11/28/2025-21506/event-data-recorders); some EDRs already include ADAS status fields [[31]](https://www.federalregister.gov/documents/2024/12/18/2024-29862/event-data-recorders); DSSAD is explicitly designed for control attribution [[50]](https://unece.org/sustainable-development/press/unece-adopts-new-regulation-emergency-lane-keeping-system-and)[[38]](https://unece.org/DAM/trans/doc/2019/wp29grva/GRVA-02-20e.pdf)[[39]](https://wiki.unece.org/download/attachments/87621710/EDR-DSSAD-01-04%20%28OICA%29%20Positions%20on%20EDR%20and%20DSSAD.pdf).

---

### 8.7 Privacy-by-design rules for driver monitoring and logging
**Rule:**
- Store raw cabin video locally with short retention unless a crash triggers preservation; export only derived event flags unless legally compelled.
- Define lawful access and retention explicitly (owner consent, court order, regulator request), with strong security controls and tamper-evidence.
- Treat fatigue/drowsiness measures as sensitive/health-adjacent under GDPR-like logic where applicable [[34]](https://www.europarl.europa.eu/cmsdata/158076/2018_11_29_wiewiorowski_event_recorders.pdf).

**Rationale:** EU data protection principles require proportionality, purpose limitation, and special protections for health-related data [[34]](https://www.europarl.europa.eu/cmsdata/158076/2018_11_29_wiewiorowski_event_recorders.pdf); California privacy enforcement is moving toward connected-vehicle scrutiny [[35]](https://cppa.ca.gov/announcements/2023/20230731.html).

---

### 8.8 Create presumptive fault “bands” tied to scenario classes (a practical adjudication tool)
Rather than pretending every crash is a blank slate, regulators could define rebuttable presumptions:

1) **Level 2, within ODD, driver demonstrably attentive, system makes unsafe control decision**  
→ presumption leaning toward **product defect** (OEM), rebuttable by misuse/tampering.

2) **Level 2, within ODD, driver demonstrably inattentive/disengaged for extended period**  
→ presumption leaning toward **driver negligence**, with OEM share considered if DMS/HMI controls were inadequate to prevent foreseeable disengagement [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf)[[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf).

3) **Level 2, outside ODD**  
→ strong presumption of **driver responsibility**, but allow OEM share if ODD was not communicated/enforced and out-of-ODD use was reasonably foreseeable [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf).

4) **Mode confusion / inadvertent override documented**  
→ presumptive **shared responsibility** unless OEM can show unambiguous HMI + robust mitigation [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf)[[11]](https://www.sciencedirect.com/science/article/abs/pii/S0925753520302423).

5) **Level 3 authorized operation within ODD with compliant takeover/MRM**  
→ default presumption leans toward **system responsibility during engaged DDT**; driver responsibility increases after a properly timed transition demand + non-response, consistent with R157 logic [[37]](https://unece.org/sites/default/files/2023-12/R157e.pdf).

This “fault band” concept mirrors the UK AEVA intuition (victim compensated first; then allocate responsibility via evidence) [[65]](https://www.brownejacobson.com/insights/the-word-september-2023/automated-vehicle-insurance-and-the-automated-and-electric-vehicles-act-2018)[[66]](https://www.stewartslaw.com/news/unanswered-questions-following-automated-electric-vehicles-act-2018/) and leverages DSSAD/EDR improvements to make allocation consistent [[50]](https://unece.org/sustainable-development/press/unece-adopts-new-regulation-emergency-lane-keeping-system-and)[[30]](https://www.federalregister.gov/documents/2025/11/28/2025-21506/event-data-recorders)[[31]](https://www.federalregister.gov/documents/2024/12/18/2024-29862/event-data-recorders).

---

## 9) Bottom line: defining the boundaries of responsibility

In shared human–machine driving, the boundary is best expressed as **a matrix of (automation level) × (ODD) × (engagement monitoring integrity) × (mode-transition clarity) × (system limitation management)**:

- **Drivers** remain primarily responsible in **Level 2** because they are expected to do OEDR and supervise continuously [[1]](https://wiki.unece.org/download/attachments/128418539/SAE%20J3016_202104.pdf). Evidence like hazard visibility and lack of driver response can strongly support driver negligence [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf).
- **Manufacturers** can bear significant responsibility when system design **predictably defeats the Level 2 supervision premise**—ineffective driver monitoring [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf)[[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf), permissive ODD enabling misuse [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf), confusing mode transitions [[6]](https://static.nhtsa.gov/odi/inv/2022/INCR-EA22002-14496.pdf)[[11]](https://www.sciencedirect.com/science/article/abs/pii/S0925753520302423), unsafe edge-case behavior without adequate warning [[2]](https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR2001.pdf), or misleading marketing that inflates reasonable reliance [[20]](https://www.thedrive.com/wp-content/uploads/2022/08/08/21-02189-California-DMV-Tesla-Autopilot-Complaint.pdf)[[21]](https://ca-times.brightspotcdn.com/18/c1/a05dcc814f8abc76bef3677c61a9/tesla-manufacturer-accusation.pdf)[[49]](https://www.dmv.ca.gov/portal/news-and-media/news-releases/dmv-finds-tesla-violated-california-state-law/)[[25]](https://www.reuters.com/article/business/dutch-regulator-weighs-teslas-use-of-autopilot-name-idUSKBN12H1EK/)[[70]](https://www.reuters.com/business/autos-transportation/china-bans-smart-autonomous-driving-terms-vehicle-ads-2025-04-17/).
- **For Level 3**, responsibility shifts meaningfully toward the system during engaged operation, but only where takeover demands, driver availability recognition, and minimum risk maneuvers meet formal standards like UNECE R157 and operate within a tightly defined ODD [[37]](https://unece.org/sites/default/files/2023-12/R157e.pdf)[[48]](https://www.dmv.ca.gov/portal/news-and-media/california-dmv-approves-mercedes-benz-automated-driving-system-for-certain-highways-and-conditions/).

The regulatory goal should not be to “pick a side,” but to **engineer the responsibility boundary into the product** through enforceable ODD gating, robust driver monitoring, safe transitions, mandatory logging, and truthful capability communication—so that when crashes occur, allocation is evidence-based, consistent, and fair.
